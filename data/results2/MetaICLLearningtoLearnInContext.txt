Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 2791 - 2809
July 10-15, 2022 ©2022 Association for Computational Linguistics
MetaICL: Learning to Learn In Context
Sewon Min1,2
Mike Lewis2
Luke Zettlemoyer1,2
Hannaneh Hajishirzi1,3
1University of Washington
2Meta AI
3Allen Institute for AI
{sewon,lsz,hannaneh}@cs.washington.edu
mikelewis@fb.com
Abstract
We introduce MetaICL (Meta-training for In-
Context Learning), a new meta-training frame-
work for few-shot learning where a pretrained
language model is tuned to do in-context learn-
ing on a large set of training tasks. This meta-
training enables the model to more effectively
learn a new task in context at test time, by sim-
ply conditioning on a few training examples
with no parameter updates or task-speciﬁc tem-
plates. We experiment on a large, diverse col-
lection of tasks consisting of 142 NLP datasets
including classiﬁcation, question answering,
natural language inference, paraphrase detec-
tion and more, across seven different meta-
training/target splits. MetaICL outperforms a
range of baselines including in-context learn-
ing without meta-training and multi-task learn-
ing followed by zero-shot transfer. We ﬁnd
that the gains are particularly signiﬁcant for
target tasks that have domain shifts from the
meta-training tasks, and that using a diverse
set of the meta-training tasks is key to im-
provements.
We also show that MetaICL
approaches (and sometimes beats) the per-
formance of models fully ﬁnetuned on the
target task, and outperforms much bigger
models with nearly 8x parameters.
Finally,
we show that MetaICL is complementary to
human-written instructions, and the best per-
formance can be achieved by combining both
approaches.
1
Introduction
Large language models (LMs) have recently been
shown to be able to do in-context learning (Brown
et al., 2020), where they learn a new task simply
by conditioning on a few training examples and
predicting which tokens best complete a test input.
This type of learning is attractive because the model
learns a new task through inference alone, without
any parameter updates. However, performance sig-
niﬁcantly lags behind supervised ﬁnetuning, results
are often high variance (Zhao et al., 2021; Perez
et al., 2021), and it can be difﬁcult to engineer the
templates that convert existing tasks to this format.
In this paper, we address these challenges by in-
troducing MetaICL: Meta-training for In-Context
Learning. MetaICL tunes a pretrained language
model on a large set of tasks to learn how to in-
context learn, and is evaluated on strictly new un-
seen tasks. Each meta-training example matches
the test setup—it includes k + 1 training examples
from one task that will be presented together as
a single sequence to the language model, and the
output of the ﬁnal example is used to calculate the
cross-entropy training loss. Simply ﬁnetuning the
model in this data setup directly leads to better in-
context learning—the model learns to recover the
semantics of the task from the given examples, as
must be done for in-context learning of a new task
at test time. This approach is related to recent work
that uses multi-task learning for better zero-shot
performance at test time (Khashabi et al., 2020;
Zhong et al., 2021; Mishra et al., 2022; Wei et al.,
2022; Sanh et al., 2022). However, MetaICL is dis-
tinct as it allows learning new tasks from k exam-
ples alone, without relying on a task reformatting
(e.g., reducing everything to question answering)
or task-speciﬁc templates (e.g., converting different
tasks to a language modeling problem).
We experiment on a large, diverse collection of
tasks taken from Ye et al. (2021) and Khashabi et al.
(2020), including 142 text classiﬁcation, question
answering, natural language inference and para-
phrase detection datasets. We report seven different
settings, all with no overlap between meta-training
and target tasks. This leads to 52 unique target
tasks in total, which is the largest among all recent
related work to the best of our knowledge.
Experimental results show that MetaICL consis-
tently outperforms baselines including (1) a variety
of LM in-context learning baselines without meta-
training (Brown et al., 2020; Zhao et al., 2021;
Holtzman et al., 2021; Min et al., 2022), and (2)
2791
multi-task learning followed by zero-shot trans-
fer (Zhong et al., 2021; Wei et al., 2022; Sanh
et al., 2022). Gains over multi-task zero-shot trans-
fer are particularly signiﬁcant when meta-training
tasks and target tasks are dissimilar, e.g. there
are large differences in task formats, domains, or
required skills. This demonstrates that MetaICL
enables the model to recover the semantics of the
task in context during inference even when the tar-
get does not share similarities with meta-training
tasks. MetaICL often gets close to (and sometimes
beats) the performance of models trained with su-
pervised ﬁnetuning on the target datasets, and per-
form as well as models with 8x parameters. We
also perform extensive ablations to identify key in-
gredients for success of MetaICL such as the num-
ber and diversity of meta-training tasks. Finally,
we demonstrate MetaICL without any templates is
better than recent work using human-written nat-
ural instructions, while the best performance is
achieved by combining both approaches. Code
and data are publicly released at github.com/
facebookresearch/MetaICL .
2
Related Work
In-context learning
Brown et al. (2020) propose
to use a language model (LM) conditioned on a
concatenation of training examples for few-shot
learning with no parameter updates. It has been
further improved by later work (Zhao et al., 2021;
Holtzman et al., 2021; Min et al., 2022), showing
promising results on a variety of tasks. However,
in-context learning with an LM achieves poor per-
formance when the target task is very different from
language modeling in nature or the LM is not large
enough. Moreover, it can have high variance and
poor worst-case accuracy (Perez et al., 2021; Lu
et al., 2021).
Our paper is based on the core idea of in-context
learning by conditioning on training examples. We
show that, by explicitly training on an in-context
learning objective, MetaICL achieves substantial
improvements even with smaller LMs.
Meta-training via multi-task learning
Our
work is broadly inspired by a large body of work
in meta-learning (Vilalta and Drissi, 2002; Finn
et al., 2017) and multi-task learning (Evgeniou
and Pontil, 2004; Ruder, 2017). Prior work has
shown that multi-task learning on a large collec-
tion of tasks leads to better performance on a new
task, either when tested zero-shot (Khashabi et al.,
2020; Zhong et al., 2021; Mishra et al., 2022; Wei
et al., 2022) or when further ﬁnetuned (Aghajanyan
et al., 2021; Ye et al., 2021). In particular, the for-
mer is closely related to our work, as it eliminates
the need for parameter updates on a target task.
However, these zero-shot models are either limited
to tasks sharing the same format as training tasks
(e.g., a question answering format) (Khashabi et al.,
2020; Zhong et al., 2021), or rely heavily on task-
speciﬁc templates (Mishra et al., 2022; Wei et al.,
2022; Sanh et al., 2022) which are difﬁcult to en-
gineer due to high variance in performance from
very small changes (Mishra et al., 2021).
In this paper, we propose a meta-training method
for better in-context learning that improves few-
shot performance.
We show that it effectively
learns semantics of a new task with no manual
effort, signiﬁcantly outperforming zero-shot trans-
fer methods.1 Furthermore, while Wei et al. (2022)
show that meta-training helps only when the model
has 68B or more parameters, our experiments
demonstrate improvements with a much smaller
model (770M).
Chen et al. (2022), concurrently to our work, pro-
pose meta-training for in-context learning. Our ap-
proach differs in a number of ways: we remove re-
quirements of human-written templates or instruc-
tions, and include more diverse tasks, stronger base-
lines, and extensive experiments in much larger
scale with many meta-training/target splits.
3
MetaICL
We introduce MetaICL: Meta-training for In-
Context Learning. Table 1 provides an overview
of the approach. The key idea is to use a multi-task
learning scheme over a large collection of meta-
training tasks, in order for the model to learn how
to condition on a small set of training examples, re-
cover the semantics of a task, and predict the output
based on it. Following previous literature (Brown
et al., 2020), the training examples are concate-
nated and provided as an single input to the model,
which is feasible for k-shot learning (e.g., k = 16).
At test time, the model is evaluated on an unseen
target task that comes with k training examples,
and inference directly follows the same data format
as in meta-training.
1We show that MetaICL without instructions is still better
than zero-shot transfer with instructions, but by using instruc-
tions, performance of MetaICL further improves (Section 5.2).
2792
Meta-training
Inference
Task
C meta-training tasks
An unseen target task
Data given
Training examples Ti = {(xi
j, yi
j)}Ni
j=1, ∀i ∈[1, C] (Ni ≫k)
Training examples (x1, y1), · · · , (xk, yk),
Test input x
Objective
For each iteration,
argmaxc∈CP(c|x1, y1, · · · , xk, yk, x)
1. Sample task i ∈[1, C]
2. Sample k + 1 examples from Ti: (x1, y1), · · · , (xk+1, yk+1)
3. Maximize P(yk+1|x1, y1, · · · , xk, yk, xk+1)
Table 1: Overview of MetaICL (Section 3). MetaICL uses the same in-context learning setup at both meta-training
and inference. At meta-training time, k + 1 examples for a task is sampled, where the last example acts as the test
example and the rest k examples act as the training examples. Inference is the same as typical in-context learning
where k labeled examples are used to make a prediction for a test input.
3.1
Meta-training
The model is meta-trained on a collection of tasks
which we call meta-training tasks. For every itera-
tion, one meta-training task is sampled, and k + 1
training examples (x1, y1), · · · , (xk+1, yk+1) are
sampled from the training examples of the cho-
sen task. We then supervise the model by feed-
ing the concatenation of x1, y1, · · · , xk, yk, xk+1
to the model as an input and train the model to gen-
erate yk+1 using a negative log likelihood objec-
tive. This simulates in-context learning at inference
where the ﬁrst k examples serve as training exam-
ples and the last (k + 1)-th example is regarded as
the test example.
3.2
Inference
For a new target task, the model is given k train-
ing examples (x1, y1), · · · , (xk, yk) as well as a
test input x. It is also given a set of candidates
C which is either a set of labels (in classiﬁcation)
or answer options (in question answering). As in
meta-training, the model takes a concatenation of
x1, y1, · · · , xk, yk, x as the input, and compute the
conditional probability of each label ci ∈C. The
label with the maximum conditional probability is
returned as a prediction.
3.3
Channel MetaICL
We introduce a noisy channel variant of MetaICL
called Channel MetaICL, following Min et al.
(2022). In the noisy channel model, P(y|x) is
reparameterized to P(x|y)P(y)
P(x)
∝P(x|y)P(y). We
follow Min et al. (2022) in using P(y) =
1
|C| and
modeling P(x|y) which allows us to use the chan-
nel approach by simply ﬂipping xi and yi. Specif-
ically, at meta-training time, the model is given
a concatenation of y1, x1, · · · , yk, xk, yk+1 and is
Meta-train
Target
Setting
# tasks # examples
Setting
# tasks
HR
61
819,200
LR
26
Classiﬁcation
43
384,022
Classiﬁcation
20
Non-Classiﬁcation
37
368,768
QA
37
486,143
QA
22
Non-QA
33
521,342
Non-NLI
55
463,579
NLI
8
Non-Paraphrase
59
496,106
Paraphrase
4
Table 2: Statistics of seven different settings. Each row
indicates meta-training/target tasks for each setting. ‘#
tasks’ in meta-training is equivalent to C in Table 1.
For all settings, there is no overlap in tasks between
meta-training and target. ‘HR’ and ‘LR’ indicate high
resource and low resource, respectively. Datasets and
the task ontology are taken from CROSSFIT (Ye et al.,
2021) and UNIFIEDQA (Khashabi et al., 2020). Full
datasets for each split are provided in Appendix A.
trained to generate xk+1. At inference, the model
computes argmaxc∈CP(x|y1, x1, · · · , yk, xk, c).
4
Experimental Setup
4.1
Datasets
We
use
a
large
collection
of
tasks
taken
from CROSSFIT (Ye et al., 2021) and UNI-
FIEDQA (Khashabi et al., 2020). We have 142
unique tasks in total, covering a variety of prob-
lems including text classiﬁcation, question answer-
ing (QA), natural language inference (NLI) and
paraphrase detection. All tasks are in English.
We experiment with seven distinct settings as
shown in Table 2, where there is no overlap be-
tween the meta-training and target tasks. The num-
ber of unique target tasks in total is 52, which is sig-
niﬁcantly larger than other relevant work (Khashabi
et al., 2020; Zhong et al., 2021; Mishra et al., 2022;
2793
Method
Meta
Target
train
train
# samples
LMs
0-shot


0
PMI 0-shot


0
Channel 0-shot


0
In-context


k
PMI In-context


k
Channel In-context


k
Meta-trained
Multi-task 0-shot


0
Channel Multi-task 0-shot


0
MetaICL (Ours)


k
Channel MetaICL (Ours)


k
Fine-tune
Fine-tune


k
Fine-tune w/ meta-train


k
Table 3: Summary of the baselines and MetaICL. ‘train’
indicates whether the model is trained with parameter
updates, and ‘# samples’ indicates the number of train-
ing examples used on a target task. Our baselines in-
clude a range of recently introduced methods (Holtz-
man et al., 2021; Zhao et al., 2021; Min et al., 2022;
Wei et al., 2022) as described in Section 4.2.
Wei et al., 2022; Sanh et al., 2022). Each target
task is either classiﬁcation or multi-choice, where
a set of candidate options (C in Table 1) is given.
HR→LR (High resource to low resource): We ex-
periment with a setting where datasets with 10,000
or more training examples are used as meta-training
tasks and the rest are used as target tasks. We think
using high resource datasets for meta-training and
low resource datasets as targets is a realistic and
practical setting for few-shot learning.
X→X (X={Classiﬁcation, QA}): We experiment
with two settings with meta-training and target
tasks sharing the task format, although with no
overlap in tasks.
Non-X→X (X={Classiﬁcation, QA, NLI, Para-
phase}): Lastly, we experiment with four settings
where meta-training tasks do not overlap with tar-
get tasks in task format and required capabilities.
These settings require the most challenging gener-
alization capacities.
Each setting has a subset of target tasks with
no domain overlap with any meta-training tasks
(e.g., ﬁnance, poem, climate or medical). We report
both on all target tasks or on target tasks with no
domain overlap only. Full details of the settings and
datasets with citations are provided in Appendix A.
[P]: Time Warner is the world’s largest media and Internet
company.
[H]: Time Warner is the world’s largest company.
Labels: entailment, not_entailment
Holtzman et al. (2021)
Input
[P] question: [H] true or false? answer:
Output {true, false}
Wei et al. (2022)
Input
[P] Based on the paragraph above, can we
conclude that [H]?
Output {yes, no}
Ours
Input
[P] [H]
Output {entailment, not_entailment}
Table 4: Example input-output pairs for an NLI task.
We show human-authored templates taken from prior
work as references.
4.2
Baselines
We compare MetaICL and Channel MetaICL with
a range of baselines, as summarized in Table 3.
0-shot: We use a pretrained LM as it is and run
zero-shot inference, following Brown et al. (2020).
In-context: We use the pretrained LM as it is and
use in-context learning by conditioning on a con-
catenation of k training examples, following Brown
et al. (2020).
PMI 0-shot, PMI In-context: We use the PMI
method from Holtzman et al. (2021); Zhao et al.
(2021) for 0-shot and In-context learning.
Channel 0-shot, Channel In-context: We use the
noisy channel model from Min et al. (2022) for
0-shot and In-context learning.
Multi-task 0-shot: We train the LM on the same
meta-training tasks without in-context learning ob-
jective, i.e., maximize P(y|x) without k other train-
ing examples, and then use zero-shot transfer on
a target task. This is equivalent to MetaICL with
k = 0. This is a typical multi-task learning ap-
proach from previous work (Khashabi et al., 2020;
Zhong et al., 2021; Wei et al., 2022).
Channel Multi-task 0-shot: We have a channel
variant of Multi-task 0-shot.
Fine-tune: We ﬁne-tune the LM on an individual
target task. This is not directly comparable to other
methods as parameter updates are required for ev-
ery target task.
Fine-tune w/ meta-train: We train the LM on
meta-training tasks ﬁrst and then further ﬁne-tuned
it on a target task. This is not directly comparable
to other methods for the same reason as above.
2794
Method
HR→LR
Class
→Class
non-Class
→Class
QA
→QA
non-QA
→QA
non-NLI
→NLI
non-Para
→Para
All target tasks
0-shot
34.8
34.2
34.2
40.2
40.2
25.5
34.2
PMI 0-shot
35.1
33.8
33.8
40.2
40.2
27.9
39.2
Channel 0-shot
36.5
37.3
37.3
38.7
38.7
33.9
39.5
In-context
38.2/35.3
37.4/33.9
37.4/33.9
40.1/38.7
40.1/38.7
34.0/28.3
33.7/33.1
PMI In-context
39.2/33.7
38.8/30.0
38.8/30.0
40.3/38.8
40.3/38.8
33.0/28.0
38.6/33.4
Channel In-context
43.1/38.5
46.3/40.3
46.3/40.3
40.8/38.1
40.8/38.1
39.9/34.8
45.4/40.9
Multi-task 0-shot
35.6
37.3
36.8
45.7
36.0
40.7
30.6
Channel Multi-task 0-shot
38.8
40.9
42.2
42.1
36.4
36.8
35.1
MetaICL
43.3/41.7
43.4/39.9
38.1/31.8
46.0/44.8
38.5/36.8
49.0/44.8
33.1/33.1
Channel MetaICL
49.1/46.8
50.7/48.0
50.6/48.1
44.9/43.5
41.9/40.5
54.6/51.9
52.2/50.3
Fine-tune
46.4/40.0
50.7/44.0
50.7/44.0
41.8/39.1
41.8/39.1
44.3/32.8
54.7/48.9
Fine-tune w/ meta-train
52.0/47.9
53.5/48.5
51.2/44.9
46.7/44.5
41.8/39.5
57.0/44.6
53.7/46.9
Target tasks in unseen domains
0-shot
32.6
32.6
32.6
45.9
45.9
33.4
38.3
PMI 0-shot
28.9
28.9
28.9
44.4
44.4
33.4
32.9
Channel 0-shot
29.1
29.1
29.1
41.6
41.6
33.1
32.6
In-context
30.6/27.5
30.6/27.5
30.6/27.5
45.6/44.7
45.6/44.7
52.0/41.3
35.8/34.1
PMI In-context
34.9/27.7
34.9/27.7
34.9/27.7
45.4/44.7
45.4/44.7
47.8/35.2
38.5/33.3
Channel In-context
39.6/33.6
39.6/33.6
39.6/33.6
44.7/40.6
44.7/40.6
40.4/35.7
44.1/36.8
Multi-task 0-shot
35.4
28.0
28.6
71.2
40.3
33.5
35.0
Channel Multi-task 0-shot
36.3
31.1
34.3
54.4
39.4
50.8
34.1
MetaICL
35.3/32.7
32.3/29.3
28.1/25.1
69.9/68.1
48.3/47.2
80.1/77.2
34.0/34.0
Channel MetaICL
47.7/44.7
41.9/37.8
48.0/45.2
57.9/56.6
47.2/45.0
62.0/57.3
51.0/49.9
Fine-tune
44.9/37.6
44.9/37.6
44.9/37.6
43.6/39.1
43.6/39.1
56.3/33.4
56.6/51.6
Fine-tune w/ meta-train
53.3/43.2
53.2/43.7
46.1/36.9
67.9/66.2
44.5/42.8
71.8/58.2
65.6/61.4
Table 5: Main results, using GPT-2 Large. Two numbers indicate the average and the worst-case performance
over different seeds used for k target training examples. Bold indicates the best average result except results from
ﬁne-tuned models that are not comparable. ‘Class’ indicates ‘Classiﬁcation’.
4.3
Evaluation
We use Macro-F12 and Accuracy as evaluation met-
rics for classiﬁcation tasks and non-classiﬁcation
tasks, respectively.
For a target task, we use k = 16 training exam-
ples, sampled uniformly at random. We relax the
assumption of perfect balance between labels on
k training examples, following Min et al. (2022).
Because in-context learning is known to have high
variance (Zhao et al., 2021; Perez et al., 2021; Lu
et al., 2021), we use 5 different sets of k training
examples. We ﬁrst compute the average and the
worst-case performance over seeds for every target
task, and then report the macro-average of them
over all target tasks.
4.4
Experiment Details
As a base LM, we use GPT-2 Large (Radford
et al., 2019) which consists of 770M parameters.3
For baselines without meta-training (raw LMs), we
also compare with GPT-J (Wang and Komatsuzaki,
2More suitable than accuracy for imbalanced classiﬁcation.
3Appendix C.2 reports performance for other LM sizes.
2021), which is the largest public causal LM at the
time of writing, consisting of 6B parameters.
Elimination of templates
Prior work uses
human-authored templates to transform the input-
output pair to a natural language sentence (Zhong
et al., 2021; Mishra et al., 2022; Wei et al., 2022;
Chen et al., 2022). They require expensive manual
effort (as 136 different templates are required for
136 tasks in this paper) and cause unstable model
performance due to many different ways of writ-
ing (Mishra et al., 2021). We eliminate templates,
using the given input (or a concatenation of in-
puts if there are multiple) and label words provided
in the original datasets.4 A comparison of input-
output schemes from prior work and our approach
is shown in Table 4.
Training details
All implementation is done in
PyTorch (Paszke et al., 2019) and Transform-
ers (Wolf et al., 2020). For meta-training, we use
4In our preliminary experiments, we explored templates
taken from prior work, but found that they do not consistently
improve few-shot performance, even when they do improve
zero-shot performance.
2795
Method
HR→LR
Class
→Class
non-Class
→Class
QA
→QA
non-QA
→QA
non-NLI
→NLI
non-Para
→Para
All target tasks
Channel In-context
43.1/38.5
46.3/40.3
46.3/40.3
40.8/38.1
40.8/38.1
39.9/34.8
45.4/40.9
MetaICL
43.3/41.7
43.4/39.9
38.1/31.8
46.0/44.8
38.5/36.8
49.0/44.8
33.1/33.1
Channel MetaICL
49.1/46.8
50.7/48.0
50.6/48.1
44.9/43.5
42.1/40.8
54.6/51.9
52.2/50.3
GPT-J Channel In-context
48.6/44.4
51.5/47.0
51.5/47.0
47.0/45.2
47.0/45.2
47.2/41.7
51.0/47.5
Target tasks in unseen domains
Channel In-context
39.6/33.6
39.6/33.6
39.6/33.6
44.7/40.6
44.7/40.6
40.4/35.7
44.1/36.8
MetaICL
35.3/32.7
32.3/29.3
28.1/25.1
69.9/68.1
48.3/47.2
80.1/77.2
34.0/34.0
Channel MetaICL
47.7/44.7
41.9/37.8
48.0/45.2
57.9/56.6
47.2/45.0
62.0/57.3
51.0/49.9
GPT-J Channel In-context
42.8/38.4
42.8/38.4
42.8/38.4
55.7/54.4
55.7/54.4
51.1/40.4
52.0/46.5
Table 6: Comparison between raw LM in-context learning (based on GPT-2 Large and GPT-J) and MetaICL
(based on GPT-2 Large). GPT-2 Large used unless otherwise speciﬁed. Two numbers indicate the average and the
worst-case performance over different seeds used for k target training examples. For raw LM baselines, Channel
In-context is reported because it is the best raw LM baseline overall across the settings; full results based on GPT-J
are provided in Appendix C.1.
up to 16,384 training examples per task. We use a
batch size of 8, learning rate of 1 × 10−5 and a se-
quence length of 1024. For multi-task 0-shot base-
lines (the baselines with no in-context learning), we
use a sequence length of 256. We train the model
for 30, 000 steps.5 To save memory during meta-
training, we use an 8-bit approximation (Dettmers
et al., 2022) of an Adam optimizer (Kingma and
Ba, 2015) and mixed precision (Micikevicius et al.,
2017). Training was done for 4.5 hours with eight
32GB GPUs. This is drastically more efﬁcient than
recent prior work, e.g., 270 hours of a 512GB TPU
in Sanh et al. (2022).
More details about preprocessing and training
can be found in Appendix B.
5
Experimental Results
5.1
Main Results
Table 5 reports the full results using GPT-2 Large,
where we compute the average and the worst-case
performance of every target task and report the
macro-average over them. The top and the bottom
respectively evaluate on all target tasks and target
tasks in unseen domains only.
Our baselines are strong
We ﬁrst discuss the re-
sults of ours baselines. Among raw LMs without
meta-training (the ﬁrst six rows of Table 5), we
observe that channel in-context baselines are the
most competitive, consistent with ﬁndings from
Min et al. (2022). We then ﬁnd that Multi-task 0-
shot baselines do not outperform the best raw LM
5We also explored training longer, but it did not improve
performance.
baseline in most settings, despite being supervised
on a large set of meta-training tasks. This some-
what contradicts ﬁndings from Wei et al. (2022);
Sanh et al. (2022).
This is likely for two rea-
sons. First, our models are much smaller than
theirs (770M vs. 11B–137B); in fact, Wei et al.
(2022) reports Multi-task 0-shot starts to be better
than raw LMs only when the model size is 68B or
larger. Second, we compare with much stronger
channel baselines which they did not; Multi-task
0-shot outperforms non-channel LM baselines but
not channel LM baselines.
MetaICL outperforms baselines
MetaICL and
Channel MetaICL consistently outperform a range
of strong baselines. In particular, Channel MetaICL
achieves the best performance in 6 out of 7 set-
tings.
Gains are particularly signiﬁcant in the
HR→LR, non-NLI→NLI and non-Para→Para set-
tings (6–15% absolute). This is noteworthy be-
cause HR→LR targets the common low-resource
case where new tasks have very few labeled ex-
amples, and the other two represent large data dis-
tribution shifts where the test tasks are relatively
different from the meta-training tasks. This demon-
strates that MetaICL can infer the semantics of new
tasks in context even when there are no closely
related training tasks.
While MetaICL signiﬁcantly outperforms base-
lines in most settings, it only marginally outper-
forms Multi-task 0-shot in the QA→QA setting,
as an exception. This is likely because the meta-
training and target tasks are relatively similar, al-
lowing the Multi-task 0-shot baseline to achieve
very strong performance.
Nonetheless, perfor-
2796
25
30
35
40
45
50
0
4
8
16
32
Channel In-context (all)
Channel MetaICL (all)
Channel In-context (unseen)
Channel MetaICL (unseen)
Figure 1: Ablation on the number of training examples
(k) in the HR→LR setting. k = 0 is equivalent to the
zero-shot methods.
mance of Multi-task 0-shot in QA signiﬁcantly
drops when the model is trained on non-QA tasks,
while performance of MetaICL drops substantially
less.
Gains are larger on unseen domains
Gains
over Multi-task 0-shot are more signiﬁcant on tar-
get tasks in unseen domains. In particular, Multi-
task 0-shot is generally less competitive compared
to raw LM baselines, likely because they require
more challenging generalization. MetaICL suffers
less from this problem and is consistently better or
comparable to raw LM baselines across all settings.
Comparison to ﬁne-tuning
MetaICL matches
or sometimes even outperforms ﬁne-tuned mod-
els without meta-training.
This is a promising
signal, given that no prior work has shown mod-
els with no parameter updates on the target can
match or outperform supervised models. Nonethe-
less, ﬁne-tuning with meta-training exceeds both
MetaICL and ﬁne-tuning without meta-training, be-
cause meta-training helps in supervised learning as
it does in in-context learning. This indicates that
there is still room for improvement in methods that
allow learning without parameter updates .
Comparison to GPT-J
In Table 6, we compare
GPT-2 Large based models with raw LM baselines
based on GPT-J which consists of 6B parameters.
MetaICL, despite being 8x smaller, outperforms or
matches GPT-J baselines.
5.2
Ablations
Varying number of training examples
We vary
the number of training examples (k) from 0, 4, 8,
16 to 32. In-context learning with k = 0 is equiv-
alent to the zero-shot method. Results are shown
in Figure 1. Increasing k generally helps across all
30
35
40
45
50
7
15
30
61
# meta-training tasks
Multi-task 0-shot
Channel Multi-task 0-shot
MetaICL
Channel MetaICL
50
45
40
35
30
# meta-training tasks
7
15
30
61
30
35
40
45
50
50
45
40
35
30
Figure 2: Ablation on the number of meta-training
tasks ({7, 15, 30, 61}). The graph of the average (top)
and the box chart (bottom) over different meta-training
sets using 10 different random seeds (except for 61).
models, and Channel MetaICL outperforms the raw
in-context learning over all values of k. We addi-
tionally ﬁnd that the performance tends to saturate
when k is closer to 16, likely because the sequence
length limit of the language model makes it hard to
encode many training examples.
Number of meta-training tasks
To see the im-
pact of the number of meta-training tasks, we sub-
sample {7, 15, 30} meta-training tasks out of 61 in
the HR→LR setting. For each, we use ten different
random seeds to additionally see the impact of the
choice of meta-training tasks.
Figure 2 reports the results. On average, perfor-
mance generally increases as the number of tasks
increase, which is consistent with results in Mishra
et al. (2022); Wei et al. (2022). Across different
numbers of meta-training tasks, Channel MetaICL
consistently outperforms other models. Nonethe-
less, there is nonnegligible variance across different
choices of meta-training (the bottom of Figure 2),
indicating that a choice of meta-training gives sub-
stantial impact in performance.
Diversity in meta-training tasks
We hypothe-
size that the diversity in meta-training tasks may
impact performance of MetaICL. To verify this hy-
pothesis, we create two settings by subsampling 13
2797
Method
Diverse
No Diverse
0-shot
34.9
PMI 0-shot
34.8
Channel 0-shot
36.8
In-context
38.2/35.4
PMI In-context
38.9/33.3
Channel In-context
42.9/38.5
Multi-task 0-shot
35.2
29.9
Channel Multi-task 0-shot
41.6
38.3
MetaICL
45.6/43.4
38.8/35.4
Channel MetaICL
47.2/44.7
45.3/42.6
Table 7: Ablation on the diversity of meta-training
tasks in the HR→LR setting. For both settings, the
number of meta-training tasks is 13, and the number of
target tasks is 26 as in the original HR→LR setting. A
full list of meta-training tasks is shown in Appendix A.
out of 61 meta-training datasets in the HR→LR set-
ting. One setting is diverse in their task formats and
required capacities: QA, NLI, relation extraction,
sentiment analysis, topic classiﬁcation, hate speech
detection and more. The other setting is less di-
verse, including tasks related to sentiment analysis,
topic classiﬁcation and hate speech detection only.
A full list of datasets is reported in Appendix A.
Using these two settings, we compare multi-task
zero-shot transfer baselines and MetaICL.
Results are reported in Table 7. We ﬁnd that
MetaICL with a diverse set outperforms MetaICL
with a non-diverse set by a substantial margin. This
shows that diversity among meta-training tasks
is one of substantial factors for the success of
MetaICL.
In Appendix C.3, we include ablations that pro-
vide more insights on the choice of meta-training
tasks, such as (1) high quality data with diverse
domains tend to help (e.g., GLUE family (Wang
et al., 2018)) and (2) adversarially collected data
tends to be unhelpful. However, more systematic
studies on how to choose the best meta-training
tasks and how they relate to particular target tasks
should be done, which we leave for future work.
Are instructions necessary?
Most recent work
has used human-written natural instructions for
zero- or few-shot learning (Mishra et al., 2022; Wei
et al., 2022; Sanh et al., 2022). While we argue for
not using instructions to avoid manual engineering
and high variance, we also ask: are instructions
still useful with MetaICL? On one hand, learning to
condition on k examples may remove the necessity
of instructions. On the other hand, instructions may
still be complementary and provide the model with
Method
w/o Instruct
w/ Instruct
# instruct/task
0
1
8.3
0-shot
33.3
34.2
PMI 0-shot
34.6
27.8
Channel 0-shot
32.5
30.6
In-context
34.5/31.5
45.2/42.3
PMI In-context
37.7/32.7
41.9/37.6
Channel In-context
39.0/35.4
39.6/35.3
MT 0-shot
35.7
32.6
37.1
Channel MT 0-shot
36.7
30.6
36.0
MetaICL
40.4/37.7
42.6/41.0
43.2/41.0
Channel MetaICL
42.2/40.0
45.3/43.9
46.9/44.2
Table 8: Ablation on the impact of natural instruc-
tions. ‘w/ Instruct’ uses instructions from Sanh et al.
(2022), either one per meta-training task or all avail-
able ones; ‘w/o Instruct’ does not use instructions, as
in all of our other experiments. ‘# instruct/task’ indi-
cates the number of instructions per meta-training task
on average. ‘MT 0-shot’ indicates ‘Multi-task 0-shot’
baselines. Both settings have the same meta-training
and target tasks, 32 and 12, respectively. A full list of
tasks is shown in Appendix A.
extra useful infomration.
We aim to answer this question by using 32 meta-
training tasks and 12 target tasks from the HR→LR
setting for which human-written instructions are
available in Sanh et al. (2022).6 We have two vari-
ants: (a) using one instruction per meta-training
task, and (b) using all available instructions includ-
ing 267 instructions in total (8.3 per meta-training
task) which Sanh et al. (2022) found to be better
than (a). We then compare MetaICL and a range of
baselines with and without instructions.
Results are reported Table 8. As in Wei et al.
(2022) and Sanh et al. (2022), Multi-task 0-shot
outperforms the raw-LM 0-shot baseline. How-
ever, MetaICL with no instructions is better than
Multi-task 0-shot with instructions. Furthermore,
MetaICL achieves further improvements when in-
structions are jointly used, signiﬁcantly outperform-
ing all baselines. In fact, when increasing the num-
ber of instructions per task from 0, 1 to 8.3, per-
formance of MetaICL improves much more than
performance of Multi-task 0-shot does. To sum-
marize, (1) learning to in-context learn (MetaICL)
outperforms learning to learn from instructions; (2)
MetaICL and using instructions are largely comple-
mentary, and (3) MetaICL actually beneﬁts more
from using instructions than Multi-task 0-shot does.
Importantly, Channel MetaICL trained on avail-
6github.com/bigscience-workshop/
promptsource
2798
able tasks and instructions still achieves lower
performance than Channel MetaICL without tem-
plates/instructions (46.9 from Table 8 vs.
49.1
from Table 5). This is likely because the model
with instructions was trained with less meta-
training tasks, which was unavoidable since in-
structions are only available on 32 out of 61 meta-
training tasks. This supports our earlier choice
of not using human-written templates/instructions,
since writing templates and instructions for every
task requires extensive effort.
It is worth noting that, it is nonetheless difﬁcult
to make direct comparisons with Wei et al. (2022)
and Sanh et al. (2022) because there are many mov-
ing components: size of LMs, types of LMs (e.g.,
causal LM vs. masked LM), splits between meta-
training and target tasks, and more.
6
Conclusion
In this paper, we introduced MetaICL, a new few-
shot learning method where an LM is meta-trained
to learn to in-context learn, i.e. condition on train-
ing examples to recover the task and make pre-
dictions. We experiment with a large, diverse col-
lection of tasks, consisting of 142 unique tasks in
total and 52 unique target tasks, using seven dif-
ferent settings. MetaICL outperforms a range of
strong baselines including in-context learning with-
out meta-training and multi-task learning followed
by zero-shot transfer, and outperforms or matches
8x bigger models. We identify ingredients for suc-
cess of MetaICL such as the number and diversity
of meta-training tasks. We also demonstrate that,
while MetaICL is better than recent work using
natural instructions, they are complementary and
the best performance is achieved by integrating
MetaICL with instructions.
Limitation & Future work
Our work is limited
in multiple dimensions. First, in-context learn-
ing approaches in general requires much longer
context at both meta-training and inference due to
feeding the concatenation of the training data, thus
being less efﬁcient compared to baselines that do
not use in-context learning. Second, our work ex-
periment with a casual language model with mod-
est size (GPT-2 Large, 770M parameters). Future
work may investigate extending our approach to a
masked language model and a larger model. Third,
our experiments focus on classiﬁcation and multi-
choice tasks where a set of candidate options is
given. Future work may study applying our ap-
proach for a wider range of tasks including free-
form generation. Other avenues for future work
include further improving MetaICL to outperform
supervised models with meta-training, identiﬁca-
tion of which meta-training tasks are helpful on
target tasks, and how to better combine human-
written instructions and MetaICL.
Acknowledgements
We thank Ari Holtzman and Victoria Lin for com-
ments and discussions, and Tim Dettmers for help
with experiments. This research was supported
by NSF IIS-2044660, ONR N00014-18-1-2826,
an Allen Distinguished Investigator Award, and a
Sloan Fellowship.
References
Armen Aghajanyan, Anchit Gupta, Akshat Shrivas-
tava, Xilun Chen, Luke Zettlemoyer, and Sonal
Gupta. 2021.
Muppet:
Massive multi-task rep-
resentations with pre-ﬁnetuning.
arXiv preprint
arXiv:2101.11038.
Tiago A. Almeida, José María G. Hidalgo, and Akebo
Yamakami. 2011. Contributions to the study of sms
spam ﬁltering: New collection and results. In Pro-
ceedings of the 11th ACM Symposium on Document
Engineering.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Proceedings of the sec-
ond PASCAL challenges workshop on recognising
textual entailment.
Francesco Barbieri, Jose Camacho-Collados, Luis Es-
pinosa Anke, and Leonardo Neves. 2020. TweetE-
val: Uniﬁed benchmark and comparative evaluation
for tweet classiﬁcation. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2020.
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo
Giampiccolo. 2009. The ﬁfth pascal recognizing tex-
tual entailment challenge. In TAC.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013.
Semantic parsing on Freebase from
question-answer pairs. In EMNLP.
Chandra Bhagavatula, Ronan Le Bras, Chaitanya
Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-
nah Rashkin, Doug Downey, Wen tau Yih, and Yejin
Choi. 2020. Abductive commonsense reasoning. In
ICLR.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-
feng Gao, and Yejin Choi. 2020. Piqa: Reasoning
about physical commonsense in natural language. In
AAAI.
2799
Michael Boratko, Xiang Li, Tim O’Gorman, Rajarshi
Das, Dan Le, and Andrew McCallum. 2020. Pro-
toQA: A question answering dataset for prototypical
common-sense reasoning. In EMNLP.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,
Jared
D
Kaplan,
Prafulla
Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
NeurIPS.
Nicholas
Carlini,
Florian
Tramer,
Eric
Wallace,
Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ul-
far Erlingsson, et al. 2021. Extracting training data
from large language models. In 30th USENIX Secu-
rity Symposium (USENIX Security 21).
Ankush Chatterjee, Kedhar Nath Narahari, Meghana
Joshi, and Puneet Agrawal. 2019.
SemEval-2019
task 3: EmoContext contextual emotion detection in
text. In Proceedings of the 13th International Work-
shop on Semantic Evaluation.
Michael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-
nandez, and Doug Downey. 2019.
CODAH: An
adversarially-authored question answering dataset
for common sense. In Proceedings of the 3rd Work-
shop on Evaluating Vector Space Representations
for NLP.
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020. Tabfact: A large-scale
dataset for table-based fact veriﬁcation. In ICLR.
Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,
and He He. 2022. Meta-learning via language model
in-context tuning. In ACL.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. BoolQ: Exploring the surprising
difﬁculty of natural yes/no questions.
In NAACL-
HLT.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005.
The pascal recognising textual entailment
challenge. In Machine Learning Challenges Work-
shop.
Pradeep Dasigi,
Nelson F. Liu,
Ana Marasovi´c,
Noah A. Smith, and Matt Gardner. 2019. Quoref:
A reading comprehension dataset with questions re-
quiring coreferential reasoning. In EMNLP.
Thomas Davidson, Dana Warmsley, Michael Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language. In
Proceedings of the 11th International AAAI Confer-
ence on Web and Social Media.
Ona de Gibert, Naiara Perez, Aitor García-Pablos, and
Montse Cuadros. 2018. Hate Speech Dataset from a
White Supremacy Forum. In Proceedings of the 2nd
Workshop on Abusive Language Online (ALW2).
Marie-Catherine de Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
Proceedings of Sinn und Bedeutung.
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke
Zettlemoyer. 2022. 8-bit optimizers via block-wise
quantization. In ICLR.
T. Diggelmann, Jordan L. Boyd-Graber, Jannis Bu-
lian, Massimiliano Ciaramita, and Markus Leippold.
2020. Climate-fever: A dataset for veriﬁcation of
real-world climate claims. ArXiv.
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel
Stanovsky, Sameer Singh, and Matt Gardner. 2019.
DROP: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In NAACL.
Matthew Dunn, Levent Sagun, Mike Higgins, V. U.
Güney, Volkan Cirik, and Kyunghyun Cho. 2017.
Searchqa:
A new q&a dataset augmented with
context from a search engine.
arXiv preprint
arXiv:1704.05179.
Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci,
Christophe Gravier,
Jonathon Hare,
Frederique
Laforest, and Elena Simperl. 2018. T-REx: A large
scale alignment of natural language with knowledge
base triples. In LREC.
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi–task learning. In Proceedings of
the tenth ACM SIGKDD international conference on
Knowledge discovery and data mining.
Manaal Faruqui and Dipanjan Das. 2018. Identifying
well-formed natural language questions. In EMNLP.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. In ICML.
2800
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL workshop on textual entailment and
paraphrasing.
Andrew Gordon, Zornitsa Kozareva, and Melissa
Roemmele. 2012.
SemEval-2012 task 7: Choice
of plausible alternatives: An evaluation of common-
sense causal reasoning. In The First Joint Confer-
ence on Lexical and Computational Semantics (Se-
mEval).
Harsha Gurulingappa, Abdul Mateen Rajput, Angus
Roberts, Juliane Fluck, Martin Hofmann-Apitius,
and Luca Toldo. 2012.
Development of a bench-
mark corpus to support the automatic extraction of
drug-related adverse effects from medical case re-
ports. Journal of Biomedical Informatics.
Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.
Question-answer driven semantic role labeling: Us-
ing natural language to annotate natural language. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named en-
tities in text. In EMNLP.
Ari Holtzman, Peter West, Vered Schwartz, Yejin Choi,
and Luke Zettlemoyer. 2021. Surface form compe-
tition: Why the highest probability answer isn’t al-
ways right. In EMNLP.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-
Yew Lin, and Deepak Ravichandran. 2001. Toward
semantics-based answer pinpointing.
In Proceed-
ings of the First International Conference on Human
Language Technology Research.
Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and
Yejin Choi. 2019.
Cosmos QA: Machine reading
comprehension with contextual commonsense rea-
soning. In EMNLP.
Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-
baseQA: A new factoid QA data set matching
trivia-style question-answer pairs with Freebase. In
NAACL-HLT.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018.
Looking
beyond the surface: A challenge set for reading
comprehension over multiple sentences. In NAACL-
HLT.
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. 2020.
UniﬁedQA: Crossing for-
mat boundaries with a single qa system. In Findings
of EMNLP.
Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2019.
QASC: A
dataset for question answering via sentence compo-
sition. In AAAI.
Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2020.
Qasc:
A
dataset for question answering via sentence compo-
sition. In AAAI.
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
Scitail: A textual entailment dataset from science
question answering. In AAAI.
Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.
Tomás Kociský, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, Gábor Melis, and
Edward Grefenstette. 2018. The narrativeqa reading
comprehension challenge. TACL.
Neema Kotonya and Francesca Toni. 2020.
Ex-
plainable automated fact-checking for public health
claims. In EMNLP.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur P. Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: A benchmark for question an-
swering research. TACL.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard H. Hovy. 2017.
RACE: Large-scale
reading comprehension dataset from examinations.
In EMNLP.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
D. Kontokostas, Pablo N. Mendes, Sebastian Hell-
mann, M. Morsey, Patrick van Kleef, S. Auer, and
C. Bizer. 2015. Dbpedia - a large-scale, multilingual
knowledge base extracted from wikipedia. Semantic
Web.
Hector J. Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Proceedings of the Thirteenth International Confer-
ence on Principles of Knowledge Representation
and Reasoning.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In CoNLL.
Xin Li and Dan Roth. 2002. Learning question classi-
ﬁers. In COLING.
Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-
ang Ren. 2020. Birds have four legs?! NumerSense:
Probing Numerical Commonsense Knowledge of
Pre-Trained Language Models. In EMNLP.
2801
Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-
ner. 2019. Reasoning over paragraph effects in situ-
ations. In Proceedings of the 2nd Workshop on Ma-
chine Reading for Question Answering.
Annie Louis, Dan Roth, and Filip Radlinski. 2020. “I’d
rather just go to bed”: Understanding indirect an-
swers. In EMNLP.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian
Riedel, and Pontus Stenetorp. 2021.
Fantastically
ordered prompts and where to ﬁnd them: Overcom-
ing few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.
Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-
lenius, and Pyry Takala. 2014. Good debt or bad
debt: Detecting semantic orientations in economic
texts. J. Assoc. Inf. Sci. Technol.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014. A SICK cure for the evaluation of com-
positional distributional semantic models. In LREC.
Binny Mathew,
Punyajoy Saha,
Seid Muhie Yi-
mam, Chris Biemann, Pawan Goyal, and Ani-
mesh Mukherjee. 2020. Hatexplain: A benchmark
dataset for explainable hate speech detection. arXiv
preprint arXiv:2012.10289.
Julian McAuley and J. Leskovec. 2013. Hidden factors
and hidden topics: understanding rating dimensions
with review text. Proceedings of the 7th ACM con-
ference on Recommender systems.
Clara H. McCreery, Namit Katariya, Anitha Kannan,
Manish Chablani, and Xavier Amatriain. 2020. Ef-
fective transfer learning for identifying similar ques-
tions: Matching user questions to covid-19 faqs.
In Proceedings of the 26th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data
Mining.
Paulius Micikevicius, Sharan Narang, Jonah Alben,
Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg,
Michael Houston,
Oleksii Kuchaiev,
Ganesh Venkatesh, et al. 2017.
Mixed precision
training. In ICLR.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. In EMNLP.
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2022. Noisy channel language
model prompting for few-shot text classiﬁcation. In
ACL.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin
Choi, and Hannaneh Hajishirzi. 2021.
Reframing
instructional prompts to gptk’s language.
arXiv
preprint arXiv:2109.07830.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
In ACL.
Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,
and Grigorios Tsoumakas. 2020.
Ethos: an on-
line hate speech detection dataset.
arXiv preprint
arXiv:2006.08328.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020.
CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In EMNLP.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012.
Annotated Gigaword.
In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction (AKBC-WEKEX).
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In EMNLP.
Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-
versarial NLI: A new benchmark for natural lan-
guage understanding. In ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In ACL.
Dimitris Pappas, Petros Stavropoulos, Ion Androut-
sopoulos, and Ryan McDonald. 2020. BioMRC: A
dataset for biomedical machine reading comprehen-
sion. In Proceedings of the 19th SIGBioMed Work-
shop on Biomedical Language Processing.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019.
Pytorch:
An imperative
style, high-performance deep learning library.
In
NeurIPS.
Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.
True few-shot learning with language models.
In
NeurIPS.
Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim
Rocktäschel, Yuxiang Wu, Alexander H. Miller, and
Sebastian Riedel. 2020.
How context affects lan-
guage models’ factual predictions.
In Automated
Knowledge Base Construction.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In EMNLP.
2802
Mohammad
Taher
Pilehvar
and
Jose
Camacho-
Collados. 2019. WiC: the word-in-context dataset
for evaluating context-sensitive meaning representa-
tions. In NAACL-HLT.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
blog.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for squad. In ACL.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In EMNLP.
Matthew Richardson, Christopher J. C. Burges, and
Erin Renshaw. 2013. Mctest: A challenge dataset
for the open-domain machine comprehension of text.
In EMNLP.
Anna Rogers, Olga Kovaleva, Matthew Downey, and
Anna Rumshisky. 2020. Getting closer to ai com-
plete question answering: A set of prerequisite real
tasks. In AAAI.
Sebastian Ruder. 2017.
An overview of multi-task
learning in deep neural networks.
arXiv preprint
arXiv:1706.05098.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-
vatula, and Yejin Choi. 2020a. WINOGRANDE: an
adversarial winograd schema challenge at scale. In
AAAI.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2020b. Winogrande: An adver-
sarial winograd schema challenge at scale. In AAAI.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun
Raja, Manan Dey, M Saiful Bari, Canwen Xu, Ur-
mish Thakker, Shanya Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak,
Debajyoti Datta, Jonathan Chang, Mike Tian-Jian
Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden,
Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-
heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-
son Alan Fries, Ryan Teehan, Stella Biderman, Leo
Gao, Tali Bers, Thomas Wolf, and Alexander M.
Rush. 2022.
Multitask prompted training enables
zero-shot task generalization. In ICLR.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019a. Social IQa: Com-
monsense reasoning about social interactions.
In
EMNLP.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019b. Social iqa: Com-
monsense reasoning about social interactions.
In
EMNLP-IJCNLP.
Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,
Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-
textualized affect representations for emotion recog-
nition. In EMNLP.
Emily Sheng and David Uthus. 2020. Investigating so-
cietal biases in a poetry composition system. In Pro-
ceedings of the Second Workshop on Gender Bias in
Natural Language Processing.
Damien Sileo, Tim Van De Cruys, Camille Pradel, and
Philippe Muller. 2019.
Mining discourse markers
for unsupervised sentence representation learning.
In NAACL-HLT.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013.
Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP.
Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,
and Claire Cardie. 2019. DREAM: A challenge data
set and models for dialogue-based reading compre-
hension. TACL.
Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau
Yih, and Ashish Sabharwal. 2019a.
Quarel:
A
dataset and models for answering questions about
qualitative relationships. In AAAI.
Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter
Clark. 2019b. QuaRTz: An open-domain dataset of
qualitative relationship questions. In EMNLP.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. Commonsenseqa: A ques-
tion answering challenge targeting commonsense
knowledge. In NAACL-HLT.
Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-
ter Clark, and Antoine Bosselut. 2019. WIQA: A
dataset for “what if...” reasoning over procedural
text. In EMNLP.
James
Thorne,
Andreas
Vlachos,
Christos
Christodoulopoulos,
and
Arpit
Mittal.
2018.
FEVER: a large-scale dataset for fact extraction and
VERiﬁcation. In NAACL-HLT.
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2017. Newsqa: A machine compre-
hension dataset. In Rep4NLP@ACL.
Sowmya Vajjala and Ivana Luˇci´c. 2018.
On-
eStopEnglish corpus: A new corpus for automatic
readability assessment and text simpliﬁcation.
In
Proceedings of the Thirteenth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions.
Ricardo Vilalta and Youssef Drissi. 2002. A perspec-
tive view and survey of meta-learning. Artiﬁcial in-
telligence review.
2803
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis plat-
form for natural language understanding. In Black-
boxNLP Workshop: Analyzing and Interpreting Neu-
ral Networks for NLP.
Ben Wang and Aran Komatsuzaki. 2021.
GPT-
J-6B:
A
6
Billion
Parameter
Autoregressive
Language Model.
https://github.com/
kingoflolz/mesh-transformer-jax.
William Yang Wang. 2017. “liar, liar pants on ﬁre”: A
new benchmark dataset for fake news detection. In
ACL.
Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-
hananey, Wei Peng, Sheng-Fu Wang, and Samuel R.
Bowman. 2020. Blimp: The benchmark of linguis-
tic minimal pairs for english. TACL.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
TACL.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2022. Finetuned lan-
guage models are zero-shot learners. In ICLR.
Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.
Crowdsourcing multiple choice science questions.
In Proceedings of the 3rd Workshop on Noisy User-
generated Text.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In NAACL-
HLT.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In EMNLP: System Demonstrations.
Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulka-
rni, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and
William Yang Wang. 2019. TWEETQA: A social
media focused question answering dataset. In ACL.
Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
WikiQA: A challenge dataset for open-domain ques-
tion answering. In EMNLP.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D. Manning. 2018. HotpotQA: A dataset for
diverse, explainable multi-hop question answering.
In EMNLP.
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.
Crossﬁt: A few-shot learning challenge for cross-
task generalization in nlp. In EMNLP.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao,
Shanelle Roman,
Zilin Zhang,
and Dragomir Radev. 2018.
Spider:
A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-SQL task. In
EMNLP.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and
Yejin Choi. 2018. SWAG: A large-scale adversar-
ial dataset for grounded commonsense inference. In
EMNLP.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can a
machine really ﬁnish your sentence? In ACL.
Sheng Zhang, X. Liu, J. Liu, Jianfeng Gao, Kevin
Duh, and Benjamin Van Durme. 2018.
Record:
Bridging the gap between human and machine com-
monsense reading comprehension.
arXiv preprint
arXiv:1810.12885.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. In NeurIPS.
Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
PAWS: Paraphrase adversaries from word scram-
bling. In NAACL-HLT.
Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In
ICML.
Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.
2021.
Adapting language models for zero-shot
learning by meta-tuning on dataset and prompt col-
lections. In Findings of EMNLP.
Victor Zhong, Caiming Xiong, and Richard Socher.
2017.
Seq2sql:
Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv:1709.00103.
Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan
Roth. 2019. “going on a vacation” takes longer than
“going for a walk”: A study of temporal common-
sense understanding. In EMNLP.
2804
A
Dataset List
Table 14 and Table 15 report a list of datasets used
in the settings detailed in Section 4.1. The ﬁrst 10
rows are for settings described in Section 4.1; the
next two rows are for settings used for ablations
on the diversity of meta-training tasks (Table 7 of
Section 5.2); the last two rows are for settings used
for ablations on using natural instructions (Table 8
of Section 5.2). Bold datasets are target datasets
with no overlap in domain with meta-training tasks.
All datasets are taken from CROSSFIT (Ye et al.,
2021) (except we exclude datasets that are unavail-
able from their repository7 or the scope is notably
different from other tasks, e.g., solving math prob-
lems or breaking down compositional questions)
and UNIFIEDQA (Khashabi et al., 2020).
How meta-training/target splits are determined
The HR→LR setting is created based on the train-
ing data size as described in Section 4.1. Settings
involving Classiﬁcation, NLI and Paraphrase are
taken from CROSSFIT. Settings involving QA are
created by combining QA datasets from CROSSFIT
and datasets from UNIFIEDQA.
Statistics are reported in Table 2 and Table 9.
The number of tasks is the largest among recent
related work: we have 142 unique tasks, while
Khashabi et al. (2020), Zhong et al. (2021), Mishra
et al. (2022), Wei et al. (2022) and Sanh et al.
(2022) use 32, 62, 61, 42 and 62 tasks, respec-
tively. References for all datasets are provided in
Table 15. Data and splits are available at github.
com/facebookresearch/MetaICL .
B
Implementation Details
Preprocessing details
For all models with meta-
training and the raw GPT-J, we separate the input
and the output with one newline (\n), and separate
between examples with three newlines. For the raw
GPT-2, we use spaces instead of newlines. This
choice was made in order to report the best baseline
performance we were able to achieve: when raw
LMs are used, GPT-2 is signiﬁcantly better with
spaces than with newlines, and GPT-J is signiﬁ-
cantly better with newlines than with spaces.8 We
note that MetaICL is less sensitive to these format-
7github.com/INK-USC/CrossFit
8For example, in the HR→LR setting, the raw GPT-2 is
about 4% better with spaces then with newlines, and the raw
GPT-J is about 5% better with spaces and then with newlines
(all with the channel in-context learning method).
Setting
Input
Output
Mean Median Mean Median
Meta-training tasks
HR
81.7
73
2.8
2
Classiﬁcation
45.8
41
1.1
1
Non-Classiﬁcation
77.7
69
4.2
3
QA
142.6
137
2.7
2
Non-QA
68.7
56
2.3
2
Non-NLI
44.3
39
1.1
1
Non-Paraphrase
45.0
39
1.1
1
Target tasks
LR
29.7
25
1.9
1
Classiﬁcation
44.9
38
1.0
1
QA
74.4
69
4.6
4
NLI
45.4
41
1.0
1
Paraphrase
42.2
41
1.0
1
Table 9: Length statistics of tasks used in different set-
tings, before any truncation. We compute the mean and
the median of each task, and report the macro-average
over all tasks for each setting.
ting differences, having less than 2% differences
between using spaces and using newlines.
When the concatenation of k examples is too
long, we truncate each example to have at most
256 tokens, and truncate the earlier tokens of the
concatenation so that the LM sees the recent tokens.
Additionally, for extractive question answering
datasets as meta-training tasks, the input passage
is truncated with a guarantee that the groundtruth
answer is included in the input passage. We do not
do this truncation for target datasets.
Comparison with baselines in training and in-
ference cost
Although being trained for the same
global steps (30,000 steps), it takes 3 hours to train
Multi-task 0-shot baselines (in contrast to 4.5 hours
for MetaICL), likely because the sequence length
is 4x shorter. At inference, Multi-task 0-shot base-
lines are roughly 4x more efﬁcient, also because
the sequence length is 4x shorter.9 We did not con-
trol for the training time and the inference time for
comparison since both models are efﬁcient enough.
Ablations in using instructions
When we
choose one instruction per task at meta-training
tasks, we choose one by (1) ﬁrst excluding the
instruction if its name contains no_option,
(2) then taking the instruction which name con-
tains multiple_choice, most_correct or
9Let L be the sequence length, the memory requirement for
attention layers and feed-forward layers are O(L2) and O(L),
respectively. In practice, feed-forward layers are responsible
for most memory usage when the size of the transformers is
large, thus empirical memory usage tends to be linear to L.
2805
Method
HR→LR
{Class,non-Class}
→Class
{QA,non-QA}
→QA
non-NLI
→NLI
non-Para
→Para
All tasks
0-shot
31.5
31.5
45.6
25.7
30.0
PMI 0-shot
36.9
30.2
44.3
30.2
37.6
Channel 0-shot
39.7
41.5
42.1
36.2
45.0
In-context
43.8/39.1
43.6/34.3
50.8/48.3
35.0/27.6
41.3/33.2
PMI In-context
43.0/37.4
44.8/36.6
48.8/46.9
31.5/26.0
38.4/33.6
Channel In-context
48.6/44.4
51.5/47.0
47.0/45.2
47.2/41.7
51.0/47.5
Target tasks in unseen domains
0-shot
31.2
31.2
47.5
33.5
34.1
PMI 0-shot
25.2
25.2
43.8
36.1
34.4
Channel 0-shot
37.2
37.2
46.9
53.4
54.7
In-context
33.1/25.4
33.1/25.4
57.4/53.1
46.7/36.1
34.1/34.1
PMI In-context
35.4/28.2
35.4/28.2
54.5/50.9
33.9/33.9
32.5/32.4
Channel In-context
42.8/38.4
42.8/38.4
55.7/54.4
51.1/40.4
52.0/46.5
Table 10: Performance of raw LM baselines using GPT-J (6B). Two numbers indicate the average and the worst-
case accuracy over different seeds used for k target training examples. ‘Class’ indicate ‘Classiﬁcation’.
All tasks
Target tasks in unseen domains
S
M
L
XL
S
M
L
XL
Channel In-context
41.5/37.4
42.2/37.7
43.1/38.5
43.5/39.9
40.9/35.9
38.8/34.7
39.6/33.6
40.0/37.2
MT 0-shot
35.4
36.4
35.6
-
34.9
32.2
35.4
-
Channel MT 0-shot
40.4
37.9
38.8
-
33.8
35.9
36.3
-
MetaICL
39.7/36.2
40.3/36.4
43.3/41.7
-
36.9/32.6
38.1/35.0
35.3/32.7
-
Channel MetaICL
46.2/43.1
44.3/41.5
49.1/46.8
-
46.9/42.6
43.1/39.8
47.7/44.7
-
Table 11: Ablation on the size of the LM on the HR→LR setting. We use small, medium, large, and XL variants
of GPT-2. We were unable to meta-train the XL variant due to memory limit.
most_suitable if there are any, and (3) if not,
then randomly sampling one. We choose one in-
struction per target task at test time using the same
process. This is different Sanh et al. (2022) where
the median of the performance over all instructions
is reported. We think our choice better reﬂects the
real use-case scenario—choosing one instruction
that looks the most reasonable to human.
C
Additional Results & Analyses
C.1
GPT-J results
Table 10 reports the full results of raw LM baselines
based on GPT-J, consisting of 6B parameters. See
Section 5.1 for discussion.
C.2
Varying LM sizes
We vary the size of the GPT-2 models—small,
medium, large, and XL—with 124M, 355M, 774M,
and 1.5B parameters, respectively. Results are re-
ported in Table 11. We ﬁnd that (1) increasing
the model size generally helps, (2) for all model
sizes, Channel MetaICL signiﬁcantly outperforms
baselines, and (3) MetaICL enables a much smaller
model to outperform a bigger model, e.g., Chan-
nel MetaICL based on GPT-2 Small outperforms
the GPT-2 XL baseline that is 12x bigger (46.2 vs.
43.5).
C.3
Which meta-training tasks are more
helpful?
Based on large variance across different choices of
meta-training (Figure 2 of Section 5.2), we think
certain tasks are more helpful for meta-training
than other tasks. In this context, we create 50
sets of seven meta-training tasks using 50 different
random seeds. We then measure the correlation
between tasks/task pairs/task triples and average
performance of Channel MetaICL when the task is
included in the meta-training tasks.
Table 12 reports the result. We ﬁrst ﬁnd that high
quality datasets with diverse domain like GLUE
family (Wang et al., 2018) are often helpful. We
also ﬁnd that datasets that are collected adversar-
ially (e.g. paws, art) or are notably dissimilar
from all other tasks (e.g. wikisql that requires
semantic parsing) are often unhelpful. Nonethe-
less, we were not able to ﬁnd good explanations for
other cases, e.g., many sentiment analysis datasets
being particularly helpful even though only 3 out
2806
Single task
Helpful:
tweet_eval-offensive, glue-sst2, glue-mnli, wino_grande, kilt_hotpotqa
Unhelpful: race-middle, cosmos_qa, dbpedia_14, gigaword, wikisql
Task pair
Helpful:
(yelp_review_full, glue-mnli), (yelp_review_full, wino_grande), (hateexplain, glue-sst2), (hateexplain, glue-
mnli), (hateexplain, glue-qqp),
Unhelpful: (paws, dbpedia_14), (paws, art), (paws, cosmos_qa), (cosmos_qa, dbpedia_14), (quail, art)
Task triple
Helpful
(yelp_review_full, glue-qqp, glue-mnli), (yelp_review_full, glue-sst2, glue-mnli), (yelp_review_full, hateex-
plain, glue-mnli), (yelp_review_full, hateexplain, qqp), (yelp_review_full, hate_speech_offensive, glue-mnli),
Unhelpful
(paws, dbpedia_14, art), (paws, dbpedia_14, cosmos_qa), (paws, cosmos_qa, art), (dbpedia_14, cosmos_qa,
art), (quail, paws, dbpedia_14)
Table 12: Analysis of which meta-training tasks give good performance in Channel MetaICL. We report ﬁve most
helpful and the most unhelpful tasks (or task sets), respectively.
Method
Train labels
Test labels
Original
Replaced
All target tasks
Random
36.0
36.0
0-shot
34.2
23.8/16.8
Channel 0-shot
37.3
31.4/22.9
In-context
37.4/33.9
30.5/25.0
Channel In-context
46.3/40.3
37.7/31.3
MT 0-shot
Original
37.3
25.5/16.4
Channel MT 0-shot Original
40.9
28.6/19.9
MetaICL
Original
43.4/39.9
30.1/24.0
Channel MetaICL
Original
50.7/48.0
36.5/28.9
MT 0-shot
Replaced
24.4
23.1/15.5
Channel MT 0-shot Replaced
36.7
34.1/28.4
MetaICL
Replaced
40.7/36.0
43.5/35.2
Channel MetaICL
Replaced
47.1/42.7
39.5/33.7
Table 13: Ablation where label words are replaced with
random English word in the class→class setting. Orig-
inal and Replaced indicate original label words and la-
bels that are replaced to random English words, respec-
tively. When tested on Replaced, ﬁve random seeds
used to sample English words.
of 26 target datasets are sentiment analysis, and
dbpedia_14/cosmos_qa/race-middle be-
ing unhelpful. Moreover, we think which tasks
are helpful largely depends on the choice of target
tasks, and we should not make early conclusions
that certain tasks are helpful/unhelpful in all cases.
We think future work should investigate these im-
pacts in a more systematic way.
C.4
Does MetaICL generalize when semantic
hints from label words are removed?
Our experiments use label words taken from the
original dataset, which often contain semantic
hints—hints on what each label is supposed to
mean (entailment and not_entailment
for the NLI task, and positive and negative
for the sentiment analysis task).
If the model
is truly learning the task in-context, it should
generalize when label words are replaced with
random English words, e.g., entailment and
not_entailment are replaced with apple
and orange, thus not giving any hints about the
task. In this context, we run experiments where
each label word is replaced with a random word
sampled from 61,569 common English words.10
We use ﬁve seeds for sampling random words, and
report the average and the worst-case performance.
Results in Table 13 show that raw LMs (the ﬁrst
block of the table) and models trained on the orig-
inal data (the second block) achieve near random
guessing performance. This indicates that having
semantic hints from label words is a necessary con-
dition for all models to perform the task.
Next, we meta-train the MT 0-shot baseline and
MetaICL where, for each iteration of meta-training,
we similarly map label words with random words.
The mapping from the label set to sampled English
words is independent for each iteration, so that the
model never sees the same mapping during meta-
training and hence does not overﬁt to a speciﬁc
mapping. Results are reported in the third block of
Table 13. MT 0-shot baselines are still not better
than random guessing, which is expected as they
have no way to grasp the meaning of each label.
On the other hand, MetaICL beneﬁts from training
on the replaced data, improving performance from
30.1% to 43.5% while retaining most performance
on the original data (43.4% →40.7%).
Still, overall performance is relatively poor. We
think future work should investigate the model that
can in-context learn any task.
10pypi.org/project/english-words.
2807
Setting: HR→LR Meta-train
piqa, hate_speech_offensive, google_wellformed_query, social_i_qa, circa, quoref, glue-sst2, scitail, emo, cosmos_qa, freebase_qa, ag_news, art, paws,
kilt_ay2, glue-qnli, quail, ade_corpus_v2-classiﬁcation, sciq, hatexplain, emotion, glue-qqp, kilt_fever, kilt_nq, dbpedia_14, kilt_zsre, hellaswag, squad-
with_context, hotpot_qa, glue-mnli, ropes, squad-no_context, kilt_hotpotqa, discovery, superglue-record, race-middle, race-high, lama-trex, swag, gigaword,
amazon_polarity, biomrc, tab_fact, tweet_eval-emoji, tweet_eval-offensive, tweet_eval-sentiment, tweet_qa, imdb, lama-conceptnet, liar, anli, wiki_qa, kilt_trex,
wikisql, wino_grande, wiqa, search_qa, xsum, yahoo_answers_topics, yelp_polarity, yelp_review_full
Setting: HR→LR Target
quarel, ﬁnancial_phrasebank, openbookqa, codah, qasc, glue-mrpc, dream, sick, commonsense_qa, medical_questions_pairs, quartz-with_knowledge,
poem_sentiment, quartz-no_knowledge, glue-wnli, climate_fever, ethos-national_origin, ethos-race, ethos-religion, ai2_arc, hate_speech18, glue-rte,
superglue-cb, superglue-copa, tweet_eval-hate, tweet_eval-stance_atheism, tweet_eval-stance_feminist
Setting: Classiﬁcation Meta-train
Meta-Train: superglue-rte, tweet_eval-sentiment, discovery, glue-rte, superglue-wsc, glue-mrpc, tweet_eval-stance_hillary, tweet_eval-offensive, emotion, ha-
texplain, glue-cola, sick, paws, ethos-sexual_orientation, glue-qqp, tweet_eval-emotion, sms_spam, health_fact, glue-mnli, imdb, ethos-disability, glue-wnli,
scitail, trec-ﬁnegrained, yahoo_answers_topics, liar, glue-sst2, tweet_eval-stance_abortion, circa, tweet_eval-stance_climate, glue-qnli, tweet_eval-emoji, ethos-
directed_vs_generalized, ade_corpus_v2-classiﬁcation, hate_speech_offensive, superglue-wic, google_wellformed_query, tweet_eval-irony, ethos-gender, on-
estop_english, trec, rotten_tomatoes, kilt_fever
Setting: Non-Classiﬁcation Meta-train
ade_corpus_v2-dosage, art, biomrc, blimp-anaphor_number_agreement, blimp-ellipsis_n_bar_2, blimp-sentential_negation_npi_licensor_present, blimp-
sentential_negation_npi_scope, commonsense_qa, crows_pairs, dream, freebase_qa, gigaword, hellaswag, hotpot_qa, kilt_ay2, kilt_hotpotqa, kilt_trex,
kilt_zsre, lama-conceptnet, lama-google_re, lama-squad, numer_sense, openbookqa, piqa, proto_qa, qa_srl, quarel, quartz-no_knowledge, race-high, ropes,
sciq, social_i_qa, spider, superglue-multirc, wikisql, xsum, yelp_review_full
Setting: Classiﬁcation Target
tweet_eval-stance_feminist, ethos-national_origin, tweet_eval-hate, ag_news, amazon_polarity, hate_speech18, poem_sentiment, climate_fever, medi-
cal_questions_pairs, tweet_eval-stance_atheism, superglue-cb, dbpedia_14, wiki_qa, emo, yelp_polarity, ethos-religion, ﬁnancial_phrasebank, tab_fact, anli,
ethos-race
Setting: QA Meta-train
biomrc, boolq, freebase_qa, hotpot_qa, kilt_hotpotqa, kilt_nq, kilt_trex, kilt_zsre, lama-conceptnet, lama-google_re, lama-squad, lama-trex, mc_taco,
numer_sense, quoref, ropes, search_qa, squad-no_context, squad-with_context, superglue-multirc, superglue-record, tweet_qa, web_questions, uni-
ﬁedqa:squad2, uniﬁedqa:natural_questions_with_dpr_para, uniﬁedqa:race_string, uniﬁedqa:squad1_1, uniﬁedqa:drop, uniﬁedqa:newsqa, uniﬁedqa:narrativeqa,
uniﬁedqa:winogrande_xl, uniﬁedqa:social_iqa, uniﬁedqa:quoref, uniﬁedqa:physical_iqa, uniﬁedqa:ropes, uniﬁedqa:commonsenseqa, uniﬁedqa:boolq
Setting: Non-QA Meta-train
hate_speech_offensive, google_wellformed_query, circa, glue-sst2, scitail, emo, ag_news, art, paws, kilt_ay2, glue-qnli, ade_corpus_v2-classiﬁcation, hatex-
plain, emotion, glue-qqp, kilt_fever, dbpedia_14, glue-mnli, discovery, gigaword, amazon_polarity, tab_fact, tweet_eval-emoji, tweet_eval-offensive, tweet_eval-
sentiment, imdb, liar, anli, wikisql, xsum, yahoo_answers_topics, yelp_polarity, yelp_review_full
Setting: QA Target
ai2_arc, codah, cosmos_qa, dream, hellaswag, openbookqa, qasc, quail, quarel, quartz-no_knowledge, quartz-with_knowledge, sciq, superglue-
copa, swag, wino_grande, wiqa, uniﬁedqa:qasc, uniﬁedqa:qasc_with_ir, uniﬁedqa:openbookqa, uniﬁedqa:openbookqa_with_ir, uniﬁedqa:mctest, uni-
ﬁedqa:ai2_science_middle
Setting: Non-NLI Meta-train
ade_corpus_v2-classiﬁcation, ag_news, amazon_polarity, circa, climate_fever, dbpedia_14, discovery, emo, emotion, ethos-directed_vs_generalized, ethos-
disability, ethos-gender, ethos-national_origin, ethos-race, ethos-religion, ethos-sexual_orientation, ﬁnancial_phrasebank, glue-cola, glue-mrpc, glue-qqp, glue-
sst2, google_wellformed_query, hate_speech18, hate_speech_offensive, hatexplain, health_fact, imdb, kilt_fever, liar,
medical_questions_pairs, onestop_english, paws, poem_sentiment, rotten_tomatoes, sick, sms_spam, superglue-wic, superglue-wsc, tab_fact, trec, trec-
ﬁnegrained, tweet_eval-emoji, tweet_eval-emotion, tweet_eval-hate, tweet_eval-irony, tweet_eval-offensive, tweet_eval-sentiment, tweet_eval-stance_abortion,
tweet_eval-stance_atheism, tweet_eval-stance_climate, tweet_eval-stance_feminist, tweet_eval-stance_hillary, wiki_qa, yahoo_answers_topics, yelp_polarity
Setting: NLI Target
anli, glue-mnli, glue-qnli, glue-rte, glue-wnli, scitail, sick, superglue-cb
Setting: Non-Paraphrase Meta-train
ade_corpus_v2-classiﬁcation, ag_news, amazon_polarity, anli, circa, climate_fever, dbpedia_14, discovery, emo, emotion, ethos-directed_vs_generalized,
ethos-disability, ethos-gender, ethos-national_origin, ethos-race, ethos-religion, ethos-sexual_orientation, ﬁnancial_phrasebank, glue-cola, glue-mnli, glue-
qnli, glue-rte, glue-sst2, glue-wnli, google_wellformed_query, hate_speech18, hate_speech_offensive, hatexplain, health_fact, imdb, kilt_fever, liar, on-
estop_english, poem_sentiment, rotten_tomatoes, scitail, sick, sms_spam, superglue-cb, superglue-rte, superglue-wic, superglue-wsc, tab_fact, trec, trec-
ﬁnegrained, tweet_eval-emoji, tweet_eval-emotion, tweet_eval-hate, tweet_eval-irony, tweet_eval-offensive, tweet_eval-sentiment, tweet_eval-stance_abortion,
tweet_eval-stance_atheism, tweet_eval-stance_climate, tweet_eval-stance_feminist, tweet_eval-stance_hillary, wiki_qa, yahoo_answers_topics, yelp_polarity
Setting: Non-Paraphrase Target
Target: glue-mrpc, glue-qqp, medical_questions_pairs, paws
Setting: HR→LR Diverse Meta-train
glue-mnli, glue-qqp, glue-sst2, hate_speech_offensive, kilt_hotpotqa, kilt_zsre, lama-trex, race-high, scitail, tweet_eval-offensive, wino_grande, ya-
hoo_answers_topics, yelp_review_full
Setting: HR→LR No Diverse Meta-train
ag_news, amazon_polarity, dbpedia_14, emo, emotion, glue-sst2, imdb, tweet_eval-emoji, tweet_eval-offensive, tweet_eval-sentiment, yahoo_answers_topics,
yelp_polarity, yelp_review_full
Setting: HR→LR Instructions Meta-train
ag_news, amazon_polarity, anli, art, circa, cosmos_qa, dbpedia_14, discovery, emo, emotion, freebase_qa, gigaword, google_wellformed_query, hel-
laswag, imdb, liar, paws, piqa, quail, quoref, ropes, sciq, scitail, social_i_qa, swag, tab_fact, wiki_qa, wiqa, xsum, yahoo_answers_topics, yelp_polarity,
yelp_review_full
Setting: HR→LR Instructions Target
ai2_arc, climate_fever, codah, commonsense_qa, dream, ﬁnancial_phrasebank, medical_questions_pairs, openbookqa, poem_sentiment, qasc, quarel, sick
Table 14: Full datasets for all settings. The ﬁrst 10 rows are for main settings described in Section 4.1; the last
four rows are settings used for ablations in Section 5.2. Splits and dataname names consistent to those in Ye et al.
(2021) and Khashabi et al. (2020). Bold indicates the test dataset with no overlap in domain with meta-training
tasks. A preﬁx unifiedqa: indicates that the dataset taken is from UNIFIEDQA; otherwise, from CROSSFIT.
References for all datasets are provided in Table 15.
2808
ade_corpus_v2-classiﬁcation (Gurulingappa et al., 2012), ade_corpus_v2-dosage (Gurulingappa et al., 2012), ag_news Gulli
(link), ai2_arc (Clark et al., 2018), amazon_polarity (McAuley and Leskovec, 2013), anli (Nie et al., 2020), art (Bha-
gavatula et al., 2020), biomrc (Pappas et al., 2020), blimp-anaphor_number_agreement (Warstadt et al., 2020), blimp-
ellipsis_n_bar_2 (Warstadt et al., 2020), blimp-sentential_negation_npi_licensor_present (Warstadt et al., 2020), blimp-
sentential_negation_npi_scope (Warstadt et al., 2020), boolq (Clark et al., 2019), circa (Louis et al., 2020), cli-
mate_fever (Diggelmann et al., 2020), codah (Chen et al., 2019), commonsense_qa (Talmor et al., 2019), cosmos_qa (Huang
et al., 2019), crows_pairs (Nangia et al., 2020), dbpedia_14 (Lehmann et al., 2015), discovery (Sileo et al., 2019),
dream (Sun et al., 2019), emo (Chatterjee et al., 2019), emotion (Saravia et al., 2018), ethos-directed_vs_generalized (Mol-
las et al., 2020), ethos-disability (Mollas et al., 2020), ethos-gender (Mollas et al., 2020), ethos-national_origin (Mol-
las et al., 2020), ethos-race (Mollas et al., 2020), ethos-religion (Mollas et al., 2020), ethos-sexual_orientation (Mollas
et al., 2020), ﬁnancial_phrasebank (Malo et al., 2014), freebase_qa (Jiang et al., 2019), gigaword (Napoles et al., 2012),
glue-cola (Warstadt et al., 2019), glue-mnli (Williams et al., 2018), glue-mrpc (Dolan and Brockett, 2005), glue-qnli (Ra-
jpurkar et al., 2016), glue-qqp (data.quora.com/First-Quora-Dataset-Release-Question-Pairs), glue-
rte (Dagan et al., 2005; Bar-Haim et al., 2006)(Giampiccolo et al., 2007; Bentivogli et al., 2009), glue-sst2 (Socher et al.,
2013), glue-wnli (Levesque et al., 2012), google_wellformed_query (Faruqui and Das, 2018), hate_speech18 (de Gib-
ert et al., 2018), hate_speech_offensive (Davidson et al., 2017), hatexplain (Mathew et al., 2020), health_fact (Kotonya and
Toni, 2020), hellaswag (Zellers et al., 2019), hotpot_qa (Yang et al., 2018), imdb (Maas et al., 2011), kilt_ay2 (Hoffart et al.,
2011), kilt_fever (Thorne et al., 2018), kilt_hotpotqa (Yang et al., 2018), kilt_nq (Kwiatkowski et al., 2019), kilt_trex (Elsahar
et al., 2018), kilt_zsre (Levy et al., 2017), lama-conceptnet (Petroni et al., 2019, 2020), lama-google_re (Petroni et al., 2019,
2020), lama-squad (Petroni et al., 2019, 2020), lama-trex (Petroni et al., 2019, 2020), liar (Wang, 2017), mc_taco (Zhou
et al., 2019), medical_questions_pairs (McCreery et al., 2020), numer_sense (Lin et al., 2020), onestop_english (Vajjala and
Luˇci´c, 2018), openbookqa (Mihaylov et al., 2018), paws (Zhang et al., 2019), piqa (Bisk et al., 2020), poem_sentiment (Sheng
and Uthus, 2020), proto_qa (Boratko et al., 2020), qa_srl (He et al., 2015), qasc (Khot et al., 2020), quail (Rogers et al.,
2020), quarel (Tafjord et al., 2019a), quartz-no_knowledge (Tafjord et al., 2019b), quartz-with_knowledge (Tafjord et al.,
2019b), quoref (Dasigi et al., 2019), race-high (Lai et al., 2017), race-middle (Lai et al., 2017), ropes (Lin et al., 2019),
rotten_tomatoes (Pang and Lee, 2005), sciq (Welbl et al., 2017), scitail (Khot et al., 2018), search_qa (Dunn et al., 2017),
sick (Marelli et al., 2014), sms_spam (Almeida et al., 2011), social_i_qa (Sap et al., 2019a), spider (Yu et al., 2018),
squad-no_context (Rajpurkar et al., 2016), squad-with_context (Rajpurkar et al., 2016), superglue-cb (de Marneffe et al.,
2019), superglue-copa (Gordon et al., 2012), superglue-multirc (Khashabi et al., 2018), superglue-record (Zhang et al.,
2018), superglue-rte (Dagan et al., 2005; Bar-Haim et al., 2006)(Giampiccolo et al., 2007; Bentivogli et al., 2009), superglue-
wic (Pilehvar and Camacho-Collados, 2019), superglue-wsc (Levesque et al., 2012), swag (Zellers et al., 2018),
tab_fact (Chen et al., 2020), trec (Li and Roth, 2002; Hovy et al., 2001), trec-ﬁnegrained (Li and Roth, 2002; Hovy
et al., 2001), tweet_eval-emoji (Barbieri et al., 2020), tweet_eval-emotion (Barbieri et al., 2020), tweet_eval-hate (Bar-
bieri et al., 2020), tweet_eval-irony (Barbieri et al., 2020), tweet_eval-offensive (Barbieri et al., 2020), tweet_eval-
sentiment (Barbieri et al., 2020), tweet_eval-stance_abortion (Barbieri et al., 2020), tweet_eval-stance_atheism (Barbieri
et al., 2020), tweet_eval-stance_climate (Barbieri et al., 2020), tweet_eval-stance_feminist (Barbieri et al., 2020), tweet_eval-
stance_hillary (Barbieri et al., 2020), tweet_qa (Xiong et al., 2019), uniﬁedqa:ai2_science_middle (data.allenai.org/
ai2-science-questions), uniﬁedqa:boolq (Clark et al., 2019), uniﬁedqa:commonsenseqa (Talmor et al., 2019), uni-
ﬁedqa:drop (Dua et al., 2019), uniﬁedqa:mctest (Richardson et al., 2013), uniﬁedqa:narrativeqa (Kociský et al., 2018),
uniﬁedqa:natural_questions (Kwiatkowski et al., 2019), uniﬁedqa:newsqa (Trischler et al., 2017), uniﬁedqa:openbookqa (Mi-
haylov et al., 2018), uniﬁedqa:physical_iqa (Bisk et al., 2020), uniﬁedqa:qasc (Khot et al., 2019), uniﬁedqa:quoref (Dasigi et al.,
2019), uniﬁedqa:race_string (Lai et al., 2017), uniﬁedqa:ropes (Lin et al., 2019), uniﬁedqa:social_iqa (Sap et al., 2019b), uni-
ﬁedqa:squad1_1 (Rajpurkar et al., 2016), uniﬁedqa:squad2 (Rajpurkar et al., 2018), uniﬁedqa:winogrande_xl (Sakaguchi et al.,
2020a), web_questions (Berant et al., 2013), wiki_qa (Yang et al., 2015), wikisql (Zhong et al., 2017), wino_grande (Sakaguchi
et al., 2020b), wiqa (Tandon et al., 2019), xsum (Narayan et al., 2018), yahoo_answers_topics (link), yelp_polarity (Zhang
et al., 2015), yelp_review_full (Zhang et al., 2015)
Table 15: References for 142 datasets used in the paper. A preﬁx unifiedqa: indicates that the dataset taken is
from UNIFIEDQA; otherwise, from CROSSFIT.
D
Potential Risks
MetaICL is based on the large language model that
is pretrained on a web corpus, which potentially
includes harmful and biased context, despite the
original authors’ best efforts to mine the text. There
are also potential risks in privacy and security—for
instance, Carlini et al. (2021) reported that it is
possible to design the attack algorithm to extract
a substantial amount of training data. We thus
highlight that MetaICL should be considered as a
research prototype rather than a deployable system
to real users, and continuing efforts are needed to
reduce potential risks of the model.
2809
