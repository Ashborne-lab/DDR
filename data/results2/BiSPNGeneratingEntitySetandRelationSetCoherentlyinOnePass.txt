Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2066–2077
December 6-10, 2023 ©2023 Association for Computational Linguistics
BiSPN: Generating Entity Set and Relation Set Coherently in One Pass
Yuxin He1 and Buzhou Tang1,2,∗
1Department of Computer Science, Harbin Institute of Technology, Shenzhen, China
2Peng Cheng Laboratory, Shenzhen, China
21S051047@stu.hit.edu.cn
tangbuzhou@gmail.com
Abstract
By modeling the interaction among instances
and avoiding error propagation, Set Prediction
Networks (SPNs) achieve state-of-the-art per-
formance on the tasks of named entity recogni-
tion and relation triple extraction respectively.
However, how to jointly extract entities and
relation triples via SPNs remains an unex-
plored problem, where the main challenge is
the maintenance of coherence between the pre-
dicted entity/relation sets during one-pass gen-
eration.
In this work, we present Bipartite
Set Prediction Network (BiSPN), a novel joint
entity-relation extraction model that can effi-
ciently generate entity set and relation set in
parallel. To overcome the challenge of coher-
ence, BiSPN is equipped with a novel bipartite
consistency loss as well as an entity-relation
linking loss during training. Experiments on
three biomedical/clinical datasets and a general-
domain dataset show that BiSPN achieves new
state of the art in knowledge-intensive scene
and performs competitively in general-domain,
while being more efficient than two-stage joint
extraction methods.
1
Introduction
Extracting entities and relation triples from text is a
fundamental task of Information Extraction. There
have been many efforts that decompose the prob-
lem into separate tasks, i.e. named entity recogni-
tion (NER) and relation triple extraction (RE), and
solve them respectively. Among these efforts, Set
Prediction Networks (SPNs) have demonstrated
state-of-the-art performance on NER (Tan et al.,
2021; Shen et al., 2022) and RE (Sui et al., 2020;
Tan et al., 2022).
Typically, SPNs leverage a set of learnable
queries to model the interaction among instances
(entities or relation triples) via attention mechanism
and generate the set of instances naturally. The suc-
cess of SPNs on NER and RE inspires us to explore
*Corresponding Author.
Figure 1: The target output of joint entity and relation
extraction is essentially an entity set and a relation set
that should be consistent with each other. Such co-
herence is difficult to be guaranteed when generating
entity/relation sets in parallel. And our work manages
to address this challenge.
the possibility of jointly solving the extraction of
entities and relation triples with SPNs, which is a
promising but unexplored direction.
In this paper, we propose Bipartite Set Prediction
Network (BiSPN), a variant of SPNs, to generate
target entity set and relation set in one pass. It
can not only avoid the negative effect of cascade
error but also enjoy the benefit of high inference
speed. However, it is challenged by the difficulty
to maintain the coherence between the generated
entity set and relation set, due to its parallel design.
As illustrated in Figure 1, the head/tail entities
of generated relation triples should be included in
the generated entity set. The significance of this
coherence is two-fold: 1) by requiring the gener-
ated entity set to contain the head/tail entities, the
recall of the generated entities is more guaranteed;
2) by restricting the head/tail entities within the
generated entity set, the precision and recall of the
generated triples are more guaranteed.
Despite that, it is difficult to maintain such con-
sistency when generating the two sets in parallel,
since all instance queries are assumed to be of equal
status and their hidden representations are updated
2066
using bidirectional attention without any further
restriction.
To overcome this challenge, we come up with
two novel solutions. The first one is a Bipartite
Consistency Loss function. It works by looking
for a reference entity from the generated entity
set for each relational subject/object and forcing
the subject/object to simulate the reference en-
tity. Symmetrically, it also find a reference sub-
ject/object for each entity classified as involved
in relation and force the entity to simulate the
reference subject/object. Our second solution is
an Entity-Relation Linking Loss function, which
works in the hidden semantic space. By computing
the linking scores between the projected represen-
tations of entity queries and relation queries, it en-
courages the model to learn the interaction between
entity instances and relation triple instances.
To sum up, our main contributions include:
• We present BiSPN, the first SPN-based joint
entity and relation extraction model, which is
able to generate target entity set and relation
set in one pass.
• To maintain the coherence between the gener-
ated entity set and relation set, two novel loss
functions, i.e., Bipartite Consistency Loss and
Entity-Relation Linking Loss are introduced.
• BiSPN outperforms SOTA methods on
three biomedical/clinical datasets, which is
knowledge-intensive, and achieves competi-
tive results on a general-domain benchmark.
Besides, it infers much faster than two-stage
joint extraction methods.
2
Related Work
2.1
Joint Entity and Relation Extraction
The target of joint entity and relation extraction is to
recognize all entities mentioned in a given text and
identify all entity pairs involved in relation. Exist-
ing methods for joint entity and relation extraction
fall into four categories: (1) span-based methods
(Dixit and Al-Onaizan, 2019; Zhong and Chen,
2021; Ye et al., 2022) that enumerate spans and
conduct span-level classification to extract entities,
enumerate and classify span pairs to extract rela-
tion triples; (2) table filling-based methods (Wang
et al., 2020, 2021; Yan et al., 2021) that fill out a
table for each entity/relation type via token pair
classification; (3) machine reading comprehension
(MRC)-based methods (Li et al., 2019) that casts
the task as a multi-turn question answering prob-
lem via manually designed templates; (4) autore-
gressive generation-based methods (Zeng et al.,
2018; Lu et al., 2022) that reformulate the task
as a sequence-to-sequence problem by linearizing
target entities/relations into a pointer sequence or
augmented natural language.
Among them, only the methods based on table
filling can extract entities and relation triples in
one stage, all other methods perform multi-step
prediction, suffering from cascade errors and low
inference speed. In this paper, we provide a new
choice for one-stage joint entity and relation extrac-
tion, which is based on set prediction networks.
2.2
Set Prediction Networks
Set prediction networks are originally proposed
for the task of object detection in computer vision
(Carion et al., 2020). And they are successfully
extended for information extraction by (Sui et al.,
2020; Tan et al., 2021; Shen et al., 2022; Tan et al.,
2022).
Generally, these methods employ a set of learn-
able queries as additional input, model the interac-
tion among instances (entities/relations) via self-
attention among the queries and one-way attention
between the queries and the textual context. How-
ever, all these methods can only perform named
entity recognition (Tan et al., 2021; Shen et al.,
2022) or relation triple extraction (Sui et al., 2020;
Tan et al., 2022), rather than jointly solve both of
them.
3
Methodology
3.1
Problem Formulation
Given an input sentence x = x1, x2, ..., xL, the aim
of joint entity and relation extraction is to predict
the set of entities {ei}Ne
i=1 and the set of relation
triples {rj}Nr
j=1 mentioned in the sentence. Here, L
is the length of the input sentence, Ne and Nr are
the numbers of target entities and target relation
triples respectively. The i-th entity ei is denoted as
(starti, endi, te
i), where starti, endi are the start
token index and end token index of the entity, te
i ∈
Te is the entity type label. The j-th relation triple
rj is denoted as (eh
j , tr
j, et
j), where eh
j and et
j are
the head entity (starth
j , endh
j , te,h
j ) and tail entity
(startt
j, endt
j, te,t
j ) of the relation triple, tr
j ∈Tr is
the relation type label. We additionally define a
null label ∅for the set of entity types Te and the
2067
Figure 2: An overview of BiSPN, the proposed joint entity and relation extraction framework that is capable of
generating target entity set and relation set coherently in one pass.
set of relation types Tr respectively, to indicate that
no entity or no relation is recognized.
3.2
BiSPN
As illustrated in Figure 2, the proposed model
mainly consists of a shared encoder, a shared de-
coder, an entity decoding module (in blue) and a
relation decoding module (in red).
3.2.1
Shared Encoder
The encoder of BiSPN is essentially a bidirectional
pretrained language model (Devlin et al., 2019)
with modified input style and attention design.
We first transform the input sentence x into input
embeddings X ∈RL×d, and then concatenate X
with a series of learnable entity queries Qe and
relation queries Qr to form the model input ˜X:
˜X = [X; Qe; Qr] ∈R(L+Me+Mr)×d
(1)
where d is the model dimension, Me and Mr are
hyperparameters controlling the number of entity
queries and the number of relation queries (Me ≫
Ne, Mr ≫Nr).
To prevent the randomly initialized queries from
negatively affecting the contextual token encodings,
we follow the work (Shen et al., 2022) to modify
the bidirectional self-attention into one-way self-
attention. Concretely, the upper right L × (Me +
Mr) sub-matrix of the attention mask is filled with
negative infinity value so that the entity/relation
queries become invisible to the token encodings,
while the entity/relation queries can still attend to
each other and the token encodings.
After multiple one-way self-attention layers and
feed-forward layers, the encoder outputs the con-
textual token encodings as well as the contextual
entity/relation queries.
3.2.2
Shared Decoder
The shared decoder consists of N decoding blocks.
Each decoding block includes an one-way self-
attention layer (as described above), a bidirec-
tional self-attention layer and feed-forward layers.
The one-way self-attention layer here functions as
the cross-attention layer of Transformer decoder
(Vaswani et al., 2017), which aggregates textual
context for decoding. (The main difference be-
tween one-way self-attention and cross-attention
is that the contextual token encodings also get
updated by one-way self-attention.) The bidirec-
tional self-attention layer updates the entity/relation
2068
queries via modeling the interaction among them.
After shared decoding, the decoder outputs the
updated token representations Hx, entity queries
He and relation queries Hr.
3.2.3
Entity Decoding Module
The entity decoding module consists of an entity-
view projection layer, an entity decoder and an
entity predictor.
The entity-view projection layer first linearly
transforms the token encodings Hx into entity-
view:
Hx
e = Linear(Hx)
(2)
The entity decoder, which includes multiple
layers of cross-attention and bidirectional self-
attention, receives the transformed token encodings
Hx
e as decoding context and the entity queries He
as decoder input, and output the final representation
of entity queries ˜
He:
˜
He = EntityDecoder(He|Hx
e )
(3)
The entity predictor is responsible for predicting
the boundary and entity type of each entity query.
For each entity query, it first fuse the query rep-
resentation with the transformed token encodings,
and then calculate the probability of each token in
the sentence being the start/end token of the corre-
sponding entity:
Sδ
i = Linear
 Relu(Linear( ˜
He
i ) + Linear(Hx
e ))

(4)
P δ
i = Softmax(Sδ
i ), δ ∈{start, end}
(5)
where Sδ
i ∈RL is a vector of logits of each token
being the start/end token of the entity associated
with i-th entity query, P δ
i is the corresponding prob-
ability distribution.
An MLP-based classifier is leveraged to predict
the type of the entity associated with the i-th entity
query:
P te
i
= Softmax(MLP( ˜
He
i ))
(6)
During inference, the predicted boundary and
entity type corresponding to the k-th entity query
are calculated as:
scorek(i, j) = P start
k
[i] + P end
k
[j]
(7)
(
ˆ
startk,
ˆ
endk) =
arg max
(i,j): 0<j−i<L
scorek(i, j) (8)
ˆte
k = arg max P te
k
(9)
Note that, the entity predictor will filter out the
entity whose predicted type label is ∅.
3.2.4
Relation Decoding Module
The relation decoding module consists of a relation-
view projection layer, a relation decoder, a head-tail
predictor and a relation type predictor.
The relation-view projection layer and relation
decoder work in the same manner as the entity-
view projection layer and entity decoder, except
that the relation decoder splits relation queries into
head/tail queries before decoding:
[Hh; Ht] = Linear(Hr)
(10)
Hx
r = Linear(Hx)
(11)
˜
Hh, ˜
Ht, ˜
Hr = RelationDecoder(Hh, Ht, Hr|Hx
r )
(12)
The head-tail predictor then predicts the bound-
ary and entity type of the head/tail entity associated
with each relation queries. This process is similar
to the entity prediction process (Equation 4-10).
The only difference is that the entities queries be-
comes the head/tail queries ˜Hh/t and the token
encodings is now in relation-view Hx
r .
The relation type predictor classifies the category
of i-th relation query according to ˜
Hr
i :
P tr
i
= Softmax(MLP( ˜
Hr
i ))
(13)
3.3
Prediction Loss
To train the model, we should find the optimal
assignment between the gold entity set and the gen-
erated entity set, as well as the optimal assignment
between the gold relation set and the generated re-
lation set, which are calculated in the same way as
in (Tan et al., 2021; Shen et al., 2022) using the
Hungarian algorithm (Kuhn, 1955).
After the optimal assignments are obtained, we
calculate the following prediction loss Lpred for
each sample:
Lent = −
Me
X
i=1
 log P start
i
[startφ(i)] + log P end
i
[endφ(i)]
+ log P te
i [te
φ(i)]

(14)
Lh/t
ent = −
Mr
X
j=1
 log P start
j,h/t[starth/t
σ(j)] + log P end
j,h/t[endh/t
σ(j)]
+ log P te
j,h/t[te,h/t
σ(j)]

(15)
Lrel = Lh/t
ent −
Mr
X
j=1
log P tr
j [tr
σ(j)]
(16)
Lpred = Lent + Lrel
(17)
where φ(i) is the index of the gold entity assigned
to the i-th generated entity, σ(j) is the index of the
2069
gold relation triple assigned to the j-th generated
relation triple, Lh/t
ent represents the loss of head/tail
entity prediction.
3.4
Bipartite Consistency Loss
To calculate the bipartite consistency loss, we first
find a reference entity from the generated entity
set for each head/tail entity. A reference entity is
defined as the entity most similar to the referring
head/tail entity. Concretely, the similarity between
ea, the a-th generated entity, and eh/t
b , the head/tail
entity of the b-th generated relation triple is mea-
sured in KL divergence between the start/end/type
probability distributions of ea and eh/t
b :
sim(ea, eh/t
b ) =
X
δ∈{start,end,te}
−DKL(P δ
a||P δ,h/t
b
)
where DKL(P||Q) is the KL divergence between
target distribution P and approximate distribution
Q; P δ,h/t
b
means P δ,h
b
when eh/t
b
is a head entity,
otherwise P δ,h/t
b
means P δ,t
b .
We want every head/tail entity to simulate its
reference entity, which is equivalent to maximizing
the similarity. Hence, the consistency loss in the
relation →entity direction is computed as:
Lrel→ent = −
Mr
X
i=1

max
j∈(1,Me) sim(ej, eh
i )
+
max
j∈(1,Me) sim(ej, et
i)

(18)
Symmetrically, we also find a reference head/tail
entity for each generated entity that is classified as
having relation. The classification is conducted by
a binary classifier, which is trained with a binary
cross-entropy loss function:
phas-rel
i
= sigmoid(MLP( ˜
He
i ))
(19)
Lhas-rel = −1
Me
Me
X
i=1

yhas-rel
i
log phas-rel
i
+
(1 −yhas-rel
i
) log(1 −phas-rel
i
)

(20)
where yhas-rel
i
= 1 only if the gold entity assigned
to the i-th entity query is involved in relation.
The consistency loss in the entity →relation
direction is then calculated as follows:
˜
sim(eh/t
b , ea) =
X
δ∈{start,end,te}
−DKL(P δ,h/t
b
||P δ
a)
Lent→rel = −
X
i∈Ω
max
j∈(1,Mr)
˜
sim(eh/t
j , ei)
(21)
Ω= {i | phas-rel
i
≥0.5, i ∈(1, Me)}
(22)
where Ωis the set of indices of the entities classified
as involved in relation.
We sum up Lent→rel, Lrel→ent and Lhas-rel to get
the overall bipartite consistency loss Lent↔rel.
3.5
Entity-Relation Linking Loss
While the bipartite consistency loss softly aligns
the predicted distributions between the generated
entity set and relation set during training, the entity-
relation linking loss encourages BiSPN to model
the interaction between entity queries and relation
queries.
To this end, we first project the intermediate rep-
resentations of entity queries and relation queries
and then compute the linking scores between them
via a Biaffine layer:
¯He = Linear(He)
(23)
¯Hr = Linear(Hr)
(24)
Slink = Biaffine( ¯He, ¯Hr) ∈RMe×Mr
(25)
With the linking scores, we calculate the follow-
ing binary cross-entropy loss:
P link = sigmoid(Slink)
(26)
Llink = −
1
MeMr
Me
X
i=1
Mr
X
j=1

ylink
i,j log P link
i,j
+ (1 −ylink
i,j ) log(1 −P link
i,j )

(27)
where ylink
i,j = 1 only if the gold entity assigned to
the i-th entity query appears in the gold relation
triple assigned to the j-th relation query.
4
Experiments
4.1
Experiment Setups
Statistics
ACE05
BioRelEx
ADE
Text2DT
# Sentences
14525
2010
4272
500
Avg sent. length
14.6
29.0
20.3
66.5
# Entity types
7
33
2
6
# Relation types
6
3
1
6
# Entities
38287
9871
17808
3280
# Relations
7691
3235
6821
3196
# Ent. per sent.
2.63
4.91
4.17
6.56
# Rel. per sent.
0.53
1.61
1.60
6.39
Table 1: Statistics of the ACE05, BioRelEx, ADE and
Text2DT datasets.
2070
Paradigm
Method
ACE05
BioRelEx
ADE
Text2DT
Ent-F1
Rel-F1
Ent-F1
Rel-F1
Ent-F1
Rel-F1
Ent-F1
Rel-F1
Two-stage
KECI (2021)
-
-
87.4
66.1
90.7
81.7
-
-
MADR (2023)
-
-
-
-
91.8
80.1
-
-
PL-Marker (2022)
89.8
66.5
87.4
66.3
92.2
83.2
96.3
93.3
One-stage
PFN (2021)
88.4
64.9
87.1
65.8
91.3
83.2
95.1
92.9
UniRE (2021)
88.8
64.3
87.3
65.6
91.6
82.9
95.4
92.5
TOP1 (2021; 2022)
-
-
-
-
-
-
-
94.4
BiSPN (Ours)
89.7
65.5
87.5
67.0
92.1
83.7
97.1
94.9
w/o Lent↔rel
89.2
64.0
86.1
66.2
91.4
83.5
96.8
94.4
w/o Llink
89.5
65.1
87.4
66.7
91.8
83.5
97.1
94.6
w/o Lent↔rel, Llink
88.9
63.4
85.7
66.0
90.9
83.0
96.2
94.1
Table 2: Main results on the ACE05, BioRelEx, ADE and Text2DT datasets. The bold font indicates the best score
and the underline font indicates the second-best score.
Datasets.
We experiment on one general-domain
dataset (ACE05) and three knowledge-intensive
datasets (BioRelEx, ADE, Text2DT). ACE05
(Walker et al., 2006) includes a corpus of
newswire, broadcast news, telephone conversa-
tions. BioRelEx (Khachatrian et al., 2019) contains
biomedical literature about binding interactions be-
tween proteins and/or biomolecules. ADE (Gu-
rulingappa et al., 2012) consists of medical reports
describing drug-related adverse effects. Text2DT1
is originally an benchmark for medical Decision
Trees extraction task in China Health Information
Processing Conference 2022. And we only use
its entity/relation annotation for experiments. See
Table 1 for detailed statistics of the datasets.
We additionally experiment on the SciERC
dataset, where we follow the same setting as in
(Wang et al., 2021; Ye et al., 2022). See Appendix
B for the results.
Evaluation Metrics.
Strict evaluation metrics
are applied, where an entity is confirmed correct
only if its boundary and entity type are correctly
predicted; a relation triple is confirmed correct only
if its relation type and head/tail entity are correctly
predicted. For ACE05 and BioRelEx, we report the
averaged Micro F1 scores over 3 random seeds. For
ADE, we follows (Ji et al., 2020; Lai et al., 2021) to
conduct 10-fold cross-validation and report the av-
eraged Macro F1 scores. For Text2DT, we follows
the top-1 system on the evaluation task to ensemble
5 models trained with different random seeds and
report the Micro F1 scores.
1http://www.cips-chip.org.cn/2022/eval3
Method
ACE05
BioRelEx
Rel-F1
sent/s
Rel-F1
sent/s
KEIC (2021)
-
-
66.1
15.7
UniRE (2021)
64.3
134.6
65.8
107.2
PL-Marker (2022)
66.5
32.0
66.4
21.9
BiSPN (Ours)
65.5
59.7
67.0
40.8
Table 3: Inference speed comparison on the ACE05,
BioRelEx datasets.
4.2
Implementation Details
We implement BiSPN with Pytorch (Paszke et al.,
2019) and run experiments on NVIDIA Tesla V100
GPUs. For ACE05, we follow (Wang et al., 2021;
Ye et al., 2022) to initialize the shared encoder with
BERT-base (Devlin et al., 2019). For BioRelEx and
ADE, we follow (Haq et al., 2023; Lai et al., 2021)
to initialize the shared encoder with BioBERT-base.
For Text2DT, we initialize the shared encoder with
Chinese-bert-wwm-ext (Cui et al., 2021). The de-
coding modules are randomly initialized. Follow-
ing (Shen et al., 2022), we freeze the encoder in
the first 5 epochs and unfreeze it in the remaining
epochs. The learning rate of decoding modules
is set to be larger than the learning rate of the en-
coder. We adopt an AdamW optimizer (Loshchilov
and Hutter, 2017) equipped with a linear warm-up
scheduler to tune the model. See Appendix A for
details of hyperparameter tuning.
4.3
Compared Baselines
We compare BiSPN with several SOTA methods
listed as follows.
KECI (Lai et al., 2021): A knowledge-enhanced
two-stage extraction model based on span graphs.
2071
MADR (Haq et al., 2023): A pipeline of inde-
pendent NER and RE models.
PL-Marker (Ye et al., 2022): A span-based
method that models the interrelation between spans
by packing levitated markers in the encoder.
PFN (Yan et al., 2021): A partition filter network
that models two-way interaction between NER and
RE subtasks.
UniRE (Wang et al., 2021): A method based on
table filling, featured with a unified label space for
one-stage joint entity and relation extraction.
TOP1: The top-1 system on the Text2DT evalu-
ation task, which combines PFN (Yan et al., 2021)
with Efficient GlobalPointer (Su, 2022).
Note that, we do not compare with SPN (Sui
et al., 2020), UniRel (Tang et al., 2022) and QIDN
(Tan et al., 2022), since these methods can only
extract relation triples and cannot recognize those
entities uninvolved in relation.
4.4
Main Results
Table 2 summarizes the overall performance
of BiSPN and compared baselines on ACE05,
BioRelEx, ADE and Text2DT. In terms of entity
extraction, BiSPN performs competitively with
or slightly better than SOTA methods on ACE05,
BioRelEx and ADE, while outperforming the
SOTA method PL-Marker by 0.8 F1 on Text2DT.
In terms of relation extraction, BiSPN boosts
SOTA performance by 0.7, 0.5 and 0.5 F1 on
BioRelEx, ADE and Text2DT respectively, verify-
ing the effectiveness of our method on knowledge-
intensive scene. However, although BiSPN out-
performs SOTA one-stage methods by 0.6 F1 on
ACE05, it is no match for the two-stage method
PL-Marker on this general-domain dataset. We will
look into the reason behind this in Section 4.6.2.
4.5
Inference Efficiency
We compare the inference efficiency of KEIC,
UniRE, PL-Marker, BiSPN on the BioRelEx and
Text2DT datasets. For a fair comparison, the ex-
periments are all conducted on a server with In-
tel(R) Xeon(R) E5-2698 CPUs and NVIDIA Tesla
V100 GPUs. And we fix the batch size as 8 during
evaluation. As shown in Table 3, BiSPN can pro-
cess around 40∼60 sentences per second, which
is 2 times faster than the SOTA two-stage method
PL-Marker. Although UniRE, a SOTA one-stage
method, is about 2.5 times faster than BiSPN, its
performance of relation extraction is uncompetitive
against BiSPN.
Figure 3: Visualization of attention between entity
queries and relation queries of a sample from BioRelEx.
The input text in this sample is “Moreover, the in vitro
binding of NF-B or Sp1 to its target DNA was not af-
fected by the presence of K-12”.
Method
# Rel ≥0
# Rel ≥1
# Rel ≥2
[10051, 2050]
[2643, 597]
[1203, 302]
PL-Marker
66.5
63.8
57.9
BiSPN
65.5 (-1.0)
63.4 (-0.4)
58.2 (+0.3)
Table 4: Relation F1 scores of BiSPN on ACE05 with
different settings of knowledge density. The numbers in
each bracket are the numbers of training, testing samples
in the dataset under specific setting.
4.6
Analysis
4.6.1
Effects of Lent↔rel and Llink
We conduct ablation study and qualitative visualiza-
tion to analyze how the bipartite consistency loss
Lent↔rel and entity-relation linking loss Llink work.
The results of ablation study are shown in Table
2. Without the bipartite consistency loss, the entity
F1 scores drop by 0.5, 1.4, 0.7, 0.3 and the rela-
tion F1 scores drop by 1.5, 0.8, 0.2, 0.5 on ACE05,
BioRelEx, ADE and Text2DT respectively. With-
out the entity-relation linking loss, the entity F1
scores decrease by 0.2, 0.1, 0.3, 0 and the relation
F1 scores decrease by 0.4, 0.3, 0.2, 0.3 on the four
datasets respectively. After removing the bipartite
consistency loss and entity-relation linking loss to-
gether, the entity F1 scores decrease by 0.8, 1.8,
1.2, 0.9 and the relation F1 scores decrease by 2.1,
1.0, 0.7, 0.8 on the four datasets respectively.
Three conclusions can be derived from these
results: 1) both the bipartite consistency loss and
2072
Figure 4: Cases from ACE05 and BioRelEx. False negative predictions are in blue.
entity-relation linking loss contribute to the overall
performance of BiSPN; 2) the bipartite consistency
loss is much more effective than the entity-relation
linking loss; 3) the effects of the two losses are not
orthogonal but still complementary in some degree.
To visualize the attention between entity queries
and relation queries, we record the attention weight
in the last bidirectional self-attention layer of
shared decoder for a sample from BioRelEx. Since
the original weight is bidirectional, we average
the weight over two directions and conduct nor-
malization to obtain the weight for visualization.
As shown in Figure 3, without Lent↔rel and Llink,
the interaction between entity queries and relation
queries is chaotic and pointless. After applying
Lent↔rel or Llink separately, the relation queries as-
sociated with gold relation triples tend to focus
on the entity queries associated with gold entities.
When applying the two loss functions together, the
relation queries associated with gold relation triples
further concentrate on the entity queries whose tar-
get entities are the same as their head/tail entities.
This phenomenon coincides with the purpose of
the two loss functions.
4.6.2
Influence of Knowledge Density
The performance of BiSPN on ACE05 is not so
good as its performance on BioRelEx, ADE and
Text2DT. We hypothesize it is the sparsity of rela-
tion triples that hinders the learning of BiSPN. As
listed in Table 1, the average number of relations
per sentence is 0.53 on ACE05. In contrast, the
average numbers of relations per sentence are 1.61,
1.60 and 6.39 on the other three datasets.
To verify our hypothesis, we filter samples ac-
cording to the number of relations they contain
and experiment on different versions of the filtered
dataset. As shown in Table 4, when the samples
without relation are discarded, the performance gap
between PL-Marker and BiSPN narrows from 1.0
to 0.4. When further discarding the samples with
less than 2 relation triples, BiSPN even performs
slightly better than PL-Marker. This reveals that the
strength of BiSPN emerges in knowledge-intensive
scene, which is reasonable, since BiSPN works by
modeling interaction among knowledge instances.
4.7
Case Study
Figure 4 illustrates two test cases from ACE05 and
BioRelEx respectively. In the first case, BiSPN
without Lent↔rel and Llink fails to recognizes the
WEA entity “more” and the PART-WHOLE rela-
tion between “it” and “more”, while BiSPN suc-
cessfully extracts them by considering the context
in entity-view and relation-view concurrently. Like-
wise, in the second case, BiSPN successfully rec-
ognizes the entity “promoter” after Lent↔rel and
Llink is applied. However, it still fails to recognize
“RNA polymerase II preinitiation complex”, which
is complicated and may require domain knowledge
for recognition.
5
Conclusion
In this work, we present BiSPN, a novel joint entity
relation extraction framework based on bipartite set
prediction. It generates entity set and relation set
in a distributed manner, so as to avoid error prop-
agation. To maintain the coherency between the
generated entity set and relation set, we come up
with two novel loss designs, namely bipartite con-
sistency loss and entity-relation linking loss. The
first one pulls closer the predicted boundary/type
distributions of entities and head/tail entities, while
the second one enforces the interaction between
entity queries and relation queries. Extensive ex-
periments demonstrate the advantage of BiSPN in
2073
knowledge-intensive scene, as well as the effective-
ness of the proposed bipartite consistency loss and
entity-relation linking loss.
Limitations
As mentioned in Section 4.6.2, the performance
of our BiSPN framework can be hindered when
the distribution of relation triples are overly sparse
in a dataset. This suggests that BiSPN is a better
choice for biomedical and clinical domains but not
the general-domain, where knowledge sparsity is
common.
Another limitation of BiSPN is its reliance on a
fixed number of entity/relation queries. Although it
is possible to set the number of queries larger in or-
der to let BiSPN generalize to longer text input, the
cost is additional memory and time consumption
that grows quadratically. To effectively address
this, future work can draw lessons from the field
of dynamic neural network and explore dynamic
selection of instance queries.
Acknowledgments
We thank the reviewers for their valuable sugges-
tions. This study is partially supported by National
Key R&D Program of China (2021ZD0113402
),
National
Natural
Science
Foundation
of
China (62276082), Major Key Project of PCL
(PCL2021A06), Shenzhen Soft Science Research
Program Project (No.KX20220705152815035) and
the Fundamental Research Fund for the Central
Universities (HIT.DZJJ.2023117).
References
Nicolas Carion, Francisco Massa, Gabriel Synnaeve,
Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. 2020. End-to-end object detection with
transformers. In Computer Vision – ECCV 2020,
pages 213–229, Cham. Springer International Pub-
lishing.
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and
Ziqing Yang. 2021. Pre-training with whole word
masking for chinese bert.
IEEE Transactions on
Audio, Speech and Language Processing.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Kalpit Dixit and Yaser Al-Onaizan. 2019. Span-level
model for relation extraction. In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics, pages 5308–5314, Florence, Italy.
Association for Computational Linguistics.
Harsha Gurulingappa, Abdul Mateen Rajput, Angus
Roberts, Juliane Fluck, Martin Hofmann-Apitius, and
Luca Toldo. 2012. Development of a benchmark
corpus to support the automatic extraction of drug-
related adverse effects from medical case reports.
Journal of Biomedical Informatics, 45(5):885–892.
Text Mining and Natural Language Processing in
Pharmacogenomics.
Hasham Ul Haq, Veysel Kocaman, and David Talby.
2023. Mining Adverse Drug Reactions from Unstruc-
tured Mediums at Scale, pages 361–375. Springer
International Publishing, Cham.
Bin Ji, Jie Yu, Shasha Li, Jun Ma, Qingbo Wu, Yusong
Tan, and Huijun Liu. 2020. Span-based joint en-
tity and relation extraction with attention-based span-
specific and contextual semantic representations. In
Proceedings of the 28th International Conference on
Computational Linguistics, pages 88–99, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.
Hrant Khachatrian, Lilit Nersisyan, Karen Ham-
bardzumyan, Tigran Galstyan, Anna Hakobyan, Ar-
sen Arakelyan, Andrey Rzhetsky, and Aram Galstyan.
2019. BioRelEx 1.0: Biological relation extraction
benchmark.
In Proceedings of the 18th BioNLP
Workshop and Shared Task, pages 176–190, Florence,
Italy. Association for Computational Linguistics.
H. W. Kuhn. 1955. The hungarian method for the assign-
ment problem. Naval Research Logistics Quarterly,
2(1-2):83–97.
Tuan Lai, Heng Ji, ChengXiang Zhai, and Quan Hung
Tran. 2021.
Joint biomedical entity and relation
extraction with knowledge-enhanced collective in-
ference. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 6248–6260, Online. Association for Computa-
tional Linguistics.
Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan,
Duo Chai, Mingxin Zhou, and Jiwei Li. 2019. Entity-
relation extraction as multi-turn question answering.
In Proceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1340–
1350, Florence, Italy. Association for Computational
Linguistics.
Ilya Loshchilov and Frank Hutter. 2017.
De-
coupled
weight
decay
regularization.
Cite
arxiv:1711.05101Comment: Published as a confer-
ence paper at ICLR 2019.
2074
Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu
Lin, Xianpei Han, Le Sun, and Hua Wu. 2022. Uni-
fied structure generation for universal information
extraction. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 5755–5772, Dublin,
Ireland. Association for Computational Linguistics.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch:
An imperative style, high-performance deep learning
library. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems
32, pages 8024–8035. Curran Associates, Inc.
Yongliang Shen, Xiaobin Wang, Zeqi Tan, Guangwei
Xu, Pengjun Xie, Fei Huang, Weiming Lu, and Yuet-
ing Zhuang. 2022. Parallel instance query network
for named entity recognition. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
947–961, Dublin, Ireland. Association for Computa-
tional Linguistics.
Jianlin Su. 2022. Efficient globalpointer.
Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Xian-
grong Zeng, and Shengping Liu. 2020. Joint En-
tity and Relation Extraction with Set Prediction Net-
works. arXiv e-prints, page arXiv:2011.01675.
Zeqi Tan, Yongliang Shen, Xuming Hu, Wenqi Zhang,
Xiaoxia Cheng, Weiming Lu, and Yueting Zhuang.
2022. Query-based instance discrimination network
for relational triple extraction. In Proceedings of
the 2022 Conference on Empirical Methods in Nat-
ural Language Processing, pages 7677–7690, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Zeqi Tan, Yongliang Shen, and Shuai Zhang. 2021.
A sequence-to-set network for nested named entity
recognition. In Proceedings of the 30th International
Joint Conference on Artificial Intelligence, IJCAI-21.
Wei Tang, Benfeng Xu, Yuyue Zhao, Zhendong Mao,
Yifeng Liu, Yong Liao, and Haiyong Xie. 2022.
UniRel: Unified representation and interaction for
joint relational triple extraction. In Proceedings of
the 2022 Conference on Empirical Methods in Nat-
ural Language Processing, pages 7087–7099, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of the 31st International
Conference on Neural Information Processing Sys-
tems, NIPS’17, page 6000–6010, Red Hook, NY,
USA. Curran Associates Inc.
Christopher Walker, Stephanie Strassel, and Kazuaki
Maeda. 2006. The automatic content extraction (ace)
program tasks, data, and evaluation. In Linguistic
Data Consortium, page 57, Philadelphia.
Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou,
Lei Li, and Junchi Yan. 2021. UniRE: A unified label
space for entity relation extraction. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 220–231, Online.
Association for Computational Linguistics.
Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen
Liu, Hongsong Zhu, and Limin Sun. 2020. TPLinker:
Single-stage joint extraction of entities and relations
through token pair linking. In Proceedings of the
28th International Conference on Computational Lin-
guistics, pages 1572–1582, Barcelona, Spain (On-
line). International Committee on Computational Lin-
guistics.
Zhiheng Yan, Chong Zhang, Jinlan Fu, Qi Zhang, and
Zhongyu Wei. 2021. A partition filter network for
joint entity and relation extraction. In Proceedings of
the 2021 Conference on Empirical Methods in Nat-
ural Language Processing, pages 185–197, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Deming Ye, Yankai Lin, Peng Li, and Maosong Sun.
2022. Packed levitated marker for entity and relation
extraction. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,
May 22-27, 2022, pages 4904–4917. Association for
Computational Linguistics.
Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,
and Jun Zhao. 2018. Extracting relational facts by
an end-to-end neural model with copy mechanism.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 506–514, Melbourne, Australia.
Association for Computational Linguistics.
Zexuan Zhong and Danqi Chen. 2021. A frustratingly
easy approach for entity and relation extraction. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 50–61, Online. Association for Computational
Linguistics.
A
Hyperparameter Configuration
We tune the hyperparameters for each dataset by
manually trying different values of each hyperpa-
rameter within a specific interval and choosing the
2075
value that results in the highest relation F1 on the
development set (For ADE, the validation set of
its first fold of data is employed as the develop-
ment set). After trial, we find it optimal to set the
numbers of shared decoder layers, entity decoder
layers and relation decoder layers as 4, 1, 1 for all
datasets. The trial intervals and final configuration
of other hyperparameters are shown in Table 5.
B
Experiment Results on the SciERC
dataset
Here, we append the results of additional exper-
iment on the SciERC dataset. As shown in Ta-
ble 6, in terms of entity recognition, BiSPN un-
derperforms SOTA two-stage method PL-Marker
by Ent-F1, but still outperforms SOTA one-stage
methods (PFN, UniRE) substantially. In terms of
relation triple extraction, BiSPN establishes new
SOTA (Rel-F1) on the SciERC dataset. In terms
of inference speed, BiSPN is about faster than PL-
Marker, but slower than other one-stage methods.
The results after ablating the consistency loss and
the linking loss verify the effectiveness of them on
the dataset.
2076
Parameter
Trial Interval
ACE05
BioRelEx
ADE
Text2DT
SciERC
Epochs
[50, 100]
70
75
70
80
70
Warmup Rate
[0.01, 0.2]
0.1
0.1
0.1
0.1
0.1
Encoder lr
[1e-5, 5e-5]
2e-5
2e-5
2e-5
2e-5
2e-5
Decoder lr
[1e-5, 5e-5]
4e-5
4e-5
4e-5
4e-5
4e-5
Batch Size
[2, 64]
8
8
8
8
8
Me
[20, 40]
40
30
30
30
35
Mr
[20, 40]
35
25
25
30
30
α
[1e-5, 1e-2]
1e-4
1e-4
1e-3
1e-4
β
[1e-5, 1e-2]
1e-3
1e-4
1e-4
1e-3
Table 5: Configuration of Hyperparameters. lr represents the initial learning rate. Me, Mr are the number of entity
queries and the number of relation queries respectively. α, β are the weights of the bipartite consistency loss and
entity-relation linking loss respectively.
Method
Ent-F1
Rel-F1
sent/s
PFN (2021)
66.8
38.4
55.1
UniRE (2021)
68.4
36.9
79.6
PL-Marker (2022)
69.9
41.6
18.5
BiSPN (Ours)
68.9
42.0
37.2
w/o Lent
67.7
41.4
-
w/o Llink
68.0
41.3
-
w/o Lent↔rel, Llink
67.1
40.8
-
Table 6: Results on the SciERC dataset.
2077
