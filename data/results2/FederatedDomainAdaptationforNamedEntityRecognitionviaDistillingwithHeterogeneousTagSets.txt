Findings of the Association for Computational Linguistics: ACL 2023, pages 7449–7463
July 9-14, 2023 ©2023 Association for Computational Linguistics
Federated Domain Adaptation for Named Entity Recognition via Distilling
with Heterogeneous Tag Sets
Rui Wang1
Tong Yu2†
Junda Wu3
Handong Zhao2
Sungchul Kim2
Ruiyi Zhang2
Subrata Mitra2
Ricardo Henao1,4
1Duke University
2Adobe Research
3New York University
4KAUST
{rui.wang16, ricardo.henao}@duke.edu
jw6466@nyu.edu
{tyu, hazhao, sukim, ruizhang, sumitra}@adobe.com
Abstract
Federated learning involves collaborative train-
ing with private data from multiple platforms,
while not violating data privacy. We study the
problem of federated domain adaptation for
Named Entity Recognition (NER), where we
seek to transfer knowledge across different plat-
forms with data of multiple domains. In addi-
tion, we consider a practical and challenging
scenario, where NER datasets of different plat-
forms of federated learning are annotated with
heterogeneous tag sets, i.e., different sets of
entity types. The goal is to train a global model
with federated learning, such that it can pre-
dict with a complete tag set, i.e., with all the
occurring entity types for data across all plat-
forms. To cope with the heterogeneous tag
sets in a multi-domain setting, we propose a
distillation approach along with a mechanism
of instance weighting to facilitate knowledge
transfer across platforms. Besides, we release
two re-annotated clinic NER datasets, for test-
ing the proposed method in the clinic domain.
Our method shows superior empirical perfor-
mance for clinic NER with federated learning.
1
Introduction
Federated learning for Named Entity Recognition
(NER) is the task of collaboratively learn with NER
datasets from multiple platforms, while not violat-
ing data privacy, i.e., without sharing data across
different platforms (Ge et al., 2020). A platform
can be an institution, e.g., a hospital or a drug
company, where a private collection of clinic NER
dataset are locally stored. In reality, data from dif-
ferent platforms are usually sampled from different
clinic domains, due to different patient groups, etc.
Additionally, different schemes may be used when
annotating for different platforms. This happen
when healthcare providers use customized tag sets
to create their own datasets (Beryozkin et al., 2019).
As an example, a hospital may hold a dataset of
†Corresponding Author
clinical reports from doctors annotated with enti-
ties of patient Disease and Drugs prescribed by the
doctors, while a drug company may have text data
of patient feedback, annotated with Drugs and their
adverse drug effects (ADE). In this case, it would
be mutually beneficial for the hospital and the drug
company if they can train in a federated manner
a shared (global) NER model with both datasets.
The global model should in principle predict with
the complete tag set, i.e., {Disease, Drugs, ADE},
enabling the hospital to also recognize the ADE in
their clinic reports and the drug company to iden-
tify Disease from their patient feedback, without
sharing or re-annotating their local datasets. This
can be regarded as a problem of domain adaptation,
since the key challenge is to efficiently transfer
knowledge of locally unlabeled entity types, i.e.,
Disease and ADE across domains/platforms, so that
the resulting global model can work for both the
hospital and the drug company.
So motivated, we study federated domain adap-
tation for clinic NER in the multi-domain setting
where datasets from multiple platforms represent-
ing different domains. Further, we address a more
challenging scenario in which different platforms
also annotate with different tag sets, i.e., set of
entity types. The goal is to benefit all platforms
from federated learning, via training a global model
that predicts with the complete tags set, including
all the encountered entity types, for text of differ-
ent domains/platforms. Note there are previous
works studying federated NER in a multi-domain
setting (Ge et al., 2020; Zhao et al., 2021). How-
ever, these works generally presume that the NER
model for one platform only predicts with the entity
types annotated in the local training data, unlike
our setting that requires predicting on a larger tag
set (the complete tag set). Here, we claim that
such an assumption might not be practical in a
multi-domain setting. To illustrate this, suppose
there is a platform with annotations of training data
7449
Server
Drug
ADE
Disease
Finding
Symptom
Drug
ADE
Disease
Finding
Symptom
Drug
ADE
Disease
Finding
Symptom
…...
…...
Upload
updates
Platform 1
Platform 2
Platform K
Aggregate
Model parameters
Download for 
local training
Figure 1: Framework for federated learning for clinical NER with heterogeneous tag sets. Different platforms are
annotated with different tag sets, {Drug, ADE}, {Disease}, {Finding, Symptom}, etc., denoted with the unshaded
font. The goal is to learn an NER model that predict with all the entity types, while not sharing data across platforms.
In addition, we assume the text data of different platforms are from different domains (e.g., platform 1 is from a
drug company and platform 2 is from clinical reports).
that sufficiently cover enough entity types for its
own propose of evaluation. For this platform, with
enough amount of training data locally, joint train-
ing with data from other distant domains may harm
the performance of the resulting model on its local
data, i.e., there is no guarantee that data of other
platforms is similar enough to be beneficial to its
own domain. As a result, such a platform might
be reluctant in joining federated learning, further
considering the potential risk of data leakage in any
federated learning system (Li et al., 2021). On the
contrary, we require to predict with the complete
tag set, while annotating with incomplete (subset
of the complete tag set) and heterogeneous tag sets
locally. This motivates a platform to participate
in federated learning, so that it can benefit from
knowledge of locally unlabeled entity types trans-
ferred from other platforms.
To address the heterogeneous tag sets and fa-
cility knowledge transfer across platforms, with
regards to the locally unlabeled entity types, we
propose a distillation approach, that distills knowl-
edge of unlabeled entity types from other plat-
forms via pseudo-annotations with the complete
tag set. Based on the proposed distillation, we
further propose a instance weighting mechanism,
so that knowledge learned with local data is more
transferable across platforms. We adopt a prompt-
based NER model (Chen et al., 2022) with superior
performance for cross-domain NER, and only trans-
mit prompt-related parameters (7% of the model
size) for each round of federated learning to reduce
the communication cost. We should note that a
comprehensive evaluation of the global model in
the setting considered requires testing data with the
complete tag set for each domain/platform. How-
ever, existing public clinical datasets of different
domains are usually annotated using different tag
sets (with small overlap), i.e., they lack evaluation
data that is consistently annotated with the com-
plete tag sets for multiple domains. Therefore, we
re-annotate the ADE-Corpus (Gurulingappa et al.,
2012) and SMM4H (Weissenbacher et al., 2019)
datasets using the annotation scheme of CADEC
(Karimi et al., 2015), resulting in datasets of mul-
tiple domains that are annotated consistently for
evaluation. Our contributions are as follow:
• We study federated learning for clinic NER,
where data in different platforms can be from
multiple domains and annotated with hetero-
geneous tag sets.
• We propose a distillation approach along with
a weighting mechanism to facilitate knowl-
edge transfer across different platforms.
• We release two re-annotated clinic datasets for
evaluation in clinical settings and to encour-
age future research. Empirical results show
that our method delivers superior performance
in the considered setting.
2
Preliminaries
2.1
Problem Formulation
Figure 1 illustrates our considered setting for
clinic NER. Suppose there are K platforms in
federated learning with datasets {Dk}K
k=1, Dk =
7450
Algorithm 1 Our Federated Learning Algorithm.
Assume the non-trainable parameters of the NER
model are available in each platform.
Randomly initialize the trainable parameters, θ1,
for the NER model on the server of federated
learning.
Initialize the instance weights wk
i,1 = 1, for i =
1, . . . , Nk and k = 1, . . . , K (see Section 3.3).
for the tth round of federated learning do
% Update instance weighting
Compute wk
i,t+1 according to (12).
% Local training
Download θt to each local platform.
Train locally on each platform, via distilling
with the pseudo-complete annotation set re-
sulting in {θk
t }K
k=1 (see Section 3.2).
% Aggregation
Upload {θk
t }K
k=1 to the server and aggregate
with (1), generating θt+1 for the next round
of federated learning.
end for
{Xi, Y Tk
i
}Nk
i=1, with Nk being the size of Dk. Xi
is a text sequence and Y Tk
i
is its NER label se-
quence, annotated with tag set Tk. In Figure 1,
we have T1 = {Drug, ADE}, T2 = {Disease}, etc.
We assume Xi of different platforms are from dif-
ferent text domains. The goal is to train an NER
model that predicts with the complete tag set i.e.
T = ∪K
k=1Tk, for all platforms, without data being
shared across different platforms.
2.2
Federated Learning
As illustrated in Figure 1, federated learning
involves periodical communication between the
server and platforms involving the trainable param-
eters of the model. Specifically, let θt be the train-
able parameters of the NER model before the tth
communication round of federated learning. We
assume the non-trainable parameters, e.g., the pre-
trained parameters of a PLM, are available locally
in each platform. A typical training cycle of feder-
ated learning includes:
Local Training: θt is transferred to each plat-
form and is then trained/updated locally with the
private data of each platform. Specifically, θt is
trained for Eloc epochs separately on different plat-
forms. We denote {θk
t }K
k=1 as the trainable param-
eters of different platforms from local training.
Aggregation: After local training, each platform
will transfer their updated parameters {θk
t }K
k=1 to
the server. Since the goal of our federated learning
setting is to training a global model for all plat-
forms, the server will aggregate the {θk
t }K
k=1, gen-
erating θt+1 for the next round of communication.
The aggregation is usually performed via weighted
averaging, i.e.,
θt+1 =
K
X
k=1
mkθk
t ,
(1)
where P
k mk = 1. Since aggregation is not the
focus of this work, we will discuss the values of mk
in the Appendix. Algorithm 1 shows the complete
procedure of federated learning. The proposed
distillation and instance weighting mechanism are
described in Sections 3.2 and 3.3, respectively.
3
Methodology
3.1
Model Architecture
In order to efficiently train a global model for all
the participants, we need to i) Facilitate knowl-
edge transfer across different platforms/domains,
so that each client can benefit from knowledge re-
garding locally unlabeled entity types, transferred
from other platforms. ii) Reduce the communica-
tion cost of federated learning. With these consid-
erations, we adopt LightNER (Chen et al., 2022) as
our NER model for federated learning. Below, we
briefly describe the LightNER model, along with
the rationale that we adopt it for our setting.
Sequence-to-Sequence NER: NER is convention-
ally identified a sequence labeling problem, which
predicts with a label-specific output layer (Luo
et al., 2020; Lee et al., 2020) on top of a Pre-
trained Language Model (PLM), e.g., BERT. How-
ever, such models may have inferior performance
for cross-domain problems, since the label-specific
output layer that is trained from scratch cannot
benefit from the pretrained knowledge for gener-
alization (Chen et al., 2022). To solve this, recent
works (Cui et al., 2021; Chen et al., 2022) adopt a
sequence-to-sequence framework for NER based
on the pretrained BART model (Lewis et al., 2019),
where the entity labels are predicted as natural lan-
guage tokens in the output sequence, leveraging
the pretrained semantics knowledge from BART
token embeddings for better generalization. By for-
mulating NER as sequence-to-sequence generation,
LightNER achieve superior performance for cross-
domain NER tasks, a merit that we value for our
setting involving multiple domains. Given a length-
L text sequence Xi = [xl]L
l=1 from platform k,
7451
the model should generate the following label se-
quence Y T k
i
, indicating the start/end positions and
entity types of each entity within Xi,
Y T k = [pT k
1 ; · · · ; pT k
n ], pT k
c
= [sc, ec, tc], (2)
where c = 1, · · · , n and [ ; ] denotes concatenation.
n is the number of entities in Xi. pT
c denotes
the cth entity annotated within Xi, where sc/ec
denotes its start/end position in Xi, and tc ∈T k is
the entity type.
LightNER follows the encoder-decoder architec-
ture of BART, generating the label sequence Y T
i
autoregressively, given Xi. The LightNER model
for platform k can be trained via minimizing the
following loss of cross-entropy,
LT k
i
=−
3n
X
l=1
log Pθ(yT
l |Xi, yT k
1 , · · · , yT k
l−1), (3)
where yT k
l
is the lth element of Y T k
i
and θ denotes
the trainable parameters of LightNER.
Prompt Tuning:
To preserve the pretrained
knowledge from BART for better generalization
across domains/platforms, we follow LightNER
that freezes the pretrained parameters of BART
and inserts tunable prompt parameters for training.
Specifically, let q ∈RNq×D denote an array of
Nq prompt tokens, where we have Nq = 10 and
d = 768. q is projected by a trainable layer into
the keys and values of the self-attention in each
pretrained transformer layer, with q being shared
by all layers. The projection on q follows (Chen
et al., 2022) and is detailed in Appendix B. As a
result, the number of trainable parameters in the
model is significantly reduced, i.e., only 7% of the
total model size, compared with fine-tuning all the
model parameters. This leads to reduced communi-
cation cost for federated learning, considering that
we only need to communicate trainable parameters
between the server and platforms.
3.2
Distillation with Pseudo-Complete
Annotation
The local datasets of each platform only contain
annotations of {T k}K
k=1, T k ⊂T . For platform k,
we denote entity types that are not annotated locally
as T \k, with T ∪T \k = T . During local training,
if the local trainable parameter θk
t (Algorithm 1)
is trained solely with the local annotations of T k,
the resulting NER model will learn to ignore the
entities of T \k from the input text sequences. This
contradicts our goal of predicting with the complete
tag set T . To solve this problem, we notice that
the parameter θt in Algorithm 1 is aggregated from
updates of different platforms ({θk
t−1}K
k=1) with
{T k}K
k=1. Thus, the NER model with θt should
be able to predict with the complete tag set T , in-
cluding T k from each platform k. Additionally,
considering that θt is downloaded to each platform
before the local training of the tth round of fed-
erated learning (Algorithm 1), the model with θt
should be locally available for each platform. In-
spired by this, we propose to distill from the model
with θt while training locally with θk
t , so that θk
t
can be trained with T instead of T k.
Specifically, we extract predictions regarding
T \k from the the model with θt and combine them
with the local annotations of T k, constituting the
pseudo-complete annotation. θk
t will be trained
with the pseudo-complete annotation of the com-
plete tag set T . Let Xi be the ith text sequence
from platform k and Y T k
i
= [pT k
1 ; · · · ; pT k
nloc] be
the local annotation regarding T k. nloc is the num-
ber of locally annotated entities of T k. Given Xi,
we first greedly decode the prediction ˆY T
i
from θt,
ˆyT
l = arg maxy Pθt(y|X1, ˆyT
1 , · · · , ˆyT
l−1). (4)
As mentioned above, the prediction ˆY T
i
should
have the complete tag set T . Predictions regard-
ing T \k within ˆY T
i
represents the knowledge of
un-annotated entity types in platform k, which
is transferred from other platforms. We extract
such predictions from ˆY T
i , denoted as, ˆY T \k
i
=
[pT \k
1
; · · · ; pT \k
ntrans]. ntrans is the number of enti-
ties in ˆY T \k
i
. {pT \k
c
}ntrans
c=1
is defined as in (2), rep-
resenting entities that are predicted as types from
T \k with θt. We combine ˆY T \k
i
with the existing
annotation Y T k
i
from platform k, generating the
pseudo-complete annotation,
Y T
i =[Y T k
i
; ˆY T \k
i
] = [pT
1; · · ·; pT
nloc+ntrans], (5)
where each entity is from either Y T k
i
or ˆY T \k
i
.
Y T
i
is constructed to cover the complete tag set
T , illustrated in Figure 2. For local training, θk
t is
trained with the pseudo-complete annotation Y T
i
instead of Y T k
i
, with the loss of (3).
3.3
Instance Weighting
With (5), θk
t
is expected to be trained with
{Xi, Y T
i }Nk
i=1 during local training. Let yl be the
lth element of Y T
i , which can be categorized as
7452
: Clozapine induced akathisia in children with schizophrenia.
Model of  
Clozapine, Drug, Drug, akathisia, ADR, with schizophrenia, Disease
akathisia, ADR
1) extract
Clozapine, Drug,  schizophrenia, Disease
Clozapine, Drug, akathisia, ADR, schizophrenia, Disease
Model of  
2) combine with
3) train
Figure 2: Constructing the pseudo-complete annotation Y T
i
for training of θk
i . For platform k, we show entities of
T k with blue and T \k as red. For simplicity, we denote the entity pc = [sc, ec, tc] in the model output directly as
the entity span followed by the entity type. For example, pc = [0, 1, Drug] is denoted as "Clozapine, Drug". Note
that the output with θt may have irregular subsequences, e.g., "Drug, Drug". We discard every output entity type
without appended by a text span. For this case, the second Drug is excluded from ˆY T
i
in implementation.
either yl ∈Y T k or yl ∈ˆY T \k
i
. The training loss
can be decomposed as,
LT = LT k + LT \k
(6)
= 1
Nk
Nk
X
i=1
LT k
i
+ 1
Nk
Nk
X
i=1
LT \k
i
,
(7)
where,
LT k
i
=−
X
yl∈Y T k
i
log Pθk
t (yl|X, yT
1 ,· · ·, yT
l−1),
(8)
LT \k
i =−
X
yl∈ˆY T \k
i
log Pθk
t (yl|X, yT
1 ,· · ·, yT
l−1).
(9)
{LT k}K
k=1 represents training with the local anno-
tations of T k. The knowledge learnt from LT k
will be transferred to other platforms where anno-
tations of T k is not available. Correspondingly,
{LT \k}K
k=1 represents how platform k can benefit
from knowledge of T \k that is transferred from
the other platforms where annotations for T \k
are available. For platform k, the model is ex-
pected to benefit from the knowledge learnt with
{LT k′
}k′̸=k, regarding entity types that are not lo-
cally annotated (T \k), so that it can identify entities
of T \k via training with LT \k. With this perspec-
tive, we denote {LT k}K
k=1 and {LT \k}K
k=1 as the
source and target loss, respectively, in terms of the
direction of knowledge transfer.
To facilitate the knowledge transfer across plat-
forms discussed above, we propose a weighting
mechanism for the training instances of the source
loss {LT k}K
k=1, so that the knowledge learnt from
the source loss can be more transferable for the
target loss {LT \k
t
}K
k=1. Specifically, we want to
upweight instances that are more beneficial for the
training in other platforms and vise versa. Formally,
we rewrite the source loss as,
LT k = 1
Nk
Nk
X
i=1
wk
i,t × LT k
i
,
(10)
where wk
i,t = 1 reduces to (7). wk
i,t is the weight
for the ith sample for platform k at the tth feder-
ated learning round, measuring how the knowledge
from training with LT k
i
(source) is transferable for
the target loss in other platforms, i.e., {LT \k}k′̸=k
(target). For conciseness, we omit the subscript t
that denotes the number of federated learning round
in presenting the loss functions, but only showing
it for the weight wk
i,t.
The question remaining is how to measure the
transferablility of knowledge learnt from LT k
i
in
the federated learning setting. Since the federate
learning is a privay-preserving framework that only
allows communicating model updates between the
server and platforms, we define wk
i,t according to
the gradient similarity between the source and tar-
get loss. Specifically, for the ith sample of platform
k, we first compute the gradients of its source loss
and mean of the target loss from other platforms,
which we denote as gsrc
i
and gtgt, respectively,
gsrc
i
= ∂LT k
i
∂q , gtgt =
X
k′̸=k
∂(LT \k)
∂q
,
(11)
q is the prompt embeddings as introduced in Sec-
tion 3. wk
i,t is updated with the cosine similarity
7453
between the two gradients,
wk
i,t+1 = α·wk
i,t+(1−α)· < gsrc
i
, gtgt >
||gsrc
i
||2||gtgt||2
, (12)
where α is a momentum value. < ·, · > denotes
the dot product and || · ||2 is the L2 norm. wk
i,t
is computed before local training (Algorithm 1).
For platform k, we save gsrc
i
locally and upload
the gradient of the target loss LT \k to the server
for computing gtgt. gtgt is computed on the server
side, then downloaded to each platform for updat-
ing wk
i,t with (12). We further elaborate the proce-
dures of updating wk
i,t in Algorithm 2. Note that
updating wk
i,t does not involving training of the
NER model and wk
i,t is not shared to the server or
other platforms. We should also notice that the
above uploading and downloading of gradients in-
troduce additional communication cost. With such
concern, we only compute gradients with respect
to q ∈RNq×d (as in (11)), which has only several
thousand parameters (Section 3), inducing only mi-
nor communication cost. We use q to calculate the
gradient similarity, because q is shared by each pre-
trained layer in BART (Section 3), thus it should
correspond to the general information regarding
prompt tuning.
4
Related Works
NER with Heterogeneous Tag Sets. Greenberg
et al. (2018); Beryozkin et al. (2019) investigate on
training over NER datasets with heterogeneous tag
sets. However, they assume these datasets are avail-
able in a centralized location. Such an assumption
is not practical in training with clinical data, for
which privacy preservation is of primary concern
(Hassan Mahlool and Hamzah Abed, 2022). Ad-
ditionally, they do ot explicitly consider the differ-
ences in data distribution for the text from different
datasets. Our work is orthogonal to these works,
since we assume decentralized training, i.e., feder-
ated learning, where we account for the issues of
privacy and communication costs that do not exist
in training with centralized datasets.
Federated Domain Adaptation Peng et al. (2019)
is the first work studying domain adaptation for
federated learning. Recently, Hong et al. (2021)
further studies the fairness and debasing problem in
federated domain adaptation. These works adopt a
discriminator module for adversarial domain adap-
tation, which increases the communication cost of
federated learning. Yao et al. (2022) studies fed-
erated domain adaptation via sharing statistics of
data distributions of the local platforms. However,
such an approach may be vulnerable to member-
ship inference attacks (Shokri et al., 2017), result-
ing in data leakage, thus may not be applicable to
clinical data for which data privacy is the primary
concern. Additionally, these work only consider
the task of image classification. Our work stud-
ies federated domain adaptation for clinical NER.
Note that federated domain adaptation is different
from federated learning with non-IID (Independent
and Identically Distributed) data (e.g.,, (Li et al.,
2020)). The latter focus on the problem with slow
convergence or diverged results in aggregating with
updates from non-IID data. Instead, we targets
at effectively transferring knowledge across plat-
forms/domains, so that each platform can benefit
from knowledge of locally unannotated entity types
transferred from other platforms.
Federated Learning for NER. Ge et al. (2020)
presents a pilot study of federated learning for clin-
ical NER. Zhao et al. (2021) introduces adversarial
training to solve the adversarial attack problem
for federated NER. One of the major problems
is that these approaches require sharing or com-
municating the whole NER model (or its encoder)
between the server and platforms of federated learn-
ing. This will induce huge communication cost in
training with the recent Pretrained Langauge Mod-
els (PLMs) (Kenton and Toutanova, 2019; Lewis
et al., 2019), i.e., containing hundreds of millions of
parameters. In this work, we study using a prompt-
based pretrained NER model (Chen et al., 2022)
for our federated learning, thus only communicat-
ing prompt-related parameters. This significantly
reduces the communication cost compared to fine
tuning all the pretrained parameters. Further, dif-
ferent from Ge et al. (2020); Zhao et al. (2021),
we focus on federated domain adaptation that ef-
ficiently transfer knowledge among platforms of
different domains. (Wu et al., 2021) investigates
knowledge distillation in federated learning with
NER, but is not targeting the federated domain
adaptation problem as in our setting.
5
Experiments
5.1
Baselines and Ablations
We first compare with the classic adversarial do-
main adaptation with (Ganin et al., 2016), and two
more recent works of federated domain adapta-
7454
tion (Peng et al., 2019; Hong et al., 2021). Note
that these methods are originally designed for im-
age classification. We re-implement them with
our NER model, i.e., LightNER, for comparison.
Please refer to Appendix A for details.
Note that these approaches generally require an
additional domain discriminator for adversarial do-
main matching. Such discriminator is trained and
communicated along with the NER model. This
introduces additional communication cost, as with
uploading and downloading the gradients of q in
Section 3.3. In the Appendix A, we compared
the communication cost of our instance weighting
with q with that of the discriminator. Our com-
munication cost is lower, while achieving better
performance as in Table 1 and 2.
We denote training with Algorithm 1 as Ours.
For the ablation study, we consider: (i) Ours w/o
distill&weight. This is to train the LightNER model
without distillation in Section 3.2 and instance
weighting in Section 3.3. Specifically, the model
is trained with only the local annotation Y T k in-
stead of (5) , and wt
i,k is always set to 1. (ii) Ours
w/o weight. It trains the NER model with (5) (as
in Ours), while setting wt
i,k = 1, i.e., no instance
weighting. Please refer to the appendix C for im-
plementation details.
5.2
Experiments with OntoNote 5.0
Before evaluating with clinic data, we first demon-
strate our method with OntoNote 5.0, a classic
NER dataset of 18 entity types (|T | = 18), with
data from six domains: nw, tc, wb, bn, bc and mz.
We have the number of platforms K = 6, with
each platform representing data of a different do-
main. To simulate the heterogeneous tag sets, we
assume the training data of each domain/platform
is annotated with 3 entity types (|T k| = 3), which
are randomly sampled from the 18 entity types
without replacement. For OntoNote 5.0, we study
the challenging scenario of federated domain adap-
tation that each entity type is only annotated in
one of the six platforms, i.e., T k1 ∩T k2 = ∅, for
k1, k2 ∈{1, · · · , K} and k1 ̸= k2. We randomly
sample five time and report the F1 score for each
domain, averaged over different samplings. For
each domain, the F1 score is computed via evaluat-
ing the global model on the testing dataset of this
domain with all the 18 entity types.
Table 1 shows the resulting F1 score with
OntoNote 5.0. Our method outperforms the base-
lines with a large margin. Instead of communicat-
ing domain discriminators as baselines, we commu-
nicate the gradients of prompt embeddings, which
has a smaller size (Appendix A). Additionally, the
performance gain from Ours w/o distill&weight to
Ours w/o weight shows the effectiveness of our dis-
tillation with pseudo-complete annotation (Section
3.2), which allows the NER model being trained
with the complete tag set during local training. Sim-
ilarily, the the performance gain from Ours w/o
weight to Ours validates the usefullness of our pro-
posed instance weight mechanism. Both of these
techniques contribute to the superior performance
of our trained NER model.
5.3
Experiments with the Clinic Datasets
As mentioned in Section 1 and 5.2, the evaluation
of our NER model requires testing data of different
domains with complete tag sets. However, existing
public clinic datasets are generally created with dif-
ferent annotation schemes. For example, datasets
may be annotated with different tags sets (Bery-
ozkin et al., 2019; Karimi et al., 2015), and even
the same entity type can have various definitions in
different datasets (Karimi et al., 2015). Such a lack
of consistent annotations for clinic data of differ-
ent domains poses challenges to the evaluation of
our considered setting. Broadly speaking, this also
add to the difficulty in studying general transfer
learning problems with clinic NER. For instance,
the classic domain adaptation (Long et al., 2015)
generally involves transferring knowledge from a
labeled source domain to an unlabeled target do-
main. The resulting model is evaluated with testing
data of the target domain, annotated with the same
classes/entity types as in the source domain, i.e., re-
quiring consistent annotation for data of the source
and target domain, which is hardly fullfilled with
public datasets when dealing with clinic NER.
To solve this problem, we take three clinic
datasets: CADEC (Karimi et al., 2015), ADE
(Gurulingappa et al., 2012) and SMM4H (Weis-
senbacher et al., 2019), which contains text from
three distinct text domains, i.e., formal costumer
report, medical case report and casual tweets, re-
spectively. We provide some samples of the three
dataset in the Supplymentary data. These datasets
are originally annotated with different tag sets. To
have consistent annotation across domains. We
re-annotate ADE and SMM4H with the tag sets
defined in CADEC (with the largest tag set). As
7455
Method
nw
tc
wb
bn
bc
mz
Avg
(Ganin et al., 2016)
58.75
56.95
57.31
57.57
58.38
58.15
57.85
(Peng et al., 2019)
61.32
58.32
59.64
60.10
59.75
60.09
59.87
(Hong et al., 2021)
59.27
55.27
55.86
58.16
56.79
57.33
57.11
Ours w/o distill&weight
45.10
52.15
49.42
46.52
47.55
46.71
47.91
Ours w/o weight
59.76
55.09
52.66
58.47
57.00
58.42
56.90
Ours
61.67
59.92
59.52
62.41
60.57
63.22
61.22
Table 1: F1 Scores with OntoNote 5.0.
Method
ADE
CADEC
SMM4H
Avg
(Ganin et al., 2016)
66.89
57.26
60.70
61.62
(Peng et al., 2019)
63.77
55.38
58.85
59.33
(Hong et al., 2021)
67.49
57.56
59.02
61.36
Ours w/o distill&weight
41.92
41.68
45.81
43.14
Ours w/o weight
66.80
54.91
60.29
60.67
Ours
69.25
55.68
62.22
62.38
Table 2: F1 Scores with Clinic Datasets.
a result, the three datasets are consistently anno-
tated with the same tag set of 5 entity types, T =
{Drug, ADE, Disease, Finding, Symptom}, as de-
fined in (Karimi et al., 2015). In Appendix D, we
also elaborate our annotation procedure and dataset
statistics. In simulating our setting of federated
domain adaptation with the above datasets, we set
the number of local platform K = 3. Each plat-
form holds text data of a different domain/dataset.
Unlike OntoNote 5.0, we consider a more flexi-
ble and practical scenario that allows overlapping
among tags set of different platforms. Please refer
to Appendix D on the tag sets of annotation for
each local platform in experiments.
Table 2 shows the results of federated domain
adaptation with our clinic datasets. Our method has
the highest F1 score averaged over the three con-
sidered datasets/domains. Among the three client
datasets, CADEC is larger and more diverse than
ADE and SMM4H. Thus, CADEC may contain
samples that are quite different from those in ADE
and SMM4H, and knowledge learnt with such sam-
ples may not be transferable for the training of
ADE and SMM4H. From our weighting mecha-
nism (10), such samples can be downweighted dur-
ing training to facilitate knowledge transfer across
platforms. Since such downweighted samples may
be important for the local training with CADEC,
the improvement for CADEC with our weighting
mechanism is slightly smaller than that on the other
two clinic datasets. However, we should note that
our proposed method can consistently provide im-
provement over the ablations for different datasets.
Table 2 also shows that our annotations for ADE
and SMM4H are meaningful, and can be leveraged
for the training of existing advanced NER model
(Chen et al., 2022). To faciliate future research, we
have released our annotated clinic datasets†.
5.4
Hyperparameter Analysis
Let η be the percentage of trainable parameters in
the NER model, which is proportional to the com-
munication cost during federated learning. In order
to investigate the relation between the communi-
cation cost of federated learning and the model
performance, we vary the value of η and plot η
with the averaged F1 score on OntoNote 5.0 in
Figure 3 (a). η is varied by changing the hidden
dimension h of the projection on q, explained in
Appendix B. Results in Figure 3 (a) shows that,
when η is not large (η ≤10), the model perfor-
mance can be improved with larger communication
cost (larger η). However, when the value of η gets
large enough (e.g., η ≥10), the model may overfit
to the domain specific information of each client
during local training, hindering the further improve-
ment of model performance.
Figure 3 (b) shows the F1 score on OntoNote
5.0, with varing values of Eloc, i.e. epoches of
local training per round. All the points share the
same communication cost, with the same η and
communication rounds for federated learning. The
model performance generally improves with longer
local training (larger Eloc). We should note that
increasing Eloc corresponds to larger computation
†https://github.com/RayWangWR/ClinicDataset
7456
cost in local training. The performance gets satu-
rated when Eloc get too large, i.e., Eloc ≥2, which
indicated that the local training may have reached
convergence after 2 epoches.
6
Conclusion
In this work, we study the problem of federated
domain adaptation for clinic NER. We consider
the practical setting with heterogeneous tag sets
for different platforms of federated learning. To
cope with the heterogeneous tag sets and facilitate
knowledge transfer among different platforms, we
propose distillation with pseudo-complete anno-
tation and an instance weighting mechanism. In
addition, we will release two re-annotated clinic
datasets for our considered setting. In experiments,
our trained NER model show superior performance
in the considered setting.
7
Limitations
Our work is base on the existing sequence-to-
sequence NER model, since its way of decoding
has been shown effective for knowledge transfer
between different classes (Chen et al., 2022). How-
ever, it might also be valuable to consider other
token-classification-based or CRF-based (Sutton
et al., 2012) NER models. Especially, it would be
interesting to employ the existing CRF-based dis-
tillation method (Wang et al., 2020b) to cope with
the problem of heterogeneous tag sets for NER.
8
Acknowledgements
This research was supported by ONR N00014-18-1-
2871-P00002-3. The student involved was also sup-
ported by Adobe gift research funding. We would
like to thank the anonymous reviewers for their
insightful comments. Moreover, we want to thank
Billy I. Kim for his dedicated efforts in screening
the annotations.
References
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deb-
orah Estrin, and Vitaly Shmatikov. 2020. How to
backdoor federated learning. In International Con-
ference on Artificial Intelligence and Statistics, pages
2938–2948. PMLR.
Genady Beryozkin, Yoel Drori, Oren Gilon, Tzvika
Hartman, and Idan Szpektor. 2019. A joint named-
entity recognizer for heterogeneous tag-sets using a
tag hierarchy. arXiv preprint arXiv:1905.09135.
Xiang Chen, Lei Li, Shumin Deng, Chuanqi Tan,
Changliang Xu, Fei Huang, Luo Si, Huajun Chen,
and Ningyu Zhang. 2022. Lightner: a lightweight
tuning paradigm for low-resource ner via pluggable
prompting. In Proceedings of the 29th International
Conference on Computational Linguistics, pages
2374–2387.
Leyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang.
2021. Template-based named entity recognition us-
ing bart. arXiv preprint arXiv:2106.01760.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The
journal of machine learning research, 17(1):2096–
2030.
Suyu Ge, Fangzhao Wu, Chuhan Wu, Tao Qi, Yongfeng
Huang, and Xing Xie. 2020.
Fedner: Privacy-
preserving medical named entity recognition with
federated learning. arXiv preprint arXiv:2003.09288.
Nathan Greenberg, Trapit Bansal, Patrick Verga, and An-
drew McCallum. 2018. Marginal likelihood training
of bilstm-crf for biomedical named entity recognition
from disjoint label sets. In Proceedings of the 2018
conference on empirical methods in natural language
processing, pages 2824–2829.
Harsha
Gurulingappa,
Roman
Klinger,
Martin
Hofmann-Apitius, and Juliane Fluck. 2010.
An
empirical evaluation of resources for the identifica-
tion of diseases and adverse effects in biomedical
literature.
In 2nd Workshop on Building and
evaluating resources for biomedical text mining (7th
edition of the Language Resources and Evaluation
Conference), pages 15–22.
Harsha Gurulingappa, Abdul Mateen Rajput, Angus
Roberts, Juliane Fluck, Martin Hofmann-Apitius, and
Luca Toldo. 2012. Development of a benchmark
corpus to support the automatic extraction of drug-
related adverse effects from medical case reports.
Journal of biomedical informatics, 45(5):885–892.
Dhurgham
Hassan
Mahlool
and
Mohammed
Hamzah Abed. 2022.
A comprehensive sur-
vey on federated learning: Concept and applications.
arXiv e-prints, pages arXiv–2201.
Junyuan Hong, Zhuangdi Zhu, Shuyang Yu, Zhangyang
Wang, Hiroko H Dodge, and Jiayu Zhou. 2021. Fed-
erated adversarial debiasing for fair and transferable
representations. In Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery &
Data Mining, pages 617–627.
Sarvnaz Karimi, Alejandro Metke-Jimenez, Madonna
Kemp, and Chen Wang. 2015. Cadec: A corpus of ad-
verse drug event annotations. Journal of biomedical
informatics, 55:73–81.
7457
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
Toutanova. 2019. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. In
Proceedings of NAACL-HLT, pages 4171–4186.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon
Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.
2020. Biobert: a pre-trained biomedical language
representation model for biomedical text mining.
Bioinformatics, 36(4):1234–1240.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461.
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo
Wang, Yuan Li, Xu Liu, and Bingsheng He. 2021. A
survey on federated learning systems: vision, hype
and reality for data privacy and protection. IEEE
Transactions on Knowledge and Data Engineering.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar San-
jabi, Ameet Talwalkar, and Virginia Smith. 2020.
Federated optimization in heterogeneous networks.
Proceedings of Machine Learning and Systems,
2:429–450.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael
Jordan. 2015. Learning transferable features with
deep adaptation networks. In International confer-
ence on machine learning, pages 97–105. PMLR.
Ying Luo, Fengshun Xiao, and Hai Zhao. 2020. Hi-
erarchical contextualized representation for named
entity recognition. In Proceedings of the AAAI con-
ference on artificial intelligence, volume 34, pages
8441–8448.
Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate
Saenko. 2019. Federated adversarial domain adapta-
tion. arXiv preprint arXiv:1911.02054.
Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. 2017. Membership inference attacks
against machine learning models. In 2017 IEEE sym-
posium on security and privacy (SP), pages 3–18.
IEEE.
Charles Sutton, Andrew McCallum, et al. 2012. An in-
troduction to conditional random fields. Foundations
and Trends® in Machine Learning, 4(4):267–373.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dim-
itris Papailiopoulos, and Yasaman Khazaeni. 2020a.
Federated learning with matched averaging. arXiv
preprint arXiv:2002.06440.
Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Fei
Huang, and Kewei Tu. 2020b. Structure-level knowl-
edge distillation for multilingual sequence labeling.
arXiv preprint arXiv:2004.03846.
Davy Weissenbacher, Abeed Sarker, Arjun Magge, Ash-
lynn Daughton, Karen O’Connor, Michael Paul, and
Graciela Gonzalez. 2019. Overview of the fourth so-
cial media mining for health (smm4h) shared tasks at
acl 2019. In Proceedings of the fourth social media
mining for health applications (# SMM4H) workshop
& shared task, pages 21–30.
Chuhan Wu, Fangzhao Wu, Ruixuan Liu, Lingjuan Lyu,
Yongfeng Huang, and Xing Xie. 2021. Fedkd: Com-
munication efficient federated learning via knowl-
edge distillation. arXiv preprint arXiv:2108.13323.
Chun-Han Yao, Boqing Gong, Hang Qi, Yin Cui, Yukun
Zhu, and Ming-Hsuan Yang. 2022. Federated multi-
target domain adaptation.
In Proceedings of the
IEEE/CVF Winter Conference on Applications of
Computer Vision, pages 1424–1433.
Hanyu Zhao, Sha Yuan, Niantao Xie, Jiahong Leng, and
Guoqiang Wang. 2021. A federated adversarial learn-
ing method for biomedical named entity recognition.
In 2021 IEEE International Conference on Bioinfor-
matics and Biomedicine (BIBM), pages 2962–2969.
IEEE.
A
Implementation of Baselines
Below we talk about our considered baselines.
(Ganin et al., 2016): It aligns the features of
different domains via adversarial matching, using
a domain discriminator. We add a K way domain
discriminator, on the hidden states of every layer
in the the encoder of our model NER model. The
discriminator will try to classify the domain from
which the data of the hidden states is generated.
(Peng et al., 2019): In addition to adversarial
matching with a discriminator, (Peng et al., 2019)
also consider enhancing cross-domain generaliza-
tion via disentangling the task-specific information
from the domain-specific information. Therefore,
apart from using the discriminator, we also add
the disentanglement loss on the last layer of the
decoder of our NER model.
(Hong et al., 2021): Similar to (Ganin et al.,
2016) and (Peng et al., 2019), (Hong et al., 2021)
also uses a K way discriminator for adversarial
domain matching. The difference is that it adopt
a squared adversarial loss during training, for fair-
ness among local platforms. Additionally, it mini-
mize the prediction entropy of image classification
for unlabeled samples. In order to adapt it to our
case where the prediction is a label sequence (in-
stead of a single label), we minimize the prediction
entropy on the tokens of ˆY T \k
i
.
As mentioned in Section 5.1, these approaches
generally requires a domain discriminator that is
7458
Algorithm 2 Algorithm for Instance Weighting
Input: wk
i,t, i = 1, · · · , Nk, k = 1, · · · , K.
Output: wk
i,t+1, i = 1, · · · , Nk, k = 1, · · · , K.
% Compute and save the gradients of the source
and target loss.
for k = 1, · · · , K do
for i = 1, · · · , Nk do
Compute the source and target loss, LT k
i
and LT \k
i
, respectively.
Backpropagate for the gradients ∂LT k
i
∂q
and
∂LT \k
i
∂q
, while not updating the model.
end for
Save gsrc
i
= ∂LT k
i
∂q
locally.
Compute ∂LT \k
∂q
= PNk
i=1
∂LT \k
i
∂q
, and upload
to the server.
end for
% Update the weights with via cosine similarity
between gradients.
for k = 1 · · · , K do
Compute gtgt = P
k′̸=k
∂(LT \k)
∂q
on the server
and download to platform k.
for i = 1, · · · , Nk do
Compute the cosine similarity between gsrc
i
and gtgt on platform k.
Update wk
i,t to wk
i,t+1 according to (12).
end for
end for
trained and communicated along with the model,
increasing the communication cost. We use the
same K way discriminators for all the baselines.
For each layer of BART encoder in LightNER, we
add a K way discriminator with a single linear
layer. For these discriminators of each layer, the
only parameter is a matrix of size K × d, with d
being the hidden dimension of the BART encoder.
Communication cost: We quantify it as the num-
ber of trainable parameters involved in the model.
Since the BART encoder has 12 layers, the commu-
nication cost of the discriminators is 12 × K × d,
which is 72 × d for OntoNote 5.0 and 36 × d for
the clinic datasets.
Comparitively, the communication cost for up-
dating our instance weighting is Nq×d, i.e., 10×d,
since we have Nq = 10. Therefore, our instance
weighting has less communication cost than the
discriminators.
B
Details of Prompt Implementation
We following (Chen et al., 2022) in implement-
ing the prompt in Section 3. Generally speaking,
(Chen et al., 2022) insert an array of key embed-
dings and value embeddings into the self-attention
module of each transformer layer in BART (Lewis
et al., 2019). The inserted key embeddings and
value embeddings are denoted as ΦK ∈RNq×d
and ΦV ∈RNq×d, respectively. Let Xl be the
input of a transformer layer in BART. The self-
attention module first projects Xl into embeddings
of the key (Kl), query (Ql) and value (V l),
Kl = XlW K, Ql = XlW Q, V l = XlW V
(13)
where W K, W Q, W V ∈Rd×d are the project
matrices. The self-attention output with inserted
ΦK and ΦV can be computed as,
outputl = softmax(Ql[Kl; ΦK]⊺
r
d
)[V l; ΦV ]r
(14)
where outputl denote the output from self-
attention. [; ]r denotes row concatenation. ΦK
and ΦV are projected from the prompt q ∈RNq×d
in Section 3,
[ΦK; ΦV ]c = W l
2Tanh(W l
1q)
(15)
where [; ]c denotes column concatenation. Tanh
is the tangent activation. W l
1 ∈Rd×h and W l
1 ∈
Rh×2d are two trainable linear projections for a
transformer layer. h is the hidden dimension, con-
trolling the size of trainable parameters.
C
Experiment Details
In the experiments, we show results with Nq = 10,
d = 768 and h = 400. With such configuration,
the trainable parameters (those need to be commu-
nicated) only takes up 7.04% (η = 7.04) of the
model size, significantly reducing the communi-
cation cost compared to finetuning the full model.
The model is locally trained for 1 epoch before be-
ing upload for aggregation, i.e., Eloc = 1 (Section
2.2), and train with 25 rounds of communication.
We fix the pretrained BART parameters in Light-
NER, only training and communicating the train-
able parameters for federated learning. Our model
is trained with learning rate 3e-5 and batch size 8.
We empirically set the momentum value α = 0.9.
We train with a single GPU with pytorch 1.7.0 and
python 3.8. For the weights of aggregation in equa-
tion (1), {mk}K
k=1, we initially tried with FedAvg
7459
Table 3: Statistics of the Clinic Datasets.
Dataset
# Sentences
# Drug
# ADE
# Disease
# Finding
# Symptom
ADE (Gurulingappa et al., 2012)
4258
4077
4652
1169
89
126
SMM4H (Weissenbacher et al., 2019)
1226
1471
1414
135
26
20
2
4
6
8
10
12
Percentage of Trainable Parameters (%)
54
55
56
57
58
59
60
61
62
63
F1 Score
(a)
0
1
2
3
4
5
6
Epoches per Round
40
45
50
55
60
65
F1 Score
(b)
Figure 3: Hyperparameter analysis with (a) the percentage of trainable parameters, η, and (b) epoches of local
training per round, Eloc. We show the averaged F1 score over the six domains of OntoNote 5.0.
(Wang et al., 2020a) that set mk as proportional
to the size of the dateset in its corresponding do-
main. However, we found this will lead to inferior
results for platforms whose dataset is small in size.
Therefore, we set the weights {mk}K
k=1 as uniform.
D
The Clinic Datasets
The labeling procedure:
We annotate the
text corpous of ADE (Gurulingappa et al.,
2012) and SMM4H (Weissenbacher et al., 2019)
with a tag set of 5 entity types, i.e., T
=
{Drug, ADE, Disease, Finding, Symptom}, follow-
ing the definition as in the original paper of
CADEC (Karimi et al., 2015). Following (Gurulin-
gappa et al., 2010),we have two annotators that
can discuss on the disagreement. We split the text
of ADE (Gurulingappa et al., 2012) and SMM4H
(Weissenbacher et al., 2019) into batches of 100
sentences. The annotators will work on stream-
ing of batches, and annotating each batch takes
about an hour. To ensure the quality of the result-
ing annotation, we also include a medical student
from a clinical institution, in addition to the two
annotators, to decide on sample for which the two
annotators are not confident. The medical student
and the two annotators are all student volunteers,
who are also contributing to the methodology and
experiments of this research project and credited
with their names included in the paper author list.
Table 3 show the statistics of our annotations, re-
garding the number of sentences and identified enti-
ties. We have also removed some of the duplicated
sentences in SMM4H.
Simulating heterogeneous tag sets for different
platforms: As in Section 5.3, our experiments with
the clinic datasets consider three platforms for fed-
erated learning. During the experiments, we spec-
ify different sets of annotated entity types (T k) for
different platforms to simulate local training with
heterogeneous tag sets. For instance, if T k is speci-
fied as annotated in platform k, then annotations of
T \k will be ignored in this platform. {T k}K
k=1 are
specified such that each platform contains at least
one annotated entity types whose annotations are
not available in the other platforms. Formally, for
each platform k, there exist at least one s ∈T k,
s.t., s /∈T k′, k′ ̸= k. In this way, we simulate a
practical scenario that each platform will have its
unique contribution to the federated learning sys-
tem, via enabling the global model to recognize at
least one entity types whose annotations are only
available in this platform. Such a setting is based
on the consideration that including more platforms
in the federated learning system may increase the
risk of backdoor attack (Bagdasaryan et al., 2020)
and privacy leakage (Li et al., 2021). Therefore,
it is realistic that a platform is allow to participate
in federated learning only if it can make unique
contributions to the global model, i.e., enabling the
global model to recognize entity types that are not
7460
annotated in other platforms. Additionally, since
there are 3 platforms, we allow each entity types
to be annotated in at most 2 platforms. This is be-
cause it is less necessary for knowledge of a certain
entity type to be transferred across platforms, if all
the three platforms have already had its annotation.
As in Section 5.3, we experiment with 3 plat-
forms (K = 3) using the clinic datasets, with
text of each platform being from a unique clinic
dataset. In determining the T k for each platform,
we first randomly (uniformly) sample three differ-
ent entity types (Drug, ADE and Disease as an
example) from T , one for each platform. Each of
the sampled entity types is specified as uniquely
annotated in its associated platform. Then, for
each of the rest of the entity types, denoted as
s, (s ∈{Finding, Symptom} in this example), we
first randomly decide whether the it is annotated in
n ∈{1, 2} platforms, with a bernoulli distribution
of probability 0.5 for each case. Then, we randomly
(uniformly) sample n platforms, and assume s is
annotated within these platforms. We randomly
sample 5 sets of {T k}K
k=1 with the above process.
Since the three clinic datasets do not come with
training and testing splits. We follow (Ge et al.,
2020) that randomly sample 10% of the data in
each dataset for testing, while the rest is for lo-
cal training. We have 3 random split per sampled
{T k}K
k=1, and run the experiment with each split
and sampled {T k}K
k=1. Following (Ge et al., 2020;
Chen et al., 2022) We report the average F1 score
of all the experiment runs.
7461
ACL 2023 Responsible NLP Checklist
A
For every submission:
□A1. Did you describe the limitations of your work?
Left blank.
□A2. Did you discuss any potential risks of your work?
Left blank.
□A3. Do the abstract and introduction summarize the paper’s main claims?
Left blank.
□A4. Have you used AI writing assistants when working on this paper?
Left blank.
B
□Did you use or create scientiﬁc artifacts?
Left blank.
□B1. Did you cite the creators of artifacts you used?
Left blank.
□B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Left blank.
□B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Left blank.
□B4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Left blank.
□B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Left blank.
□B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C
□Did you run computational experiments?
Left blank.
□C1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing
assistance.
7462
□C2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Left blank.
□C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
□C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D
□Did you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
□D1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Left blank.
□D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Left blank.
□D3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Left blank.
□D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Left blank.
□D5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Left blank.
7463
