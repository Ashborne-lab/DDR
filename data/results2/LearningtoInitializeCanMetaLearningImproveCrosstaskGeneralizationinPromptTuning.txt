Learning to Initialize: Can Meta Learning Improve
Cross-task Generalization in Prompt Tuning?
Chengwei Qin♣, Qian Liq, Ruochen Zhao♣, Shafiq Joty♣♠
♣Nanyang Technological University
♠Salesforce AI
q Northeastern University
{chengwei003@e.ntu, ruochen002@e.ntu, srjoty@ntu}.edu.sg
qianli@stumail.neu.edu.cn
Abstract
Prompt tuning (PT) which only tunes the em-
beddings of an additional sequence of tokens
per task, keeping the pre-trained language
model (PLM) frozen, has shown remarkable
performance in few-shot learning. Despite this,
PT has been shown to rely heavily on good
initialization of the prompt embeddings. In
this work, we study meta prompt tuning (MPT)
to systematically explore how meta-learning
can help improve (if it can) cross-task general-
ization in PT through learning to initialize the
prompt embeddings from other relevant tasks.
We empirically analyze a representative set of
meta learning algorithms in a wide range of
adaptation settings with different source/target
task configurations on a large set of few-shot
tasks. With extensive experiments and analysis,
we demonstrate the effectiveness of MPT. We
find the improvement to be significant particu-
larly on classification tasks. For other kinds of
tasks such as question answering, we observe
that while MPT can outperform PT in most
cases, it does not always outperform multi-task
learning. We further provide an in-depth analy-
sis from the perspective of task similarity.
1
Introduction
Humans can easily learn to perform new tasks with
only few data by leveraging previously acquired
knowledge from other relevant tasks. Such capa-
bility is a hallmark of human intelligence (Carey
and Bartlett, 1978). However, when it comes to
the models, they often face over-fitting issues when
they are tasked to learn from a few labeled exam-
ples (Lake et al., 2017; Linzen, 2020), a problem
commonly termed as few-shot learning (FSL).
With the recent advancements in developing
large-scale pre-trained language models (PLMs),
prompt-based methods have shown promising re-
sults in FSL. Brown et al. (2020) show that by
virtue of in-context (meta) learning, a frozen GPT-
3 model can achieve good results on a variety of
Upstream learning
Sentiment 
Analysis
Question 
Answering
...
Input
Summarization
Downstream learning
Topic 
Classification
Input
Output
The best book ever to 
anything.
Output
When was Kamala 
Harris born?
He gains the access to a 
fortune of $320 million...
Positive
October 20, 1964
...
He gets $320M 
fortune...
Usain Bolt is the greatest 
sprinter of all time.
Sports
Source Tasks
Target Task
Learner
Figure 1: Illustration of cross-task generalization, where
the model is expected to learn an unseen target task
given the knowledge acquired from previously learned
source tasks.
few-shot tasks through manually designed prompts,
which are task instructions along with a few exam-
ples expressed in natural language. However, the
performance of in-context learning has been shown
to be highly sensitive to the design of such “dis-
crete” prompts (Zhao et al., 2021). It is also limited
by the maximum sequence length supported by the
PLMs (Li and Liang, 2021). Down this line, efforts
have been made on automatically searching and
optimizing for discrete prompts (Shin et al., 2020;
Schick and Schütze, 2021; Gao et al., 2021).
As an alternative to discrete prompts, recent ef-
forts attempt to learn “soft” prompts that add ad-
ditional trainable parameters (Liu et al., 2021b; Li
and Liang, 2021; Lester et al., 2021), showing bet-
ter results than discrete prompts (Liu et al., 2021a).
Lester et al. (2021) introduce prompt tuning (PT)
that prepends a sequence of tunable tokens to the
input and optimize their embeddings keeping the
PLM frozen. Despite its strong few-shot perfor-
mance, PT has been shown to be sensitive to the
initialization of the embeddings, which might limit
its practical application (Qin and Joty, 2022b). To
address this, Gu et al. (2022) propose pre-trained
arXiv:2302.08143v3  [cs.CL]  19 Nov 2023
prompt tuning (PPT) to pre-train soft prompts using
self-supervised tasks on unlabeled data. It relies
on carefully designed pre-training tasks tailored to
the downstream tasks, and the pre-training objec-
tives are only applicable to classification tasks. Vu
et al. (2022) introduce soft prompt transfer (SPoT),
which uses the soft prompts learned from a set
of source tasks through multi-task learning to ini-
tialize the prompt for a target task.
Both PPT
and SPoT demonstrate cross-task generalization
(Fig. 1) – learning of a new task can benefit from
learning of other related tasks (Ye et al., 2021).
In a recent survey, Lee et al. (2022) claim that
meta learning (Schmidhuber, 1987) can play an im-
portant role for cross-task generalization in NLP.1
Different from multi-task learning which consid-
ers the performance on the source tasks to learn
the initial parameters, meta learning aims to find
initial parameters suitable for adapting to a target
few-shot task. Hence, it could outperform multi-
task learning in several scenarios with full-model
finetuning (Dou et al., 2019; Chen et al., 2020b).
However, to our knowledge, there is no systematic
study on the role of meta learning on PT. In a recent
work, Huang et al. (2022) adopt MAML (Finn et al.,
2017) for pre-training soft prompts. One major lim-
itation of their study is that it is limited to only one
type of meta learning algorithm and only sentiment
classification tasks, lacking comprehensive under-
standing of cross-task generalization. Min et al.
(2022) and Chen et al. (2022) show the effective-
ness of in-context learning for PLMs, whereas we
mainly focus on optimization-based meta learning.
To systematically study meta prompt tuning
(MPT) for cross-task generalization, we conduct
experiments on a large collection of few-shot tasks
involving different types of datasets with a unified
text-to-text format (Ye et al., 2021). We investigate
a wide range of adaptation settings with different
source/target task types, which helps better under-
stand the capability and limitation of meta learning
in PT. With extensive experiments, we aim to ad-
dress the following research questions:
• Q1. Can MPT improve cross-task generalization
in PT? Is it better than multi-task learning?
• Q2. What happens with more labelled data for
source/target tasks (beyond few-shot settings)?
1Unless otherwise specified, by meta learning in this paper
we generally refer to the optimization-based meta learning
algorithms, and use more specific names for the other kinds
such as in-context learning for black-box meta learning and
metric learning for non-parametric meta learning.
• Q3. Does it help with more diverse source tasks?
• Q4. Is the performance gain of MPT consistent
across different backbone models?
To answer these questions, we empirically an-
alyze MAML (Finn et al., 2017), FoMAML and
Reptile (Nichol et al., 2018), which constitute a
representative set of meta learning methods. Ex-
perimental results show that MPT can indeed help
cross-task generalization, e.g., MAML improves
the performance of PT by more than 20% on clas-
sification tasks. However, we also notice that MPT
does not always outperform multi-task learning, es-
pecially on non-classification tasks. We provide an
in-depth analysis from the perspective of task sim-
ilarity. As for Q2, we find that MPT does benefit
cross-task generalization beyond few-shot settings.
For Q3, we observe that increasing the diversity
of source tasks does not necessarily improve cross-
task generalization. Finally, the consistent gain of
MPT across different models shows its robustness
to model type and size. In summary, the two main
contributions of this work are:
• To the best of our knowledge, we are the first
to extensively explore how meta learning helps
cross-task generalization in prompt tuning.
• With extensive experiments and analysis, we
show the effectiveness and limitation of meta
prompt tuning in various source/target settings.
2
Related Work
Few-shot Learning (FSL)
FSL aims to learn a
task with only a few labeled examples, which often
leads to the over-fitting problem. Existing methods
to address this problem mainly focus on optimizing
the hypothesis space of the few-shot tasks (Tri-
antafillou et al., 2017; Finn et al., 2017; Hu et al.,
2018) or augmenting the few-shot data (Gao et al.,
2020; Qin and Joty, 2022a). Recently, large-scale
pre-trained language models (PLMs) have demon-
strated strong FSL ability through prompt-based
methods, including both discrete (Brown et al.,
2020) and soft prompts (Lester et al., 2021).
Prompt-based Learning (PL)
PL is a new
paradigm which prepends a task-specific template
or prompt to the input for learning new tasks (Liu
et al., 2021a). Initial PL methods mainly focus
on designing, searching or optimizing discrete
prompts (Brown et al., 2020; Shin et al., 2020;
Gao et al., 2021). However, discrete prompts are
hard to optimize. To solve this, recent PL methods
attempt to optimize prompts in a continuous space,
i.e., learn soft prompts (Li and Liang, 2021; Liu
et al., 2021b; Lester et al., 2021), showing impres-
sive FSL performance (Qin and Joty, 2022b). In
addition to prompt design, several recent studies
have explored the applications (Zhu et al., 2022;
Li et al., 2022; Ding et al., 2023; Qin et al., 2023;
Zhao et al., 2023) and analysis (Zhong et al., 2021;
Le Scao and Rush, 2021) of PL.
Meta Learning
Meta Learning or learning to
learn, has been applied to boost few-shot perfor-
mance on various NLP tasks, e.g., relation extrac-
tion (Han et al., 2018) and machine translation
(Gu et al., 2018). Meta learning algorithms can
be divided into three main categories. First, black-
box methods adopt additional meta learners to help
adaptation (Santoro et al., 2016; Garnelo et al.,
2018; Mishra et al., 2018; Brown et al., 2020). Sec-
ond, non-parametric methods explore how to learn
metrics that can compare the distances between
different samples, i.e., learning to compare (Koch
et al., 2015; Vinyals et al., 2016; Snell et al., 2017).
Finally, optimization-based methods aim to learn
better parameter initialization to effectively and
efficiently adapt to unseen tasks, i.e., learning to
initialize (Finn et al., 2017; Nichol et al., 2018;
Kedia et al., 2021). Lee et al. (2022) claim that
meta learning can be effective for cross-task gener-
alization, especially the optimization-based meth-
ods. They can be applied to various problems in
a model-agnostic way to improve FSL on target
tasks with model fine-tuning (Ye et al., 2021).
Summary.
Existing work shows that meta learn-
ing can improve cross-task few-shot generalization
with full model fine-tuning. However, there is no
systematic study on whether (and how) meta learn-
ing can do so with prompt tuning of PLMs. To
fill this research gap, our work provides a com-
prehensive understanding of the effectiveness and
limitation of meta learning in prompt tuning.
3
Preliminaries
In this section, we revisit the basics about prompt
tuning and optimization-based meta learning.
3.1
Prompt Tuning
Following Lester et al. (2021), we reframe all tasks
into a text-to-text format. Given a training dataset
D𝑡𝑟= {(𝑋1,𝑌1), ..., (𝑋𝑛,𝑌𝑛)} for a task T, differ-
ent from traditional model fine-tuning, prompt tun-
ing (PT) is a parameter-efficient learning method
which freezes the PLM 𝜃and prepends the input
text 𝑋𝑖with a sequence of tunable soft tokens 𝑃, pa-
rameterized by prompt embeddings 𝜙. The prompt
embeddings 𝜙are initialized from the vocabulary
of the PLM and optimized through gradient descent
with the following objective:
LT
𝜙= L(𝜙, D𝑡𝑟) = −
𝑛
∑︁
𝑖=1
log 𝑝(𝑌𝑖|[𝑃, 𝑋𝑖], 𝜙, 𝜃)
(1)
3.2
Optimization-based Meta Learning
The main goal of optimization-based meta learning
(or learning to initialize), is to learn better initial
parameters that can effectively and efficiently adapt
to a new task T new with limited data. We denote
the initial parameters (meta-parameters) as 𝜙∗.
To obtain 𝜙∗, the model needs to learn from a
series of meta-training tasks T meta = {T1, ..., T𝑛}.
The dataset D𝑖of each task T𝑖is divided into two
disjoint sets: a support set S𝑖and a query set Q𝑖.
The objective for learning 𝜙∗is
𝜙∗= arg min
𝜙
∑︁
T𝑖∈Tmeta
L

𝜙−𝛼∇𝜙L(𝜙, S𝑖)
|                 {z                 }
inner update
, Q𝑖

(2)
where L is the objective function defined in Eq. (1),
𝜙is the set of parameters to meta-learn and 𝛼is
the inner learning rate. Denoting the overall loss
as LTmeta
𝜙
= Í
T𝑖∈Tmeta L(𝜙′, Q𝑖) with 𝜙′ being the
inner-updated value of 𝜙, we use gradient descent
to update 𝜙further in the meta-training stage:
𝜙= 𝜙−𝛽∇𝜙LTmeta
𝜙
(3)
where 𝛽is the outer learning rate. This is actu-
ally the Model-Agnostic Meta-Learning or MAML
(Finn et al., 2017). Notice that optimizing Eq. (3)
requires calculating second-order gradients, which
can be quite memory-consuming.
To alleviate
this, First-order MAML (FoMAML) and Reptile
(Nichol et al., 2018) are proposed to use first-order
approximations, allowing lower memory costs.
After the meta-training stage, 𝜙∗serves as the ini-
tial parameters for learning an unseen meta-testing
task T new which is usually few-shot.
4
Approach
In this section, we first introduce the problem set-
ting and evaluation metric. Then, we illustrate the
key methods for meta prompt tuning (MPT).
4.1
Problem Setting
To evaluate cross-task generalization in prompt tun-
ing, we select a large and diverse collection of few-
shot tasks from Ye et al. (2021), covering various
types including classification, question answering
and generation. We partition the set of all tasks
T all into two disjoint parts: source tasks T src and
target tasks T tgt. Details of the tasks and partitions
are provided later in our experiment setup (§5).
Following Min et al. (2022), we can divide the
whole learning process into two stages (Fig. 1):
• Upstream learning on source tasks
In this
stage, the model has access to T src, which is re-
garded as meta-training tasks T meta in Eq. (2). We
divide the dataset D𝑖of every source task T𝑖into
training (or support) and validation (or query) sets,
and conduct optimization-based meta learning or
multi-task learning on these sets to obtain meta-
parameters 𝜙∗. Note that we use both support and
query sets for model training in multi-task learning
to ensure fair data access for both methods.
• Downstream learning on target tasks
After
the upstream learning stage, we use the learned
meta-parameters 𝜙∗as the initial point for learning
target tasks T tgt. Every target task T𝑘has its own
training set Dtr
𝑘, validation set Dval
𝑘, and test set
Dtest
𝑘. The model is required to learn from Dtr
𝑘via
prompt tuning and will be evaluated on Dtest
𝑘. The
performance on Dval
𝑘
is used for hyper-parameters
tuning and model selection.
This two-stage learning paradigm can naturally
reflect cross-task generalization where the model
needs to learn an unseen task given previously ac-
quired knowledge from other tasks.
4.2
Evaluation Metric
We evaluate the model performance on a set of
target tasks T tgt. As T tgt may cover various task
types, simply averaging the performance of differ-
ent target tasks is unreasonable. Following Ye et al.
(2021), we use average relative gain (ARG) as the
main evaluation metric. We first calculate relative
gain (RG) for each target task, i.e., relative per-
formance improvement before and after applying
the upstream (meta or multi-task) learning on the
source tasks. Then we average the relative gains
of all target tasks to obtain the final result which
indicates the overall performance improvement.
Meta-training
Source Tasks
...
...
...
...
Soft prompts
Optimization-based 
meta learning
Meta-testing
Target Tasks
...
Prompt 
initialization
...
...
...
Prompt tuning
Meta-parameters
Figure 2: Overview of Meta Prompt Tuning (MPT).
In the meta-training stage, we conduct optimization-
based meta learning on source tasks to obtain meta-
parameters (i.e., soft prompts). The meta-parameters
will then be used to initialize prompt embeddings for
learning unseen target tasks in the meta-testing stage.
4.3
Meta Prompt Tuning (MPT)
As shown in Fig. 2, the key idea of MPT is to ap-
ply optimization-based meta-training as upstream
learning to a set of source tasks in order to learn
meta parameters, which in this case are prompt em-
beddings. The learned prompt embeddings serve
as the initialization for learning unseen target tasks,
referred to as meta-testing or downstream learning.
4.3.1
Meta-training
We meta-train the prompt embeddings on source
tasks T src. Without loss of generality, we take
MAML (Finn et al., 2017) as an example. For
every iteration, we first sample one source task
T𝑖which has a support set S𝑖and a query set Q𝑖.
Then we sample a support batch B𝑠from S𝑖and
a query batch B𝑞from Q𝑖. Denoting the trainable
prompt embeddings as 𝜙, B𝑠and B𝑞are used for
one gradient update with the following objective:
L𝑖
𝜙= L(𝜙−𝛼∇𝜙L(𝜙, B𝑠), B𝑞)
𝜙= 𝜙−𝛽∇𝜙L𝑖
𝜙
(4)
where L is the task loss defined in Eq. (1), and 𝛼
and 𝛽are inner and outer learning rates, respec-
tively. During the meta-training stage, we iterate
over tasks in T src to update prompt embeddings
𝜙for a fixed number of steps. The learned meta-
parameters 𝜙∗is used in the meta-testing stage.
4.3.2
Meta-testing
In meta-testing, the model is expected to learn un-
seen target tasks T tgt. For each target task T𝑘, we
use the learned meta-parameters 𝜙∗to initialize the
prompt embeddings for the task. Denoting the train-
ing set of T𝑘as Dtr
𝑘, the learning objective during
Source
Target
Setting
#tasks
Setting
#tasks
Random
114
Random
20
Classification (Cls)
45
Classification
10
Both (Cls + Non-Cls)
23 + 22
Non-Classification
45
Classification
45
Non-Classification
12
Both (Cls + Non-Cls)
23 + 22
Non-Classification
45
QA
22
QA
15
Non-QA
33
Non-Paraphrase Cls
60
Paraphrase
4
Table 1: Statistics of ten distinct source/target task parti-
tions. Appendix A.1 for details about each partition.
meta testing is defined as:
L𝜙∗(Dtr
𝑘) = −
𝑛
∑︁
𝑖=1
log 𝑝(𝑌𝑖|[𝑃∗, 𝑋𝑖], 𝜙∗, 𝜃)
(5)
where 𝜃is the frozen PLM, (𝑋𝑖,𝑌𝑖) ∼Dtr
𝑘is a
training sample and 𝑃∗are the prompt tokens.
We evaluate the model with the best validation
performance on the test set and calculate average
relative gain on the test sets of T tgt.
5
Experimental Setup
We first describe the source/target task partitions,
and then introduce methods compared in our work.
Finally, we present the implementation details.
5.1
Task Partitions
We experiment with ten different source/target task
partitions as shown in Table 1. Depending on the
type of the target tasks, we can divide these ten
settings into several groups:
• R→R (Random→Random): We first experi-
ment with the R→R setting where both source
and target tasks are randomly selected, meaning
that they can cover any task type. This setting
mimics the learning paradigm of humans and re-
flects whether cross-task generalization can help
obtain a general-purpose few-shot learner.
• X→Cls (X=Cls, Both, Non-Cls): The target
tasks involve classification, while the source
tasks can be classification, non-classification
tasks or both. This setting helps us better under-
stand the influence of the source task distribution.
• X→Non-Cls (X=Cls, Both, Non-Cls): The only
difference between this and the previous setting
is the type of target tasks. We investigate how
meta learning improves cross-task generalization
when target tasks are non-classification tasks.
• X→QA (X=QA, Non-QA): Compared to the
previous one, this group is more fine-grained.
We only select target tasks from question answer-
ing (QA) instead of all non-classification tasks.
We conduct experiment on different source task
types, including QA and Non-QA tasks.
• NP→P (Non-Paraphrase Cls→Paraphrase):
This group has the finest granularity in our set-
ting. We choose paraphrase identification which
is a sub-category of classification as the target,
and non-paraphrase classification as the source.
The final two groups help understand how meta
learning performs in more fine-grained scenarios.
Note that we ensure that there is no overlap be-
tween the source and target tasks. Following Ye
et al. (2021), we use 16 samples per class in the
training (or support) and validation (or query) sets
for classification tasks, and 32 samples per set for
non-classification tasks. For every task, we sample
the training and validation sets 5 times with differ-
ent random seeds to reduce variance in few-shot
evaluation and cover more diverse samples in up-
stream learning. We provide full details of tasks
and partitions in Appendix A.1.
5.2
Methods Compared
We mainly use T5-Large (Raffel et al., 2019) as
the backbone language model and compare the
following methods in our work.
• Prompt Tuning (PT) on target tasks. It is our
baseline without the upstream learning. We di-
rectly apply PT (Lester et al., 2021) to target tasks
and use its performance as the basis for comput-
ing average relative gain for other methods.
• Model-Agnostic Meta-Learning (MAML). We
apply MAML (Finn et al., 2017) in the upstream
learning (meta-training) stage. The learned meta-
parameters are used to initialize prompt embed-
dings for learning target tasks.
• First-order MAML (FoMAML) and Reptile.
We also investigate two first-order meta learn-
ing algorithms: FoMAML (Finn et al., 2017)
and Reptile (Nichol et al., 2018). Compared to
MAML, they are more memory-efficient.
• Multi-task learning (MTL). We conduct multi-
task learning on source tasks instead of meta
learning to obtain initial parameters. This is a
straight-forward yet effective method as demon-
strated by Vu et al. (2022).
• Fine-tuning on target tasks. Fine-tuning is the
dominant paradigm where the whole language
model is tuned for learning target tasks. We in-
clude it to verify whether cross-task generaliza-
tion can help PT outperform fine-tuning.
In addition, we conduct experiments with differ-
ent backbone models to verify MPT’s robustness.
5.3
Implementation Details
All our methods are implemented with Py-
Torch/Transformers library (Wolf et al., 2020). We
use higher library (Grefenstette et al., 2019) for
higher-order optimization in meta learning meth-
ods. The prompt length in PT is set to 100 tokens
following Lester et al. (2021). For meta-training,
we set the inner and outer learning rates to 3e−5
and 5e−1, respectively. We use 5000 for total train-
ing steps. We set the inner batch size to 2, 4 and 4,
and inner update steps to 1, 1 and 10 for MAML,
FoMAML and Reptile, respectively. For multi-task
learning, we set the learning rate, batch size and
number of epochs to 5e−1, 4 and 20, respectively.
For MAML, we select the inner learning rate from
{2e−5, 3e−5, 5e−5}, the outer learning rate from
{2e−1, 3e−1, 5e−1}, and total training steps from
{2500, 5000, 10000}. We adopt the same three hy-
perparameters for FoMAML and Reptile.
The
search range for the inner update steps of Rep-
tile is {2, 4, 6, 8, 10}. For multi-task learning, we
select the learning rate from {2e−1, 3e−1, 5e−1},
the batch size from {2, 4, 6, 8}, and the number of
epochs from {5, 10, 20}.
For downstream learning,
we mainly fol-
low the settings in Ye et al. (2021).
For
prompt tuning, we select the learning rate from
{5e−1, 4e−1, 3e−1, 2e−1} based on the validation
performance. For fine-tuning, the search range for
the learning rate is {5e−4, 3e−4, 2e−4, 1e−4}. We
set the batch size, total training steps and evaluation
interval to 8, 3000 and 50, respectively.
Since it is infeasible to search for optimal hy-
perparameters for each of the meta- and multi-task
learning methods in each of the settings, we select
them based on the R→R setting. We randomly se-
lect 5 tasks that are not in the source and target sets
as validation tasks for hyperparameter search. The
hyperparameters with best validation performance
(ARG) are used for upstream learning. We select
the inner learning rate, the outer learning rate and
total training steps for MAML and adopt the same
three hyperparameters for FoMAML and Reptile.
6
Results and Analysis
We now address the four research questions asked
before in §1 with empirical results.
Q1. Can meta prompt tuning improve cross-task
generalization? Is it better than multi-task learning?
The ARG of different methods w.r.t. PT in var-
ious settings are shown in Table 2; more detailed
results on every target task are in Appendix A.2.
• MPT can indeed help cross-task generaliza-
tion.
From the results in Table 2, we observe that
MPT outperforms the baseline PT in most cases
with +ve ARG scores. Out of 30 different runs for
three meta learning methods in ten different set-
tings (see the 1st block of results), MPT achieves
better performance than PT in 23 runs, demonstrat-
ing its effectiveness in cross-task generalization.
For the R→R setting, MAML achieves the best
performance, showing that it is a good general-
purpose few-shot learner. For adapting to classifi-
cation tasks, MAML outperforms PT by 20.16% if
the prompt embeddings are initialized from other
classification tasks. The results in a more fine-
grained setting (NP→P) also indicate the ability of
MAML to learn classification tasks. While Reptile
performs the best (20.44%) in this setting, MAML
still outperforms PT by a large margin (11.14%).
However, as shown in Table 2, MAML falls be-
hind FoMAML when adapting to non-classification
tasks. Among the three meta learning methods, Fo-
MAML achieves the best performance (9.81%) on
non-classification target tasks in the Both→Non-
Cls setting, showing effective knowledge transfer.
We observe similar results in more fine-grained
settings QA/Non-QA→QA, where FoMAML out-
performs MAML and Reptile significantly. While
Reptile is claimed empirically to be better than
MAML/FoMAML (Lee et al., 2022), it falls short
of MAML/FoMAML in many cases. This might
be because MAML and FoMAML are more similar
compared to Reptile from a gradient perspective
(Nichol et al., 2018). And since the hyperparame-
ter search is done based on MAML (§5.3), which
means Reptile’s method may be suboptimal.
In addition, we can see that meta learning helps
PT outperform fine-tuning in several settings in-
cluding Cls→Cls (MAML, FoMAML), Both→Cls
Method
R→R
Cls
→Cls
Both
→Cls
Non-Cls
→Cls
Cls
→Non-Cls
Both
→Non-Cls
Non-Cls
→Non-Cls
QA
→QA
Non-QA
→QA
NP
→P
MAML
8.78±0.69
20.16±0.84
10.57±1.03
6.34±0.48
0.32±0.04
7.54±0.73
6.71±0.39
−16.59±1.36
3.26±0.24
11.14±0.93
FoMAML
1.24±0.18
18.80±1.13
17.84±1.21
7.32±0.42
6.42±0.51
9.81±0.64
3.88±0.31
16.63±1.58
9.83±0.76
−0.68±0.07
Reptile
8.42±0.46
−5.17±0.71
−4.18±0.37
2.42±0.21
−1.54±0.18
−3.38±0.49
0.78±0.07
0.77±0.09
−0.09±0.01
20.44±1.34
Multi-task learning
7.14±0.62
−5.64±0.92
5.73±0.43
4.97±0.39
8.51±1.16
13.47±0.97
19.67±1.72
25.65±1.93
17.23±1.08
−5.19±0.86
Fine-tuning
−12.61±1.57
16.02±1.44
16.02±1.44
16.02±1.44
−35.70±2.73
−35.70±2.73
−35.70±2.73
−47.37±2.97
−47.37±2.97
1.56±0.12
Table 2: Average relative gain (ARG %) of different methods with respect to prompt tuning (PT) in various
settings. Bold indicates the best ARG score. ‘Cls’, ‘QA’, ‘P’ and ‘NP’ respectively stand for ‘classification’,
‘question answering’, ‘paraphrase’ and ‘non-paraphrase classification’.
(FoMAML) and NP→P (MAML, Reptile), which
demonstrates the superiority of MPT.
• MPT does not always outperform multi-task
learning (MTL).
While meta learning is specifi-
cally designed for quickly adapting to unseen target
tasks, it does not always outperform MTL in PT.
From Table 2, we can observe that MTL achieves
better performance than MPT in many cases, espe-
cially on non-classification target tasks. We analyze
the reasons as follows:
• Meta learning methods have been shown to be
highly sensitive to the hyperparameters (Anto-
niou et al., 2019), which we could not tune
exhaustively due to memory/time constraints
(see Appendix A.5 for hyperparameter sensitivity
analysis). As mentioned in §5.3, we select the
hyperparameters of MAML using the R→R set-
ting, and then use the same hyperparameters for
all meta learning methods in all settings, which
might limit the performance of MPT.
• There might be less shared structure (or features)
among non-classification tasks compared to clas-
sification. The classification tasks mostly involve
sentence-level classification and in some cases
the task labels correlate well (e.g., AG News and
DBpedia). Thus, they share some common se-
mantics in both source and target tasks. The
model can learn similar patterns (inferring the la-
bel of the entire input sentence) during both meta-
training and meta-testing stages, enabling better
knowledge transfer. The non-classification set
on the other hand can include different types of
tasks such as QA and summarization; modeling
them typically requires a Seq2Seq formulation.
These tasks typically lack shared task semantics.
For example, the structure of QA is context +
question + answer, requiring reasoning ability. In
contrast, the structure of summarization is long
document + short summary, requiring summa-
rizing ability. Although it has been shown that
16-shot
32-shot
64-shot
128-shot
all
10
0
10
20
30
Average Relative Gain (%)
MAML
MTL
Figure 3: ARG (%) of MPT (MAML) and multi-task
learning w.r.t. prompt tuning (ARG = 0) for varying
data size of source tasks in the Cls→Cls setting.
QA can help summarization in content selection
(Arumae and Liu, 2019), it is more difficult for
MPT to capture transferable knowledge as suc-
cess of meta learning eventually depends on how
much the tasks share (Finn, 2022).
To provide an in-depth analysis of the differ-
ence between classification and non-classification
tasks, we consider from the perspective of task sim-
ilarity. We follow Lin et al. (2022) which shows
that the correlation between input subspaces (the
norm of projected subspace onto the other sub-
space) for two tasks can serve as the similarity
score between them. We randomly pick 5 (cls,cls)
task pairs as similar tasks. For dissimilar tasks, we
randomly pick 5 (QA, summarization) task pairs.
The average similarity score for similar task pairs
is 0.768 while for dissimilar task pairs the score is
only 0.306 (see Appendix A.6 for detailed results),
which verifies that classification tasks share more
structure than non-classification tasks.
Given the performance gap between MPT and
MTL in some settings, we believe that exploring
more advanced MPT methods could be a promising
research direction.
Q2.
What happens with more labelled data for
source/target tasks (beyond few-shot settings)?
Method
Shot
16
32
64
128
all
MPT (MAML)
20.16
9.10
5.64
8.36
3.27
Multi-task learning
−5.64
−14.17
1.96
−0.20
0.53
Table 3: ARG (%) of different methods when more
labelled data is used in target tasks.
Method
Source task number
12
24
45
MPT (MAML)
8.44
12.89
20.16
Table 4: ARG (%) of MPT (MAML) when using dif-
ferent number of source tasks in the Cls→Cls setting.
As mentioned in §5.1, we mainly explore how MPT
improves cross-task generalization when both the
source and target tasks are few-shot, which cor-
responds to the way humans learn (Lake et al.,
2017). We used 16 samples per class for classi-
fication tasks, and 32 samples per dataset for non-
classification tasks. To validate whether more la-
belled data for source/target tasks can influence the
performance of MPT, we conduct controlled exper-
iments with {32, 64, 128, all} samples per class for
source/target tasks in the Cls→Cls setting.
• Source
We report the results of MAML and
MTL with more labelled data for the source tasks
in Fig. 3. We can observe that: (i) MPT outper-
forms PT (ARG = 0) and MTL in all cases includ-
ing using the full dataset, showing its robustness
to data sizes. (ii) Increasing the number of sam-
ples in source tasks does not necessarily lead to
better cross-task generalization for MPT. The best
ARG is achieved for 16-shot rather than the full
dataset, which justifies using few-shot source tasks.
(iii) The performance of MTL improves with more
data for source tasks, showing a different learning
pattern from MPT.
• Target
Table 3 shows the results for increasing
the number of examples in target tasks. We can see
that: (i) The performance gain of MPT is evident
even using the full dataset (3.27%), demonstrating
that it does help cross-task generalization beyond
few-shot. (ii) MPT outperforms MTL by a large
margin in all settings. (iii) MTL is unstable in terms
of ARG scores; while it outperforms PT in 64-shot
(1.96%) and all samples (0.53%), it falls behind PT
in all other settings, indicating that MPT is a better
choice when adapting to classification tasks.
Method
MAML
FoMAML
Reptile
MTL
Fine-tuning
T5-Large
11.14
−0.68
20.44
−5.19
1.56
T5-Base
9.24
4.15
7.96
1.64
7.41
T5-XLarge
14.35
2.46
10.74
5.72
−9.61
BART-Large
7.63
1.16
8.94
−2.37
2.74
GPT2-Large
3.19
−2.68
4.62
−1.43
3.75
Table 5: Average relative gain (ARG %) of all methods
with different backbone models in the NP→P setting.
‘MTL’ stands for ‘multi-task learning’.
Q3. Does MPT help with more diverse source tasks?
MPT aims to learn to initialize the prompt embed-
dings from source tasks, which may cover different
types. We hypothesize that the diversity of source
tasks might influence its performance. To verify
this, we analyze the influence of different source
task selections on the same target tasks in two set-
tings: varying the type and number of tasks.
• Type of tasks.
The results of learning from dif-
ferent types of source tasks are reported in Table 2.
The performance of MPT on non-classification
target tasks improves when using more diverse
source tasks, e.g., from Non-Cls/Cls→Non-Cls to
Both→Non-Cls. However, for adapting to classi-
fication task, the best ARG is achieved when all
source tasks are classification, i.e., the Cls→Cls
setting. Hence, we can conclude that increasing
the type diversity of source tasks does not neces-
sarily improve cross-task generalization, which is
consistent with the finding in Ye et al. (2021).
• Number of tasks.
To investigate the impact of
the number of source tasks, we conduct controlled
experiments on {12, 24} source tasks sampled from
the original 45 source tasks in the Cls→Cls setting
(see Appendix A.4 for a full list). From Table 4,
we can observe that the performance of MPT keeps
improving as the number of source tasks increases,
showing better cross-task generalization.
It is worthwhile to note that while our work pro-
vides some insights on the choice of source tasks,
more systematic studies on how to select the most
suitable source tasks given a set of target tasks are
needed. We hope that future analysis can provide a
more comprehensive understanding of the relation-
ship between source and target tasks.
Q4. Is the performance gain of MPT consistent across
different backbone language models?
Target Task
Partition
ΔMPT
ΔMTL
Amazon_Polarity
R→R
3.10
2.25
Cls→Cls
7.40
10.45
AI2_ARC
R→R
12.54
5.55
Both→Non-Cls
8.17
6.69
Samsum
R→R
1.97
6.77
Both→Non-Cls
2.50
5.71
Superglue-Copa
Both→Non-Cls
1.20
10.00
QA→QA
−3.20
4.80
Table 6: Relative gain in % for MPT and MTL when
the same target task appears in different patitions.
Our experiments and analysis so far use T5-Large
as the backbone model. To verify whether the per-
formance gain of MPT is consistent across different
backbone models, we extend the experiments to T5-
Base, T5-XLarge, BART-Large and GPT2-Large
in the NP→P setting. From the results shown in Ta-
ble 5, we can see that MPT still outperforms PT and
MTL by a large margin when using other PLMs
as the backbone model, showing its robustness to
model size and type. In addition, the consistent
gain of MPT with T5-XLarge could also verify the
effectiveness of MPT for huge PLMs which have
been shown to perform better in prompt tuning
(Lester et al., 2021).
6.1
Further Analysis
Prompt tuning (PT) vs.
Fine-tuning (FT).
While PT shows strong few-shot learning ability,
FT remains the dominant paradigm. As shown in
Table 2, FT outperforms PT when adapting to clas-
sification tasks even in few-shot settings, which
might be because PT has only a few tunable param-
eters. Though MPT is based on PT, its performance
gain over FT in all cases suggests that it can learn
to initialize the prompt embeddings from source
tasks, enabling effective knowledge transfer.
Case Study
To take a closer look at the influence
of different source task types on a particular target
task, we further conduct a case study where we
ensure that the task under consideration appears
in the target task partitions.2 Results are shown in
Table 6; for example, the first block indicates that
Amazon_Polarity appears as a target task in both
R→R and Cls→Cls settings. We can observe that
there is no consistent conclusion on how we should
choose the source tasks for a specific target task,
2As before, we ensure it does not appear in the source.
which is consistent with our view in Q3.
7
Conclusion
In this paper, we have introduced meta prompt tun-
ing (MPT), which learns to initialize the prompt
embeddings for adapting to a target task. We have
identified key research questions and systematically
studied where and how meta learning can improve
cross-task generalization in prompt tuning. We
have empirically analyzed a representative set of
meta learning methods in a variety of adaptation
settings on a large, diverse collection of few-shot
tasks. Extensive experimental results and analysis
verify the effectiveness of MPT. Given the find-
ings, in the future, we would like to explore more
advanced meta learning algorithms which can con-
sistently outperform multi-task learning.
Limitations
Although comprehensive, our study of MPT in this
work has couple of limitations:
• As mentioned in §5.3, because of infeasiblity
to search for optimal hyperparameters for each
of the meta learning methods in each of the ten
settings, we choose to use the R→R setting as our
main representative setting. This could be one of
the reasons for MPT underperforming MTL in
some non-classification tasks (noted in §6-Q1).
• We mainly focus on how upstream meta learn-
ing can improve the performance on target tasks.
However, meta learning also enables faster con-
vergence. We leave how it could help reduce the
convergence time of PT as future work.
Aside from that, meta prompt tuning (MPT) as a
method has a limitation that it is Memory-intensive.
Optimization-based meta learning methods, espe-
cially MAML, are memory-intensive, which limits
the tuning of the inner batch size and inner update
steps (§5.3). One potential solution is to build more
memory-efficient meta learning libraries.
References
Tiago A. Almeida, José María G. Hidalgo, and Akebo
Yamakami. 2011. Contributions to the study of sms
spam filtering: New collection and results. In Pro-
ceedings of the 11th ACM Symposium on Document
Engineering, DocEng ’11, page 259–262, New York,
NY, USA. Association for Computing Machinery.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019. MathQA: Towards interpretable math
word problem solving with operation-based for-
malisms. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
2357–2367, Minneapolis, Minnesota. Association for
Computational Linguistics.
Antreas Antoniou, Harrison Edwards, and Amos
Storkey. 2019. How to train your MAML. In Inter-
national Conference on Learning Representations.
Kristjan Arumae and Fei Liu. 2019. Guiding extrac-
tive summarization with question-answering rewards.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 2566–2577,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Francesco Barbieri, Jose Camacho-Collados, Luis Es-
pinosa Anke, and Leonardo Neves. 2020. TweetEval:
Unified benchmark and comparative evaluation for
tweet classification. In Findings of the Association
for Computational Linguistics: EMNLP 2020, pages
1644–1650, Online. Association for Computational
Linguistics.
Max Bartolo, Alastair Roberts, Johannes Welbl, Sebas-
tian Riedel, and Pontus Stenetorp. 2020. Beat the AI:
Investigating adversarial human annotation for read-
ing comprehension. Transactions of the Association
for Computational Linguistics, 8:662–678.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA. Association for Computational Linguis-
tics.
Chandra Bhagavatula, Ronan Le Bras, Chaitanya
Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-
nah Rashkin, Doug Downey, Wen tau Yih, and Yejin
Choi. 2020. Abductive commonsense reasoning. In
International Conference on Learning Representa-
tions.
Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng
Gao, and Yejin Choi. 2020. PIQA: reasoning about
physical commonsense in natural language. In The
Thirty-Fourth AAAI Conference on Artificial Intelli-
gence, AAAI 2020, The Thirty-Second Innovative Ap-
plications of Artificial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational
Advances in Artificial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020, pages 7432–
7439. AAAI Press.
Michael Boratko, Xiang Li, Tim O’Gorman, Rajarshi
Das, Dan Le, and Andrew McCallum. 2020. Pro-
toQA: A question answering dataset for prototypi-
cal common-sense reasoning. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1122–1136,
Online. Association for Computational Linguistics.
Jan A. Botha, Manaal Faruqui, John Alex, Jason
Baldridge, and Dipanjan Das. 2018. Learning to split
and rephrase from Wikipedia edit history. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 732–737,
Brussels, Belgium. Association for Computational
Linguistics.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.
Susan Carey and E. Bartlett. 1978. Acquiring a sin-
gle new word. Proceedings of the Stanford Child
Language Conference, 15:17–29.
Ankush Chatterjee, Kedhar Nath Narahari, Meghana
Joshi, and Puneet Agrawal. 2019. SemEval-2019 task
3: EmoContext contextual emotion detection in text.
In Proceedings of the 13th International Workshop
on Semantic Evaluation, pages 39–48, Minneapo-
lis, Minnesota, USA. Association for Computational
Linguistics.
Michael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-
nandez, and Doug Downey. 2019.
CODAH: An
adversarially-authored question answering dataset
for common sense. In Proceedings of the 3rd Work-
shop on Evaluating Vector Space Representations for
NLP, pages 63–69, Minneapolis, USA. Association
for Computational Linguistics.
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020a. Tabfact: A large-scale
dataset for table-based fact verification. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.
Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke
Zettlemoyer, and Sonal Gupta. 2020b. Low-resource
domain adaptation for compositional task-oriented
semantic parsing. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 5090–5100, Online. As-
sociation for Computational Linguistics.
Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,
and He He. 2022. Meta-learning via language model
in-context tuning. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 719–730,
Dublin, Ireland. Association for Computational Lin-
guistics.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. BoolQ: Exploring the surprising
difficulty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers), pages 2924–2936, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. ArXiv
preprint, abs/1803.05457.
Arman Cohan, Waleed Ammar, Madeleine van Zuylen,
and Field Cady. 2019. Structural scaffolds for ci-
tation intent classification in scientific publications.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 3586–3596,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges Workshop,
pages 177–190. Springer.
Pradeep Dasigi, Nelson F. Liu, Ana Marasovi´c, Noah A.
Smith, and Matt Gardner. 2019. Quoref: A read-
ing comprehension dataset with questions requir-
ing coreferential reasoning. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 5925–5932, Hong Kong,
China. Association for Computational Linguistics.
Thomas Davidson, Dana Warmsley, Michael Macy, and
Ingmar Weber. 2017. Automated hate speech de-
tection and the problem of offensive language. In
Proceedings of the 11th International AAAI Confer-
ence on Web and Social Media, ICWSM ’17, pages
512–515.
Ona de Gibert, Naiara Perez, Aitor García-Pablos, and
Montse Cuadros. 2018. Hate speech dataset from
a white supremacy forum. In Proceedings of the
2nd Workshop on Abusive Language Online (ALW2),
pages 11–20, Brussels, Belgium. Association for
Computational Linguistics.
Marie-Catherine de Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
Proceedings of Sinn und Bedeutung, 23(2):107–124.
T. Diggelmann, Jordan L. Boyd-Graber, Jannis Bu-
lian, Massimiliano Ciaramita, and Markus Leip-
pold. 2020. Climate-fever: A dataset for verifica-
tion of real-world climate claims. ArXiv preprint,
abs/2012.00614.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2019. Wizard
of wikipedia: Knowledge-powered conversational
agents. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019. OpenReview.net.
Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken
Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.
Is GPT-3 a good data annotator?
In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 11173–11195, Toronto, Canada. Association
for Computational Linguistics.
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).
Zi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos.
2019.
Investigating meta-learning algorithms for
low-resource natural language understanding tasks.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 1192–
1197, Hong Kong, China. Association for Computa-
tional Linguistics.
Matthew Dunn, Levent Sagun, Mike Higgins, V. U.
Güney, Volkan Cirik, and Kyunghyun Cho. 2017.
Searchqa:
A new q&a dataset augmented with
context from a search engine.
ArXiv preprint,
abs/1704.05179.
Ondˇrej Dušek, David M. Howcroft, and Verena Rieser.
2019. Semantic noise matters for neural natural lan-
guage generation. In Proceedings of the 12th Interna-
tional Conference on Natural Language Generation,
pages 421–426, Tokyo, Japan. Association for Com-
putational Linguistics.
Ondˇrej Dušek, Jekaterina Novikova, and Verena Rieser.
2020. Evaluating the State-of-the-Art of End-to-End
Natural Language Generation: The E2E NLG Chal-
lenge. Computer Speech & Language, 59:123–156.
Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci,
Christophe Gravier, Jonathon Hare, Frederique Lafor-
est, and Elena Simperl. 2018. T-REx: A large scale
alignment of natural language with knowledge base
triples. In Proceedings of the Eleventh International
Conference on Language Resources and Evaluation
(LREC 2018), Miyazaki, Japan. European Language
Resources Association (ELRA).
Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and
Dragomir Radev. 2019. Multi-news: A large-scale
multi-document summarization dataset and abstrac-
tive hierarchical model. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics, pages 1074–1084, Florence, Italy. Asso-
ciation for Computational Linguistics.
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
Long form question answering. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 3558–3567, Florence,
Italy. Association for Computational Linguistics.
Manaal Faruqui and Dipanjan Das. 2018. Identifying
well-formed natural language questions. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 798–803,
Brussels, Belgium. Association for Computational
Linguistics.
Chelsea Finn. 2022. Deep multi-task and meta learning.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. In International conference on ma-
chine learning, pages 1126–1135. PMLR.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 3816–3830, Online. Association for Computa-
tional Linguistics.
Tianyu Gao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen
Lin, Leyu Lin, and Maosong Sun. 2020. Neural
snowball for few-shot relation learning.
Marta Garnelo, Dan Rosenbaum, Christopher Maddison,
Tiago Ramalho, David Saxton, Murray Shanahan,
Yee Whye Teh, Danilo Rezende, and SM Ali Eslami.
2018. Conditional neural processes. In International
Conference on Machine Learning, pages 1704–1713.
PMLR.
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-
sander Wawer. 2019. SAMSum corpus: A human-
annotated dialogue dataset for abstractive summa-
rization. In Proceedings of the 2nd Workshop on
New Frontiers in Summarization, pages 70–79, Hong
Kong, China. Association for Computational Linguis-
tics.
Andrew Gordon, Zornitsa Kozareva, and Melissa Roem-
mele. 2012. SemEval-2012 task 7: Choice of plau-
sible alternatives: An evaluation of commonsense
causal reasoning. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics – Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), pages 394–398, Montréal,
Canada. Association for Computational Linguistics.
Edward Grefenstette, Brandon Amos, Denis Yarats,
Phu Mon Htut, Artem Molchanov, Franziska Meier,
Douwe Kiela, Kyunghyun Cho, and Soumith Chin-
tala. 2019. Generalized inner loop meta-learning.
arXiv preprint arXiv:1910.01727.
Jiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li,
and Kyunghyun Cho. 2018. Meta-learning for low-
resource neural machine translation.
In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 3622–3631,
Brussels, Belgium. Association for Computational
Linguistics.
Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.
2022. PPT: Pre-trained prompt tuning for few-shot
learning. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 8410–8423, Dublin,
Ireland. Association for Computational Linguistics.
Harsha Gurulingappa, Abdul Mateen Rajput, Angus
Roberts, Juliane Fluck, Martin Hofmann-Apitius, and
Luca Toldo. 2012. Development of a benchmark
corpus to support the automatic extraction of drug-
related adverse effects from medical case reports.
Journal of Biomedical Informatics, 45(5):885–892.
Text Mining and Natural Language Processing in
Pharmacogenomics.
Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,
Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A
large-scale supervised few-shot relation classification
dataset with state-of-the-art evaluation. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 4803–4809,
Brussels, Belgium. Association for Computational
Linguistics.
Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.
Question-answer driven semantic role labeling: Us-
ing natural language to annotate natural language.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
643–653, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,
Hagen Fürstenau, Manfred Pinkal, Marc Spaniol,
Bilyana Taneva, Stefan Thater, and Gerhard Weikum.
2011. Robust disambiguation of named entities in
text. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 782–792, Edinburgh, Scotland, UK. Associa-
tion for Computational Linguistics.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-
Yew Lin, and Deepak Ravichandran. 2001. Toward
semantics-based answer pinpointing. In Proceedings
of the First International Conference on Human Lan-
guage Technology Research.
Zikun Hu, Xiang Li, Cunchao Tu, Zhiyuan Liu, and
Maosong Sun. 2018. Few-shot charge prediction
with discriminative legal attributes. In Proceedings of
the 27th International Conference on Computational
Linguistics, pages 487–498, Santa Fe, New Mexico,
USA. Association for Computational Linguistics.
Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and
Yejin Choi. 2019. Cosmos QA: Machine reading
comprehension with contextual commonsense rea-
soning. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
2391–2401, Hong Kong, China. Association for Com-
putational Linguistics.
Yukun Huang, Kun Qian, and Zhou Yu. 2022. Learn-
ing a better initialization for soft prompts via meta-
learning. arXiv preprint arXiv:2205.12471.
Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-
baseQA: A new factoid QA data set matching trivia-
style question-answer pairs with Freebase. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers), pages 318–323, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Akhil Kedia, Sai Chetan Chinthakindi, and Wonho Ryu.
2021.
Beyond reptile: Meta-learned dot-product
maximization between gradients for improved single-
task regularization. In Findings of the Association
for Computational Linguistics: EMNLP 2021, pages
407–420, Punta Cana, Dominican Republic. Associa-
tion for Computational Linguistics.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018. Looking
beyond the surface: A challenge set for reading com-
prehension over multiple sentences. In Proceedings
of the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 252–262, New Orleans, Louisiana. As-
sociation for Computational Linguistics.
Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2020.
Qasc: A
dataset for question answering via sentence com-
position. Proceedings of the AAAI Conference on
Artificial Intelligence, 34(05):8082–8090.
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
Scitail: A textual entailment dataset from science
question answering. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
(AAAI-18), the 30th innovative Applications of Arti-
ficial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artificial Intel-
ligence (EAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018, pages 5189–5197. AAAI Press.
Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim.
2019. Abstractive summarization of Reddit posts
with multi-level memory networks. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers), pages 2519–2531, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Gregory Koch, Richard Zemel, Ruslan Salakhutdinov,
et al. 2015. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop,
volume 2, page 0. Lille.
Neema Kotonya and Francesca Toni. 2020. Explainable
automated fact-checking for public health claims. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 7740–7754, Online. Association for Computa-
tional Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: A benchmark for question answering
research. Transactions of the Association for Compu-
tational Linguistics, 7:452–466.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. RACE: Large-scale ReAd-
ing comprehension dataset from examinations. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages 785–
794, Copenhagen, Denmark. Association for Compu-
tational Linguistics.
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenen-
baum, and Samuel J. Gershman. 2017. Building ma-
chines that learn and think like people. Behavioral
and Brain Sciences, 40:e253.
Teven Le Scao and Alexander Rush. 2021. How many
data points is a prompt worth? In Proceedings of
the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 2627–2636,
Online. Association for Computational Linguistics.
Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1203–1213, Austin,
Texas. Association for Computational Linguistics.
Hung-yi Lee, Shang-Wen Li, and Ngoc Thang Vu. 2022.
Meta learning for natural language processing: A
survey. arXiv preprint arXiv:2205.01500.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
D. Kontokostas, Pablo N. Mendes, Sebastian Hell-
mann, M. Morsey, Patrick van Kleef, S. Auer, and
C. Bizer. 2015. Dbpedia - a large-scale, multilingual
knowledge base extracted from wikipedia. Semantic
Web, 6:167–195.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Hector J. Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Proceedings of the Thirteenth International Confer-
ence on Principles of Knowledge Representation and
Reasoning, KR’12, page 552–561. AAAI Press.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In Proceedings of the 21st
Conference on Computational Natural Language
Learning (CoNLL 2017), pages 333–342, Vancouver,
Canada. Association for Computational Linguistics.
Junyi Li, Tianyi Tang, Jian-Yun Nie, Ji-Rong Wen, and
Xin Zhao. 2022. Learning to transfer prompts for text
generation. In Proceedings of the 2022 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 3506–3518, Seattle, United States.
Association for Computational Linguistics.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In COLING 2002: The 19th International
Conference on Computational Linguistics.
Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang
Ren. 2020a. Birds have four legs?! NumerSense:
Probing Numerical Commonsense Knowledge of Pre-
Trained Language Models. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 6862–6868,
Online. Association for Computational Linguistics.
Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei
Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang
Ren. 2020b. CommonGen: A constrained text gen-
eration challenge for generative commonsense rea-
soning. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020, pages 1823–1840,
Online. Association for Computational Linguistics.
Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-
ner. 2019. Reasoning over paragraph effects in situ-
ations. In Proceedings of the 2nd Workshop on Ma-
chine Reading for Question Answering, pages 58–62,
Hong Kong, China. Association for Computational
Linguistics.
Sen Lin, Li Yang, Deliang Fan, and Junshan Zhang.
2022. TRGP: Trust region gradient projection for
continual learning. In International Conference on
Learning Representations.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 158–167, Vancouver,
Canada. Association for Computational Linguistics.
Tal Linzen. 2020.
How can we accelerate progress
towards human-like linguistic generalization?
In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5210–
5217, Online. Association for Computational Lin-
guistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021a. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
arXiv preprint arXiv:2107.13586.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt
understands, too. arXiv preprint arXiv:2103.10385.
Annie Louis, Dan Roth, and Filip Radlinski. 2020. “I’d
rather just go to bed”: Understanding indirect an-
swers. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 7411–7425, Online. Association for
Computational Linguistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 142–150, Portland,
Oregon, USA. Association for Computational Lin-
guistics.
Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-
lenius, and Pyry Takala. 2014. Good debt or bad
debt: Detecting semantic orientations in economic
texts. J. Assoc. Inf. Sci. Technol., 65(4):782–796.
Irene Manotas, Ngoc Phuoc An Vo, and Vadim Sheinin.
2020. LiMiT: The literal motion in text dataset. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020, pages 991–1000, Online.
Association for Computational Linguistics.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC’14),
pages 216–223, Reykjavik, Iceland. European Lan-
guage Resources Association (ELRA).
Binny Mathew, Punyajoy Saha, Seid Muhie Yimam,
Chris Biemann, Pawan Goyal, and Animesh Mukher-
jee. 2020.
Hatexplain: A benchmark dataset for
explainable hate speech detection. ArXiv preprint,
abs/2012.10289.
Julian J. McAuley and Jure Leskovec. 2013. Hidden
factors and hidden topics: understanding rating di-
mensions with review text. In Seventh ACM Confer-
ence on Recommender Systems, RecSys ’13, Hong
Kong, China, October 12-16, 2013, pages 165–172.
ACM.
Clara H. McCreery, Namit Katariya, Anitha Kannan,
Manish Chablani, and Xavier Amatriain. 2020. Effec-
tive transfer learning for identifying similar questions:
Matching user questions to COVID-19 faqs. In KDD
’20: The 26th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining, Virtual Event, CA,
USA, August 23-27, 2020, pages 3458–3465. ACM.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question an-
swering. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 2381–2391, Brussels, Belgium. Association
for Computational Linguistics.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2022. MetaICL: Learning to learn
in context. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 2791–2809, Seattle, United States.
Association for Computational Linguistics.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and
Pieter Abbeel. 2018. A simple neural attentive meta-
learner. In International Conference on Learning
Representations.
Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,
and Grigorios Tsoumakas. 2020.
Ethos: an on-
line hate speech detection dataset. ArXiv preprint,
abs/2006.08328.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1953–1967, Online. As-
sociation for Computational Linguistics.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated Gigaword. In Proceed-
ings of the Joint Workshop on Automatic Knowledge
Base Construction and Web-scale Knowledge Ex-
traction (AKBC-WEKEX), pages 95–100, Montréal,
Canada. Association for Computational Linguistics.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1797–1807, Brussels, Bel-
gium. Association for Computational Linguistics.
Alex Nichol, Joshua Achiam, and John Schulman.
2018. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,
Jason Weston, and Douwe Kiela. 2020. Adversarial
NLI: A new benchmark for natural language under-
standing. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 4885–4901, Online. Association for Computa-
tional Linguistics.
A. Othman and M. Jemni. 2012. English-asl gloss paral-
lel corpus 2012: Aslg-pc12. In 5th Workshop on the
Representation and Processing of Sign Languages:
Interactions between Corpus and Lexicon LREC.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics (ACL’05), pages 115–124, Ann
Arbor, Michigan. Association for Computational Lin-
guistics.
Dimitris Pappas, Petros Stavropoulos, Ion Androut-
sopoulos, and Ryan McDonald. 2020. BioMRC: A
dataset for biomedical machine reading comprehen-
sion. In Proceedings of the 19th SIGBioMed Work-
shop on Biomedical Language Processing, pages 140–
149, Online. Association for Computational Linguis-
tics.
Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim
Rocktäschel, Yuxiang Wu, Alexander H. Miller, and
Sebastian Riedel. 2020. How context affects lan-
guage models’ factual predictions. In Automated
Knowledge Base Construction.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases?
In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP),
pages 2463–2473, Hong Kong, China. Association
for Computational Linguistics.
Mohammad Taher Pilehvar and Jose Camacho-Collados.
2019. WiC: the word-in-context dataset for evalu-
ating context-sensitive meaning representations. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 1267–1273,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Amir Pouran Ben Veyseh,
Franck Dernoncourt,
Quan Hung Tran, and Thien Huu Nguyen. 2020.
What does this acronym mean? introducing a new
dataset for acronym identification and disambigua-
tion. In Proceedings of the 28th International Con-
ference on Computational Linguistics, pages 3285–
3301, Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.
Chengwei Qin and Shafiq Joty. 2022a. Continual few-
shot relation learning via embedding space regular-
ization and data augmentation. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
2776–2789, Dublin, Ireland. Association for Compu-
tational Linguistics.
Chengwei Qin and Shafiq Joty. 2022b. LFPT5: A uni-
fied framework for lifelong few-shot language learn-
ing based on prompt tuning of t5. In International
Conference on Learning Representations.
Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao
Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is
chatgpt a general-purpose natural language process-
ing task solver? arXiv preprint arXiv:2302.06476.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. arXiv preprint arXiv:1910.10683.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019.
Explain your-
self! leveraging language models for commonsense
reasoning. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 4932–4942, Florence, Italy. Association for
Computational Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Anna Rogers, Olga Kovaleva, Matthew Downey, and
Anna Rumshisky. 2020. Getting closer to ai complete
question answering: A set of prerequisite real tasks.
Proceedings of the AAAI Conference on Artificial
Intelligence, 34(05):8722–8731.
Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and
Karthik Sankaranarayanan. 2018. DuoRC: Towards
complex language understanding with paraphrased
reading comprehension. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1683–
1693, Melbourne, Australia. Association for Compu-
tational Linguistics.
Gobinda Saha, Isha Garg, and Kaushik Roy. 2021. Gra-
dient projection memory for continual learning. In
International Conference on Learning Representa-
tions.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2020. Winogrande: An adversar-
ial winograd schema challenge at scale. Proceedings
of the AAAI Conference on Artificial Intelligence,
34(05):8732–8740.
Adam Santoro, Sergey Bartunov, Matthew Botvinick,
Daan Wierstra, and Timothy Lillicrap. 2016. Meta-
learning with memory-augmented neural networks.
In International conference on machine learning,
pages 1842–1850. PMLR.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019. Social IQa: Com-
monsense reasoning about social interactions. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 4463–
4473, Hong Kong, China. Association for Computa-
tional Linguistics.
Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,
Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-
textualized affect representations for emotion recog-
nition. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 3687–3697, Brussels, Belgium. Association
for Computational Linguistics.
Timo Schick and Hinrich Schütze. 2021. Exploiting
cloze-questions for few-shot text classification and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume,
pages 255–269, Online. Association for Computa-
tional Linguistics.
Jurgen Schmidhuber. 1987. Evolutionary principles in
self-referential learning. on learning now to learn:
The meta-meta-meta...-hook. Diploma thesis, Tech-
nische Universitat Munchen, Germany, 14 May.
Emily Sheng and David Uthus. 2020. Investigating
societal biases in a poetry composition system. In
Proceedings of the Second Workshop on Gender
Bias in Natural Language Processing, pages 93–106,
Barcelona, Spain (Online). Association for Computa-
tional Linguistics.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric
Wallace, and Sameer Singh. 2020. AutoPrompt: Elic-
iting Knowledge from Language Models with Auto-
matically Generated Prompts. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 4222–4235,
Online. Association for Computational Linguistics.
Damien Sileo, Tim Van De Cruys, Camille Pradel, and
Philippe Muller. 2019. Mining discourse markers
for unsupervised sentence representation learning. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 3477–3486,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Jake Snell, Kevin Swersky, and Richard Zemel. 2017.
Prototypical networks for few-shot learning.
Ad-
vances in neural information processing systems, 30.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.
Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,
and Claire Cardie. 2019. DREAM: A challenge data
set and models for dialogue-based reading compre-
hension. Transactions of the Association for Compu-
tational Linguistics, 7:217–231.
Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau
Yih, and Ashish Sabharwal. 2019a. Quarel: A dataset
and models for answering questions about qualitative
relationships. Proceedings of the AAAI Conference
on Artificial Intelligence, 33(01):7063–7071.
Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter
Clark. 2019b. QuaRTz: An open-domain dataset of
qualitative relationship questions. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 5941–5946, Hong Kong,
China. Association for Computational Linguistics.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4149–4158, Minneapolis, Minnesota. Association for
Computational Linguistics.
Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-
ter Clark, and Antoine Bosselut. 2019. WIQA: A
dataset for “what if...” reasoning over procedural text.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 6076–
6085, Hong Kong, China. Association for Computa-
tional Linguistics.
James
Thorne,
Andreas
Vlachos,
Christos
Christodoulopoulos,
and
Arpit
Mittal.
2018.
FEVER: a large-scale dataset for fact extraction
and VERification.
In Proceedings of the 2018
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers), pages 809–819, New Orleans, Louisiana.
Association for Computational Linguistics.
Eleni Triantafillou, Richard S. Zemel, and Raquel Urta-
sun. 2017. Few-shot learning through an information
retrieval lens.
Sowmya Vajjala and Ivana Luˇci´c. 2018.
On-
eStopEnglish corpus: A new corpus for automatic
readability assessment and text simplification. In Pro-
ceedings of the Thirteenth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 297–304, New Orleans, Louisiana. Association
for Computational Linguistics.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, ko-
ray kavukcuoglu, and Daan Wierstra. 2016. Match-
ing networks for one shot learning. In Advances in
Neural Information Processing Systems, volume 29.
Curran Associates, Inc.
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou’,
and Daniel Cer. 2022. SPoT: Better frozen model
adaptation through soft prompt transfer. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 5039–5059, Dublin, Ireland. Association
for Computational Linguistics.
William Yang Wang. 2017. “liar, liar pants on fire”:
A new benchmark dataset for fake news detection.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 2:
Short Papers), pages 422–426, Vancouver, Canada.
Association for Computational Linguistics.
Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-
hananey, Wei Peng, Sheng-Fu Wang, and Samuel R.
Bowman. 2020. BLiMP: The benchmark of linguis-
tic minimal pairs for English. Transactions of the
Association for Computational Linguistics, 8:377–
392.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics, 7:625–641.
Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.
Crowdsourcing multiple choice science questions.
In Proceedings of the 3rd Workshop on Noisy User-
generated Text, pages 94–106, Copenhagen, Den-
mark. Association for Computational Linguistics.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 38–45, Online. Association
for Computational Linguistics.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
ner, Yoav Goldberg, Daniel Deutch, and Jonathan
Berant. 2020. Break it down: A question understand-
ing benchmark. Transactions of the Association for
Computational Linguistics, 8:183–198.
Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulka-
rni, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and
William Yang Wang. 2019. TWEETQA: A social
media focused question answering dataset. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5020–
5031, Florence, Italy. Association for Computational
Linguistics.
Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
WikiQA: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2013–2018, Lisbon, Portugal. As-
sociation for Computational Linguistics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D. Manning. 2018. HotpotQA: A dataset for
diverse, explainable multi-hop question answering.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2369–2380, Brussels, Belgium. Association for Com-
putational Linguistics.
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.
CrossFit: A few-shot learning challenge for cross-
task generalization in NLP. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing, pages 7163–7189, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-
ing Yao, Shanelle Roman, Zilin Zhang, and Dragomir
Radev. 2018. Spider: A large-scale human-labeled
dataset for complex and cross-domain semantic pars-
ing and text-to-SQL task. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 3911–3921, Brussels, Bel-
gium. Association for Computational Linguistics.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin
Choi. 2018. SWAG: A large-scale adversarial dataset
for grounded commonsense inference. In Proceed-
ings of the 2018 Conference on Empirical Methods in
Natural Language Processing, pages 93–104, Brus-
sels, Belgium. Association for Computational Lin-
guistics.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-
chine really finish your sentence? In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 4791–4800, Florence,
Italy. Association for Computational Linguistics.
Hao Zhang, Jae Ro, and Richard Sproat. 2020. Semi-
supervised URL segmentation with recurrent neu-
ral networks pre-trained on knowledge graph enti-
ties. In Proceedings of the 28th International Con-
ference on Computational Linguistics, pages 4667–
4675, Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.
Rui Zhang and Joel Tetreault. 2019. This email could
save your life: Introducing the task of email subject
line generation. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics, pages 446–456, Florence, Italy. Association
for Computational Linguistics.
Sheng Zhang, X. Liu, J. Liu, Jianfeng Gao, Kevin
Duh, and Benjamin Van Durme. 2018.
Record:
Bridging the gap between human and machine com-
monsense reading comprehension. ArXiv preprint,
abs/1810.12885.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems 28: Annual Conference on Neural In-
formation Processing Systems 2015, December 7-12,
2015, Montreal, Quebec, Canada, pages 649–657.
Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
PAWS: Paraphrase adversaries from word scrambling.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 1298–1308,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei
Qin, and Lidong Bing. 2023. Verify-and-edit: A
knowledge-enhanced chain-of-thought framework.
arXiv preprint arXiv:2305.03268.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In
Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings
of Machine Learning Research, pages 12697–12706.
PMLR.
Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.
2021. Adapting language models for zero-shot learn-
ing by meta-tuning on dataset and prompt collections.
In Findings of the Association for Computational
Linguistics: EMNLP 2021, pages 2856–2878, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Victor Zhong, Caiming Xiong, and Richard Socher.
2017.
Seq2sql:
Generating structured queries
from natural language usin.
ArXiv preprint,
abs/1709.00103.
Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth.
2019. “going on a vacation” takes longer than “go-
ing for a walk”: A study of temporal commonsense
understanding. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP),
pages 3363–3369, Hong Kong, China. Association
for Computational Linguistics.
Wangchunshu Zhou, Canwen Xu, and Julian McAuley.
2022. Efficiently tuned parameters are task embed-
dings. arXiv preprint arXiv:2210.11705.
Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and
Hanwang Zhang. 2022. Prompt-aligned gradient for
prompt tuning. arXiv preprint arXiv:2205.14865.
A
Appendix
A.1
Task List
We report the full list of tasks used in ten differ-
ent settings in Table 9. All tasks are taken from
CROSSFIT (Ye et al., 2021).
A.2
Relative gain of Every Target Task
We mainly report average relative gain (ARG) in
our experiments (§6). In this section, we show
detailed relative gain of each target task in Fig. 4 ∼
Fig. 13.
A.3
Absolute Scores for Every Target Task
We show detailed absolute scores for each target
task in Fig. 14 ∼Fig. 23.
A.4
Details of Sampled Tasks
We sample {12, 24} tasks from the original 45
source tasks in the Cls→Cls setting to investigate
the influence of the number of source tasks. The
details of sampled tasks are shown in Table 10.
A.5
Hyperparameter Sensitivity Analysis
As mentioned in §5.3, for MAML, we select
the inner learning rate from {2e−5, 3e−5, 5e−5},
the outer learning rate from {2e−1, 3e−1, 5e−1},
and total training steps from {2500, 5000, 10000}
in the R→R setting.
The best validation
performance (10.14% ARG) is achieved with
{3e−5, 5e−1, 5000}, while the worst validation
ARG is −16.21% when using {5e−5, 2e−1, 2500}.
We can see that MPT is quite sensitive to hyper-
parameters. It performs even worse than PT with
inappropriate hyperparameters.
A.6
Task Similarity Analysis
As discussed in §6, we use the correlation between
input subspaces for two tasks as the similarity score
between them. Detailed results of randomly picked
Task Pair Index
Average
1
2
3
4
5
Similar
0.772
0.695
0.754
0.819
0.802
0.768
Dissimilar
0.326
0.311
0.283
0.315
0.297
0.306
Table 7: Similarity scores of randomly picked similar
and dissimilar task pairs.
similar and dissimilar task pairs are shown in Ta-
ble 7.
A.7
Pilot Experiments on Prompt Transfer
We conduct some pilot experiments to explore the
soft prompt transferability between different source
tasks and a given single target task. We randomly
pick 3 target tasks in the R→R setting and con-
duct prompt tuning on these tasks to obtain their
corresponding prompt embeddings {𝑃1
𝑡, 𝑃2
𝑡, 𝑃3
𝑡}.
We then conduct prompt tuning on 30 randomly
selected source tasks to obtain the soft prompts
{𝑃1
𝑠, ..., 𝑃30
𝑠}.
As shown in Lin et al. (2022), the correlation
between input subspaces (the norm of projected
subspace onto the other subspace) for two tasks
could serve as the similarity score between them,
which may also indicate the transferability. For
each source/target task, we regard the soft prompt
as the task embedding (Zhou et al., 2022) and ob-
tain its subspace by Singular Value Decomposition
(SVD) following Saha et al. (2021). We then cal-
culate the correlation scores between a given target
task and all source tasks following Lin et al. (2022).
Finally, for each target task, we apply MPT with
3 different sets of source tasks: (i) 5 source tasks
with the highest correlation scores, (ii) 5 randomly
picked source tasks, and (iii) 5 source tasks with the
lowest correlation scores. The relative gain of every
target task is shown in Table 8. We can observe that
using 5 source tasks with the highest correlation
scores achieves better performance than the other
two settings, indicating that input subspaces could
be used to measure the soft prompt transferability
between different source tasks and a given single
target task.
Note that current experiments and analysis are
for a single target task. For the average perfor-
mance of many target tasks, we need more explo-
ration.
Target
Source
highest
random
lowest
Quoref
7.28
3.61
0.95
Glue-Qnli
9.53
4.36
4.87
Samsum
5.94
4.07
-1.42
Table 8: Relative gain in % for MPT when using differ-
ent sets of source tasks.
Partition: Random Source
glue-mrpc, math_qa, quarel, e2e_nlg_cleaned, tweet_eval-stance_atheism, lama-squad, tab_fact, aqua_rat, tweet_eval-emoji, glue-wnli, codah, tweet_eval-offensive,
wiki_qa, blimp-ellipsis_n_bar_1, openbookqa, sms_spam, acronym_identification, blimp-determiner_noun_agreement_with_adj_irregular_1, ethos-national_origin,
spider, hellaswag, superglue-wsc, numer_sense, ade_corpus_v2-dosage, blimp-ellipsis_n_bar_2, kilt_ay2, squad-no_context, google_wellformed_query, xsum,
wiqa, tweet_eval-stance_abortion, reddit_tifu-tldr, ade_corpus_v2-effect, qa_srl, ethos-religion, commonsense_qa, biomrc, superglue-multirc, ethos-race, eli5-askh,
glue-qqp, paws, ethos-directed_vs_generalized, glue-sst2, tweet_eval-hate, glue-rte, blimp-anaphor_number_agreement, lama-conceptnet, hate_speech_offensive,
superglue-wic, boolq, kilt_hotpotqa, quartz-no_knowledge, aslg_pc12, sick, tweet_eval-stance_climate, tweet_eval-sentiment, crows_pairs, glue-mnli, medi-
cal_questions_pairs, break-QDMR-high-level, qasc, imdb, ethos-gender, trec-finegrained, adversarialqa, onestop_english, web_questions, duorc, swag, proto_qa,
scitail, tweet_eval-stance_feminist, limit, common_gen, scicite, blimp-irregular_past_participle_adjectives, social_i_qa, anli, kilt_zsre, cosmos_qa, superglue-record,
squad-with_context, emotion, blimp-existential_there_quantifiers_1, race-middle, kilt_wow, sciq, wino_grande, rotten_tomatoes, superglue-cb, poem_sentiment,
ropes, reddit_tifu-title, piqa, climate_fever, lama-google_re, search_qa, mc_taco, blimp-wh_questions_object_gap, hotpot_qa, emo, kilt_nq, kilt_trex, quartz-
with_knowledge, dbpedia_14, yahoo_answers_topics, superglue-copa, blimp-anaphor_gender_agreement, hate_speech18, gigaword, multi_news, aeslc, quail
Partition: Random Target
quoref, wiki_split, ethos-disability, yelp_polarity, superglue-rte, glue-cola, ethos-sexual_orientation, blimp-sentential_negation_npi_scope, ai2_arc, amazon_polarity,
race-high, blimp-sentential_negation_npi_licensor_present, tweet_eval-irony, crawl_domain, freebase_qa, glue-qnli, hatexplain, ag_news, circa, samsum
Partition: Classification Source
superglue-rte, tweet_eval-sentiment, discovery, glue-rte, superglue-wsc, scicite, glue-mrpc, tweet_eval-stance_hillary, tweet_eval-offensive, emotion, hatexplain, glue-
cola, sick, paws, ethos-sexual_orientation, glue-qqp, tweet_eval-emotion, sms_spam, health_fact, glue-mnli, imdb, ethos-disability, glue-wnli, scitail, trec-finegrained,
yahoo_answers_topics, liar, glue-sst2, tweet_eval-stance_abortion, circa, tweet_eval-stance_climate, glue-qnli, tweet_eval-emoji, ethos-directed_vs_generalized,
ade_corpus_v2-classification, ag_news, hate_speech_offensive, superglue-wic, google_wellformed_query, tweet_eval-irony, ethos-gender, onestop_english, trec,
rotten_tomatoes, kilt_fever
Partition: Non-Classification Source
ade_corpus_v2-dosage, art, biomrc, blimp-anaphor_number_agreement, blimp-ellipsis_n_bar_2, blimp-sentential_negation_npi_licensor_present, blimp-
sentential_negation_npi_scope, break-QDMR-high-level, commonsense_qa, crows_pairs, dream, duorc, eli5-asks, eli5-eli5, freebase_qa, gigaword, hellaswag,
hotpot_qa, kilt_ay2, kilt_hotpotqa, kilt_trex, kilt_zsre, lama-conceptnet, lama-google_re, lama-squad, math_qa, numer_sense, openbookqa, piqa, proto_qa, qa_srl,
quarel, quartz-no_knowledge, race-high, reddit_tifu-title, reddit_tifu-tldr, ropes, sciq, social_i_qa, spider, superglue-multirc, wiki_bio, wikisql, xsum, yelp_review_full
Partition: Both (Classification + Non-Classification) Source
ade_corpus_v2-dosage, biomrc, blimp-ellipsis_n_bar_2, blimp-sentential_negation_npi_scope, commonsense_qa, crows_pairs, duorc, hellaswag, kilt_zsre, lama-
google_re, lama-squad, math_qa, numer_sense, openbookqa, piqa, proto_qa, quartz-no_knowledge, race-high, reddit_tifu-tldr, ropes, sciq, wiki_bio, discovery,
emotion, ethos-disability, ethos-sexual_orientation, glue-cola, glue-mnli, glue-mrpc, glue-qqp, glue-rte, glue-wnli, hatexplain, health_fact, imdb, paws, scicite, sick,
sms_spam, superglue-rte, superglue-wsc, tweet_eval-emotion, tweet_eval-offensive, tweet_eval-sentiment, tweet_eval-stance_hillary
Partition: Classification Target
superglue-cb,dbpedia_14,wiki_qa,emo,yelp_polarity,ethos-religion,amazon_polarity,tab_fact,anli,ethos-race
Partition: Non-Classification Target
multi_news, superglue-copa, quail, blimp-anaphor_gender_agreement, common_gen, acronym_identification, quoref, wiki_split, ai2_arc, break-QDMR,
crawl_domain, samsum
Partition: QA Source
biomrc, boolq, freebase_qa, hotpot_qa, kilt_hotpotqa, kilt_nq, kilt_trex, kilt_zsre, lama-conceptnet, lama-google_re, lama-squad, lama-trex, mc_taco, numer_sense,
quoref, ropes, search_qa, squad-no_context, superglue-multirc, superglue-record, tweet_qa, web_questions
Partition: Non-QA Source
hate_speech_offensive, google_wellformed_query, circa, glue-sst2, scitail, emo, ag_news, art, paws, kilt_ay2, glue-qnli, ade_corpus_v2-classification, hatexplain,
emotion, glue-qqp, kilt_fever, dbpedia_14, glue-mnli, discovery, gigaword, amazon_polarity, tab_fact, tweet_eval-emoji, tweet_eval-offensive, tweet_eval-sentiment,
imdb, liar, anli, wikisql, xsum, yahoo_answers_topics, yelp_polarity, yelp_review_full
Partition: QA Target
ai2_arc, codah, cosmos_qa, dream, hellaswag, qasc, quail, quarel, quartz-no_knowledge, quartz-with_knowledge, sciq, superglue-copa, swag, wino_grande, wiqa
Partition: Non-Paraphrase Classification Source
ade_corpus_v2-classification, ag_news, amazon_polarity, anli, circa, climate_fever, dbpedia_14, discovery, emo, emotion, ethos-directed_vs_generalized, ethos-
disability, ethos-gender, ethos-national_origin, ethos-race, ethos-religion, ethos-sexual_orientation, financial_phrasebank, glue-cola, glue-mnli, glue-qnli, glue-
rte, glue-sst2, glue-wnli, google_wellformed_query, hate_speech18, hate_speech_offensive, hatexplain, health_fact, imdb, kilt_fever, liar, onestop_english,
poem_sentiment, rotten_tomatoes, scicite, scitail, sick, sms_spam, superglue-cb, superglue-rte, superglue-wic, superglue-wsc, tab_fact, trec, trec-finegrained,
tweet_eval-emoji, tweet_eval-emotion, tweet_eval-hate, tweet_eval-irony, tweet_eval-offensive, tweet_eval-sentiment, tweet_eval-stance_abortion, tweet_eval-
stance_atheism, tweet_eval-stance_climate, tweet_eval-stance_feminist, tweet_eval-stance_hillary, wiki_qa, yahoo_answers_topics, yelp_polarity
Partition: Paraphrase Target
glue-mrpc, glue-qqp, medical_questions_pairs, paws
Table 9: Full datasets for all settings described in Section 5.1. We provide references for all datasets in Table 11.
12 source tasks
superglue-rte, tweet_eval-sentiment, discovery, glue-rte, hatexplain, glue-cola, health_fact, glue-mnli, imdb, ethos-disability, glue-wnli, scitail
24 source tasks
superglue-rte, tweet_eval-sentiment, discovery, glue-rte, superglue-wsc, scicite, hatexplain, glue-cola, tweet_eval-emotion, sms_spam, health_fact, glue-mnli, imdb,
ethos-disability, glue-wnli, scitail, glue-sst2, tweet_eval-stance_abortion, glue-qnli, ethos-directed_vs_generalized, ag_news, hate_speech_offensive, ethos-gender,
kilt_fever
Table 10: Details of sampled {12, 24} tasks for investigating the impact of the number of source tasks.
Task Name
Reference
eli5-eli5
Fan et al. 2019
ethos-race
Mollas et al. 2020
tweet_qa
Xiong et al. 2019
tweet_eval-stance_hillary
Barbieri et al. 2020
piqa
Bisk et al. 2020
acronym_identification
Pouran Ben Veyseh et al. 2020
wiki_split
Botha et al. 2018
scitail
Khot et al. 2018
emotion
Saravia et al. 2018
medical_questions_pairs
McCreery et al. 2020
blimp-anaphor_gender_agreement
Warstadt et al. 2020
sciq
Welbl et al. 2017
paws
Zhang et al. 2019
yelp_review_full
Zhang et al. 2015; (link)
freebase_qa
Jiang et al. 2019
anli
Nie et al. 2020
quartz-with_knowledge
Tafjord et al. 2019b
hatexplain
Mathew et al. 2020
yahoo_answers_topics
(link)
search_qa
Dunn et al. 2017
tweet_eval-stance_feminist
Barbieri et al. 2020
codah
Chen et al. 2019
lama-squad
Petroni et al. 2019, 2020
superglue-record
Zhang et al. 2018
spider
Yu et al. 2018
mc_taco
Zhou et al. 2019
glue-mrpc
Dolan and Brockett 2005
kilt_fever
Thorne et al. 2018
eli5-asks qa
Fan et al. 2019
imdb
Maas et al. 2011
tweet_eval-stance_abortion
Barbieri et al. 2020
aqua_rat
Ling et al. 2017
duorc
Saha et al. 2018
lama-trex
Petroni et al. 2019, 2020
tweet_eval-stance_atheism
Barbieri et al. 2020
ropes
Lin et al. 2019
squad-no_context
Rajpurkar et al. 2016
superglue-rte
Dagan et al. 2005
qasc
Khot et al. 2020
hate_speech_offensive
Davidson et al. 2017
trec-finegrained
Li and Roth 2002; Hovy et al. 2001
glue-wnli
Levesque et al. 2012
yelp_polarity
Zhang et al. 2015; (link)
kilt_hotpotqa
Yang et al. 2018
glue-sst2
Socher et al. 2013
xsum
Narayan et al. 2018
tweet_eval-offensive
Barbieri et al. 2020
aeslc
Zhang and Tetreault 2019
emo
Chatterjee et al. 2019
hellaswag
Zellers et al. 2019
social_i_qa
Sap et al. 2019
kilt_wow
Dinan et al. 2019
scicite
Cohan et al. 2019
superglue-wsc
Levesque et al. 2012
hate_speech18
de Gibert et al. 2018
adversarialqa
Bartolo et al. 2020
break-QDMR
Wolfson et al. 2020
dream
Sun et al. 2019
circa
Louis et al. 2020
wiki_qa
Yang et al. 2015
ethos-directed_vs_generalized
Mollas et al. 2020
wiqa
Tandon et al. 2019
poem_sentiment
Sheng and Uthus 2020
kilt_ay2
Hoffart et al. 2011
cosmos_qa
Huang et al. 2019
reddit_tifu-title
Kim et al. 2019
superglue-cb
de Marneffe et al. 2019
kilt_nq
Kwiatkowski et al. 2019
quarel
Tafjord et al. 2019a
race-high
Lai et al. 2017
wino_grande
Sakaguchi et al. 2020
break-QDMR-high-level
Wolfson et al. 2020
tweet_eval-irony
Barbieri et al. 2020
liar
Wang 2017
openbookqa
Mihaylov et al. 2018
superglue-multirc
Khashabi et al. 2018
race-middle
Lai et al. 2017
quoref
Dasigi et al. 2019
cos_e
Rajani et al. 2019
reddit_tifu-tldr
Kim et al. 2019
ai2_arc
Clark et al. 2018
quail
Rogers et al. 2020
crawl_domain
Zhang et al. 2020
glue-cola
Warstadt et al. 2019
Task Name
Reference
art
Bhagavatula et al. 2020
rotten_tomatoes
Pang and Lee 2005
tweet_eval-emoji
Barbieri et al. 2020
numer_sense
Lin et al. 2020a
blimp-existential_there_quantifiers_1
Warstadt et al. 2020
eli5-askh qa
Fan et al. 2019
ethos-national_origin
Mollas et al. 2020
boolq
Clark et al. 2019
qa_srl
He et al. 2015
sms_spam
Almeida et al. 2011
samsum
Gliwa et al. 2019
ade_corpus_v2-classification
Gurulingappa et al. 2012
superglue-wic
Pilehvar and Camacho-Collados 2019
ade_corpus_v2-dosage
Gurulingappa et al. 2012
tweet_eval-stance_climate
Barbieri et al. 2020
e2e_nlg_cleaned
Dušek et al. 2020, 2019
aslg_pc12
Othman and Jemni 2012
ag_news
Gulli (link)
math_qa
Amini et al. 2019
commonsense_qa
Talmor et al. 2019
web_questions
Berant et al. 2013
biomrc
Pappas et al. 2020
swag
Zellers et al. 2018
blimp-determiner_noun_agreement_with_adj_irregular_1
Warstadt et al. 2020
glue-mnli
Williams et al. 2018
squad-with_context
Rajpurkar et al. 2016
blimp-ellipsis_n_bar_2
Warstadt et al. 2020
financial_phrasebank
Malo et al. 2014
sick
Marelli et al. 2014
ethos-religion
Mollas et al. 2020
hotpot_qa
Yang et al. 2018
tweet_eval-emotion
Barbieri et al. 2020
dbpedia_14
Lehmann et al. 2015
ethos-gender
Mollas et al. 2020
tweet_eval-hate
Barbieri et al. 2020
ethos-sexual_orientation
Mollas et al. 2020
health_fact
Kotonya and Toni 2020
common_gen
Lin et al. 2020b
crows_pairs
Nangia et al. 2020
ade_corpus_v2-effect
Gurulingappa et al. 2012
blimp-sentential_negation_npi_scope
Warstadt et al. 2020
lama-conceptnet
Petroni et al. 2019, 2020
glue-qnli
Rajpurkar et al. 2016
quartz-no_knowledge
Tafjord et al. 2019b
google_wellformed_query
Faruqui and Das 2018
kilt_trex
Elsahar et al. 2018
blimp-ellipsis_n_bar_1
Warstadt et al. 2020
trec
Li and Roth 2002; Hovy et al. 2001
superglue-copa
Gordon et al. 2012
ethos-disability
Mollas et al. 2020
lama-google_re
Petroni et al. 2019, 2020
discovery
Sileo et al. 2019
blimp-anaphor_number_agreement
Warstadt et al. 2020
climate_fever
Diggelmann et al. 2020
blimp-irregular_past_participle_adjectives
Warstadt et al. 2020
tab_fact
Chen et al. 2020a
gigaword
Napoles et al. 2012
glue-rte
Dagan et al. 2005
tweet_eval-sentiment
Barbieri et al. 2020
limit
Manotas et al. 2020
wikisql
Zhong et al. 2017
glue-qqp
(link)
onestop_english
Vajjala and Luˇci´c 2018
amazon_polarity
McAuley and Leskovec 2013
blimp-wh_questions_object_gap
Warstadt et al. 2020
multi_news
Fabbri et al. 2019
proto_qa
Boratko et al. 2020
wiki_bio
Lebret et al. 2016
kilt_zsre
Levy et al. 2017
blimp-sentential_negation_npi_licensor_present
Warstadt et al. 2020
Table 11: References for all datasets.
ag_news
ai2_arc
amazon_polarity
blimp-lp
blimp-s
circa
crawl_domain
disability
orientation
freebase_qa
glue-cola
glue-qnli
hatexplain
quoref
race-high
samsum
superglue-rte
tweet-irony
wiki_split
yelp_polarity
100
50
0
50
100
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 4: Random to Random (Relative Gain)
amazon_polarityanli
dbpedia_14
emo
ethos-race
ethos-religion
superglue-cbtab_fact
wiki_qa
yelp_polarity
100
50
0
50
100
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 5: Classification to Classification (Relative Gain)
amazon_polarityanli
dbpedia_14
emo
ethos-race
ethos-religion
superglue-cbtab_fact
wiki_qa
yelp_polarity
50
0
50
100
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 6: Non-Classification to Classification (Relative
Gain)
amazon_polarityanli
dbpedia_14
emo
ethos-race
ethos-religion
superglue-cbtab_fact
wiki_qa
yelp_polarity
100
50
0
50
100
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 7: Both (Classification + Non-Classification) to
Classification (Relative Gain)
acronym
ai2_arc
blimp-a
break-QDMR
common_gen
crawl_domain
multi_news quail
quoref
samsum
copa
wiki_split
80
60
40
20
0
20
40
60
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 8: Non-Classification to Non-Classification (Rel-
ative Gain)
acronym
ai2_arc
blimp-a
break-QDMR
common_gen
crawl_domain
multi_news quail
quoref
samsum
copa
wiki_split
80
60
40
20
0
20
40
60
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 9: Classification to Non-Classification (Relative
Gain)
acronym
ai2_arc
blimp-a
break-QDMR
common_gen
crawl_domain
multi_news quail
quoref
samsum
copa
wiki_split
80
60
40
20
0
20
40
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 10: Both (Classification + Non-Classification) to
Non-Classification (Relative Gain)
glue-mrpc
glue-qqp
medical
paws
20
0
20
40
60
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 11: Non-Paraphrase Classification to Paraphrase
(Relative Gain)
ai2_arc codahcosmos_qadreamhellaswag qasc
quail
quarelquartz-no
quartz-with sciq
copa
swag
wino
wiqa
50
0
50
100
150
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 12: QA to QA (Relative Gain)
ai2_arc codahcosmos_qadreamhellaswag qasc
quail
quarelquartz-no
quartz-with sciq
copa
swag
wino
wiqa
80
60
40
20
0
20
40
60
80
Relative Gain (%)
MAML
FoMAML
Reptile
MTL
FT
Figure 13: Non-QA to QA (Relative Gain)
ag_news
ai2_arc
amazon_polarity
blimp-lp
blimp-s
circa
crawl_domain
disability
orientation
freebase_qa
glue-cola
glue-qnli
hatexplain
quoref
race-high
samsum
superglue-rte
tweet-irony
wiki_split
yelp_polarity
0
20
40
60
80
100
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 14: Random to Random (Absolute Scores)
amazon_polarityanli
dbpedia_14
emo
ethos-race
ethos-religion
superglue-cbtab_fact
wiki_qa
yelp_polarity
0
20
40
60
80
100
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 15: Classification to Classification (Absolute
Scores)
amazon_polarityanli
dbpedia_14
emo
ethos-race
ethos-religion
superglue-cbtab_fact
wiki_qa
yelp_polarity
0
20
40
60
80
100
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 16: Non-Classification to Classification (Abso-
lute Scores)
amazon_polarityanli
dbpedia_14
emo
ethos-race
ethos-religion
superglue-cbtab_fact
wiki_qa
yelp_polarity
0
20
40
60
80
100
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 17: Both (Classification + Non-Classification) to
Classification (Absolute Scores)
acronym
ai2_arc
blimp-a
break-QDMR
common_gen
crawl_domain
multi_news quail
quoref
samsum
copa
wiki_split
0
20
40
60
80
100
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 18: Non-Classification to Non-Classification
(Absolute Scores)
acronym
ai2_arc
blimp-a
break-QDMR
common_gen
crawl_domain
multi_news quail
quoref
samsum
copa
wiki_split
0
20
40
60
80
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 19: Classification to Non-Classification (Abso-
lute Scores)
acronym
ai2_arc
blimp-a
break-QDMR
common_gen
crawl_domain
multi_news quail
quoref
samsum
copa
wiki_split
0
20
40
60
80
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 20: Both (Classification + Non-Classification) to
Non-Classification (Absolute Scores)
glue-mrpc
glue-qqp
medical
paws
0
10
20
30
40
50
60
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 21: Non-Paraphrase Classification to Paraphrase
(Absolute Scores)
ai2_arc codahcosmos_qadreamhellaswag qasc
quail
quarelquartz-no
quartz-with sciq
copa
swag
wino
wiqa
0
20
40
60
80
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 22: QA to QA (Absolute Scores)
ai2_arc codahcosmos_qadreamhellaswag qasc
quail
quarelquartz-no
quartz-with sciq
copa
swag
wino
wiqa
0
20
40
60
80
Absolute Scores
PT
MAML
FoMAML
Reptile
MTL
FT
Figure 23: Non-QA to QA (Absolute Scores)
