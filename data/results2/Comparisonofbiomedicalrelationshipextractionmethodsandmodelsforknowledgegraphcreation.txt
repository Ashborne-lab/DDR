Comparison of biomedical relationship extraction methods and
models for knowledge graph creation
Nikola Milosevica,b, Wolfgang Thielemannc
aUniversity of Manchester, Faculty of Science and Engineering, Oxford road, Manchester, M13 9PL, United
Kingdom
bBayer Pharmaceuticals R&D, Mullerstrasse 178, Berlin, 13353, , Germany
cBayer Pharmaceuticals R&D, Friedrich-Ebert-Str.475, Wuppertal, 42117, , Germany
Abstract
Biomedical research is growing at such an exponential pace that scientists, researchers, and prac-
titioners are no more able to cope with the amount of published literature in the domain. The
knowledge presented in the literature needs to be systematized in such a way that claims and
hypotheses can be easily found, accessed, and validated. Knowledge graphs can provide such a
framework for semantic knowledge representation from literature. However, in order to build a
knowledge graph, it is necessary to extract knowledge as relationships between biomedical enti-
ties and normalize both entities and relationship types. In this paper, we present and compare a
few rule-based and machine learning-based (Naive Bayes, Random Forests as examples of tradi-
tional machine learning methods and DistilBERT, PubMedBERT, T5, and SciFive-based models
as examples of modern deep learning transformers) methods for scalable relationship extraction
from biomedical literature, and for the integration into the knowledge graphs. We examine how
resilient are these various methods to unbalanced and fairly small datasets. Our experiments show
that transformer-based models handle well both small (due to pre-training on a large dataset) and
unbalanced datasets. The best performing model was the PubMedBERT-based model ﬁne-tuned
on balanced data, with a reported F1-score of 0.92. The distilBERT-based model followed with an
F1-score of 0.89, performing faster and with lower resource requirements. BERT-based models
performed better than T5-based generative models.
Keywords:
knowledge graphs, information extraction, machine learning, natural language processing, text
mining, text-to-text model, linked data, transformers, PubMedBERT, T5, SciFive
1. Introduction
The amount of published scientiﬁc, espe-
cially biomedical literature is growing expo-
nentially.
In 2020, over 950,000 articles
were added to Medline (of Medicine, 2020), a
repository of the biomedical literature, mean-
ing that on average, over 2600 biomedical ar-
ticles were published daily.
Scientists, re-
searchers, and professionals are not able to
cope with the amount of published research
in their area and require tools that would help
them ﬁnd relevant articles, review and validate
Preprint submitted to Web Semantics
August 9, 2022
arXiv:2201.01647v4  [cs.AI]  7 Aug 2022
claims and hypotheses.
Finding relevant articles is the task ad-
dressed by the information retrieval sub-ﬁeld
of natural language processing.
A number
of information retrieval approaches have been
examined and several domain-speciﬁc infor-
mation retrieval applications for bio-medicine
have been built, such as PubMed, PubMed-
Central, Quertle, Embase, etc. (Canese and
Weis, 2013; Roberts, 2001; Coppernoll-Blach,
2011). Information retrieval engines can use
named entity recognition algorithms and entity
normalization techniques (often using dictio-
naries or terminologies) to improve the search
results by returning semantically the most rel-
evant articles for the searched concept regard-
less of the form or a synonym used for the
searched entity (Jonnalagadda and Topham,
2010; Jonnagaddala et al., 2015; Hakala et al.,
2016).
However, information retrieval only
oﬀers a list of relevant articles for searched
terms or concepts. In order to validate a hy-
pothesis or claim, a researcher still needs to
read through a signiﬁcant amount of literature,
which may be time-consuming.
The hypothesis and claims, that researchers
often would like to validate, can be summa-
rized in a simple sentence with two interacting
concepts and a predicate describing their inter-
action (e.g. Aspirin treats pain). Hypothesis
and claims are named relationships between
concepts. These named relationships can be
extracted with evidence (sentences from arti-
cles, stating them) from biomedical literature.
Also, entities may be connected with many
other entities in relationships, ﬁnally gener-
ating a large knowledge graph. This knowl-
edge graph can be later utilized to infer knowl-
edge by following connections (A →B, B →
C, there foreA →C), and even applications
of graph machine learning to ﬁnd potentially
missing edges (relationships), or discover po-
tential leads and targets in the drug discovery
process.
The ways of validating claims, and in-
ferring new knowledge from the statements
in the knowledge graphs have been exam-
ined in areas of knowledge graph databases
(Messina et al., 2017; Miller, 2013), seman-
tic web (McGuinness et al., 2004; Parsia and
Sirin, 2004; Sirin et al., 2007; Shearer et al.,
2008) and graph machine learning (Scarselli
et al., 2008; Veliˇckovi´c et al., 2017; Qu et al.,
2019).
However, in order to perform infer-
ence and validation over a knowledge graph,
the knowledge needs to be extracted from the
text and normalized. Most of the normaliza-
tion research in the biomedical domain consid-
ers the normalization of named entities, such
as diseases, genes, and compounds (Cho et al.,
2017; Ji et al., 2020; Zhou et al., 2020). On
the other hand, it was not given much atten-
tion to the normalization of biomedical rela-
tionships. Relationship extraction research in
the biomedical domain is often limited to a cer-
tain domain (e.g. cancer or cardiovascular do-
main) and considers a limited set of possible
relationship entity pairs and relationship types
(Rindﬂesch et al., 1999; Yang et al., 2021).
Normalized relationships (graph edges) are the
pillar of successful systematization of knowl-
edge in knowledge graphs.
As part of the R&D organization within
Bayer pharmaceuticals, we focus on generat-
ing knowledge graphs relevant to drug discov-
ery, target identiﬁcation, and indication expan-
sion.
Most of the use-cases we are dealing
with are related to humans. Therefore, the de-
ﬁned relationship model and methods for re-
lationship extraction, described in this paper,
will focus on the stated use-cases.
In this paper, we propose a data model for
relationship normalization between drugs, tar-
gets, and diseases. We also examine and com-
pare several rule-based and machine learning-
based approaches. Using the proposed meth-
2
ods, we generated a knowledge graph with
links to the evidence sentences, based on the
extracted and normalized relationships from
PubMed.
In the end, we compare and dis-
cuss proposed methods for knowledge graph
creation.
2. Background
Knowledge graphs have a long history span-
ning to the 1970s (Schneider, 1973). Knowl-
edge graphs are a ﬂexible knowledge rep-
resentation framework, where knowledge is
represented as a graph of inter-related con-
cepts.
Representing knowledge in a graph
has a number of practical beneﬁts in scenar-
ios that involve integrating, managing, and
extracting value from diverse and heteroge-
neous data sources. The idea of representing
knowledge in graphs, particularly gained inﬂu-
ence with Semantic Web, and lately with the
development of knowledge graph announced
in 2012 by Google, followed by other ma-
jor tech industry players (Hogan et al., 2021).
Lately, we could see applications of knowl-
edge graphs in question answering products in
wider use, such as Alexa, Google Assistant, or
Siri (Zhang et al., 2018). Likewise, the phar-
maceutical industry identiﬁed potential bene-
ﬁts knowledge graphs can bring in accelerat-
ing drug discovery, drug development, indica-
tion expansion of existing drugs, and pharma-
covigilance.
In order to extract information and structure
them for entry into the knowledge graph, it
is necessary to perform named entity recogni-
tion of relevant entities (for bio-medicine these
could be genes/targets, compounds, diseases,
cell lines, pathways, organs, etc.), normalize
all the possible synonyms to agreed terminol-
ogy and at the end ﬁnd relationships between
co-mentioned entities and normalize the rela-
tionships to the agreed data model or ontology.
Biomedical named entity recognition and
named entity normalization have a long tradi-
tion of research since the late 1990s (Fukuda
et al., 1998; Collier et al., 2000).
A num-
ber of approaches were developed that can
be classiﬁed into dictionary-based, machine
learning-based using usually hidden Markov
models or Conditional Random Fields and
Deep Learning-based, often using language
models, such as word2vec, ELMo (Milosevic
et al., 2020), BERT, and others, with trans-
formers (Khan et al., 2020) or recurrent neural
networks (Belousov et al., 2019).
In order to systematize extracted entities and
input them into the knowledge graph, they
need to be normalized. Normalization is a pro-
cess of mapping all possible terms and vari-
ants that represent one concept to one unique
entity id or preferred term (for example the
concept of cancer can be stated using various
expressions, such as neoplasms, tumor, can-
cer, malignity, etc.). For a long time, named
entity normalization relied on good dictionar-
ies and rule-based approaches (Leaman et al.,
2015; Cohen, 2005). However, in recent years,
there have been several deep learning-based
approaches for ranking candidate entities for
normalization using convolutional neural net-
works (Li et al., 2017; Deng et al., 2019)
or language models such as Word2Vec (Cho
et al., 2017) or BERT (Ji et al., 2020).
The extraction of the actual relationship
comes as the ﬁnal step of structuring infor-
mation from a sentence. One of the ﬁrst sys-
tems to attempt relationship extraction in the
biomedical domain was EDGAR (Rindﬂesch
et al., 1999), which was extracting relation-
ships between drugs and genes in the cancer
domain, using a set of rules based on syntac-
tic analysis. Since then, several approaches to
structuring relationships were explored:
• Existence of relationship between enti-
3
ties - classiﬁes whether there is an actual
semantic relationship between two enti-
ties, or the entities are co-mentioned, but
there is no actual named relationship be-
tween them. This approach of extracting
general existence of the relationship is of-
ten applied for protein-protein interaction
extraction (Zitnik et al., 2018; Szklarczyk
et al., 2019) or gene-disease interactions
(Becker et al., 2004).
• Extracting predicate verb as relation-
ship type - predicate is not a closed set
of possible classes. Any predicate verb,
appearing in a sentence indicating a rela-
tionship between entities, is taken as the
relationship type. Normalization of rela-
tionship types is left for further process-
ing or analysis.
Some of the research
databases, such as Open Targets use this
approach (Carvalho-Silva et al., 2019), as
well as some of the commercial tools that
allow relationship extraction (e.g.
Lin-
guamatics I2E).
• Normalizing relationship types - pred-
icate is normalized into the set of well-
deﬁned types.
This approach needs a
carefully crafted data model of possible
relationships, as well as a carefully devel-
oped dataset for machine learning or ex-
traction rules. In the semantic web com-
munity, there has been research on nor-
malizing a basic set of relationships in
the general domain, such as is-a, part-
of, equal (Arnold and Rahm, 2015; Speer
and Havasi, 2013; Speer et al., 2017).
Domain-speciﬁc and more granular data
models and datasets for this approach
are rare.
BioCreative VI and BioCre-
ative VII provided data and organized
shared tasks on chemical-protein interac-
tions (Krallinger et al., 2017, 2020)
From the methodological perspective, re-
lationship extraction can be performed using
machine learning or rule-based approaches.
Rule-based approaches range from using lists
of relationship-related keywords and distances
between concepts and keywords (Abacha and
Zweigenbaum, 2011; Ravikumar et al., 2017)
to using dependency parsers and evaluating
whether concepts are related in grammatical
sense (Erkan et al., 2007; Goertzel et al.,
2006).
On the other hand, machine learn-
ing approaches can be classiﬁed into two
groups: (1) supervised learning, using crafted
datasets (Peng et al., 2018; Liu et al., 2017)
and (2) semi-supervised or distant learning ap-
proaches, where a dataset is expanded based
on known relationships assuming that men-
tions of the same entities would entail the same
relationship (Mintz et al., 2009). Since 2009,
distant learning approaches have gained pop-
ularity and proved to be eﬀective in relation-
ship extraction, despite the assumption that all
co-mentions of the same entities would entail
the same relationship is not always correct and
adds noise. In addition to these two general ap-
proaches, hybrid approaches, combining rules,
dependency trees, and machine learning ap-
proaches have been popular for relationship
extraction (Erkan et al., 2007; Muzaﬀar et al.,
2015).
Over the last decade, several datasets for
supervised biomedical relationship extraction
have been developed.
Some of the widely
adapted and used datasets for relationship ex-
traction are BioCreative VI and VII Chem-
Prot datasets (Miranda et al., 2021), BioCre-
ative V Chemical-disease (CDR) dataset (Li
et al., 2016), ADE Corpus (Gurulingappa
et al., 2012), BioInfer (Pyysalo et al., 2007),
DDI’13 (Herrero-Zazo et al., 2013), n2c2
2018 ADE (Henry et al., 2020), N-ary (Peng
et al., 2017), BioRED (Luo et al., 2022) and
others. However, few of these datasets have
4
granular manually curated relationship types
annotated (many have either binary marker, or
broad relationships, or are not in the scope of
this paper - e.g. drug-drug or protein-protein
interactions are out of the scope of this study).
3. Method
In this paper, we compare methods for rela-
tionship extraction. All of our methods use in-
house modiﬁed Linnaeus (Gerner et al., 2010)
tool for named entity recognition and nor-
malization.
For relationship extraction, we
present and compare three methods: (1) a rule-
based method, based on sentence patterns and
dictionaries of trigger verbs and phrases, (2)
a machine learning method, based on tradi-
tional machine learning models (i.e.
Naive
Bayes, Random Forests), and (3) a deep learn-
ing method based on transformer architec-
tures, such as DistilBERT (Sanh et al., 2019),
text to text T5 transformer (Raﬀel et al., 2020),
as well as domain-speciﬁc BERT-based model
called PubMedBERT (Gu et al., 2021) and
domain-speciﬁc version of T5 model, called
SciFive (Phan et al., 2021).
3.1. Named entity recognition and normaliza-
tion (modiﬁcation of Linnaeus)
Named entity recognition was done by an
internally modiﬁed version of the Linnaeus
tool (Gerner et al., 2010). We have added a
number of features that would allow us to per-
form more ﬂexible entity matching while re-
lying on the Linnaeus algorithm which is fast
and reliable with good dictionaries. The added
features include:
• Handling a deﬁned set of characters, such
as white spaces and treating multiple se-
quential white space characters as a single
white space character.
• Implementing a global ﬂag that allows
ignoring cases of letters in matches if
needed
• A ﬂag for automatic pluralization (adding
-s and -es suﬃxes) of dictionary terms
• A functionality that can handle and
transliterate Greek characters (e.g. beta
- β) as well as functionality that can
handle the variation of the position of
Greek characters (e.g. Interferon-α vs α
-interferon).
• Removing or ignoring diacritics
• Synonym level exact, case sensitive, and
regular expression matching
• Flag to match only the longest match
The dictionaries used for named entity
recognition and normalization into entities
have been carefully internally developed, ex-
panded, and reﬁned over the past ﬁfteen years
by our internal information scientists. We have
used dictionaries for human genes, diseases,
and approved drugs.
3.2. Relationship data model
Relationships of interest are relationships
between drug, gene, and disease entities. The
three pairs of relationships of interest are: (1)
Drug-Gene, (2) Drug-Disease, and (3) Gene-
Disease. Each of these pairs may have sev-
eral distinct relationship types. In order to de-
velop the relationship model, we have orga-
nized two workshops guided by the authors
with the internal experts from the Bayer R&D
department. Eighteen people participated in
these workshops and the eﬀort to create the
data model.
They are members of the fol-
lowing teams within the Bayer R&D depart-
ment: scientiﬁc and competitive intelligence
(12 people), semantics and knowledge graph
5
technologies (3 people, including authors), re-
search and early development, kidney disease
(2 people), bioinformatics (1 person).
Ex-
perts from these teams have advanced degrees
(Ph.D.) in pharmacology, biology, or medicine
and often substantial working experience in
academia and within the pharmaceutical in-
dustry. They have helped us identify the pos-
sible relationship types for the entities we fo-
cused on and validate our model. The discus-
sions were guided by authors, who proposed
the initial data model, and then it was evalu-
ated, critiqued, and expanded by the group of
experts. The authors were starting with exist-
ing data models if they existed (e.g. for gene-
disease, BioCreative data model), or known
requirements coming from the drug discov-
ery department. These models were then sim-
pliﬁed, reviewed and, in some cases, miss-
ing relationship types were added.
We or-
ganized a meeting with one bioinformatics
expert, who helped us additionally expand
and validate Gene-Disease relationships and
possible modes of action.
Furthermore, we
scouted available commercial solutions that
provide knowledge graph solutions with en-
tities we were looking for. We have identi-
ﬁed two companies which provide data that is
close to our needs - Causally1 and Biorelate2.
The model, that we created, contained more
comprehensive and more detailed relationship
types (more relationship types, relationship at-
tributes, such as modes of action for genetic
relationships) for the relevant entity pairs, at
the time of writing this paper.
3.2.1. Drug-Gene relationships
The relationships between chemical and
proteins were subject of last two BioCreative
1https://www.causaly.com/
2https://www.biorelate.com/
CPR GROUP
TYPE
CPR:1
Part of
CPR:2
Regulator
CPR:3
Up-regulator, Activator
CPR:4
Down-regulator, Inhibitor
CPR:5
Agonist
CPR:6
Antagonist
CPR:7
Modulator
CPR:8
Co-factor
CPR:9
Substrate, Product of
CPR:10
Not
Table 1: Chemical-Protein relationship as deﬁned by
BioCreative shared task
shared tasks in 2017 3 and 2020 4. Both of the
tasks deﬁned the same interaction types. These
can be seen in Table 1. However, for the ma-
jority of purposes, some of the deﬁned types
may be redundant. Therefore, we have sim-
pliﬁed the data model by merging some of the
classes and excluding ones that are rarely men-
tioned in the text. In the end, our Drug-Gene
model had the following relationship classes:
• Up-regulator/activator
• Down-regulator/inhibitor
• Regulator
• Part of
• Modulator
• Co-factor
• Substrate or product of
Note that Regulator is a type of relationship
in which it is not possible to determine the di-
rection of regulation from the sentence.
3https://biocreative.bioinformatics.
udel.edu/tasks/biocreative-vi/track-5/
4https://biocreative.bioinformatics.
udel.edu/tasks/biocreative-vii/track-1/
6
3.2.2. Drug-Disease relationships
Relationships between drugs and diseases
do not have any gold standard data model that
was previously used. Therefore a new model
was proposed containing the following rela-
tionship classes:
• Therapeutic use/Treatment
• Cause/Adverse event
• Decrease Disease
• Increase Disease
• Eﬀect on
• Biomarker
It may seem that there is redundancy be-
tween Therapeutic use and Decrease Disease
classes, or between Cause and Increase Dis-
ease classes. However, Therapeutic use and
Cause indicate relationships where the dis-
ease is an indication or counter indication for
a given drug. Increase and Decrease disease
may refer to any ﬁnding that a given drug im-
proved or worsened the state of disease, and
therefore is a weaker relationship. The weak-
est relationship is Eﬀect on, because, in this
case, only the fact that there is some eﬀect of
a drug on disease is known, without any addi-
tional details (e.g. whether it improves disease
or makes it worse).
The chemical compound can be a biomarker
for some diseases. In medicine and drug dis-
covery, it is important to have a picture of
biomarkers, and therefore it is included in the
model.
3.2.3. Gene-Disease relationships
The relationship between genes and dis-
eases is the most complex one among the three
types in scope. This is because a single gene
can improve, worsen or even cause a certain
disease. Therefore, it is often not enough to
classify the type of the relationship, the algo-
rithm needs to extract also a mode of action
on the gene. In terms of possible relationship
types, we have identiﬁed the following ones:
• Plays a role — From the sentence can
be concluded clearly that there is a con-
nection between the gene and the disease,
however, it is not clear what kind of role
the gene plays in the disease, only that it
plays some role.
• Target
– General — The gene or protein can
be considered a target for the given
disease, with no more speciﬁc infor-
mation on the modulation of the dis-
ease.
– Cause — The sentence indicates
that activation, mutation or inhibi-
tion, or any other action over a gene
is causing a given disease.
– Modulator
* Decrease disease -– There is a
clear indication that gene is re-
sponsible for decreasing and al-
leviating the disease.
* Increase Disease – There is a
clear indication that gene is
responsible for increasing and
worsening the disease.
• Biomarker — The presence or lack of a
given gene/protein is an indicator for the
diagnosis of disease or pathology.
3.2.4. Mode of action
Together with the relationship classes, if
available, mode of action is an important mod-
iﬁer for Gene-Disease relationships. It estab-
lishes the action taken on a gene in order for
7
the relationship to take place. For example, a
gene may both decrease and cause disease, de-
pending on whether the gene was activated or
inhibited. Possible modes of action are (1) in-
hibition or down-regulation, (2) activation or
up-regulation, (3) mutation or modiﬁcation.
3.2.5. Negation and speculation
The relationship between entities in a sen-
tence may be negated, which reverses the se-
mantics of the relationship.
Therefore, it is
important to detect whether the relationship is
negated.
Likewise, statements in the text can be fac-
tual, stated as well-known facts, or specula-
tive. Speculative claims need to be taken with
more caution and therefore, speculation detec-
tion is included in our model.
3.3. Relationship extraction using rule-based
method
Based on the previously described rela-
tionship model, we have developed a rule-
based method for relationship extraction. The
method relies on vocabularies for relationship
trigger words, negation cues, speculation cues,
mode of action (MoA) cues, and grammar pat-
tern rule set. An example of vocabularies and
patterns in the ruleset with an example sen-
tence from which a relationship is extracted
using given rules and vocabularies is presented
in Figure 1.
The trigger word vocabulary contains a list
of relationship trigger words and phrases, with
metadata, such as to which relationship a given
word or phrase maps, between which entities,
whether entities have to be in a given order
of mentioning or can be in reverse order (e.g.
for Drug-Disease relationship whether it is al-
lowed for the drug to be after disease in the
sentence) and whether the phrase should be in-
terpreted as a regular expression.
Mode of action cues has a mapping to the
mode of action type (i.e. Inhibition, Activa-
tion, Mutation). The vocabularies for negation
and speculation cues are simple lists of words
(e.g. no, not for negation; hypothesise, may for
speculative).
Grammar patterns deﬁne sequences that
need to be matched in order to extract relation-
ships.
This grammar has several keywords,
such as Subject, which refers to the subject en-
tity, Predicate, which refers to predicate en-
tity, Trigger, referring to trigger cue, Specu-
lation, referring to Speculation, Negation, re-
ferring to negation cue, Subject type, referring
to entity that is not subject in current evalua-
tion pair of entities, but has same type as Sub-
ject entity (i.e. Drug or Gene), Predicate type,
referring to entity having same type as pred-
icate, but not evaluated in current pair. Ad-
ditionally, there may be deﬁned a number of
words that are between labeled entities, trig-
ger words, negations, and speculative phrases.
For example the following pattern:
Speculation W3 Subject W3 Trigger
W3 Predicate
would match sequences where the specu-
lative cue is followed by up to three tokens,
followed by Subject, followed by up to three
tokens, followed by trigger phrase, followed
by another three tokens and predicate. This
means it would match sentences such as ”We
hypothesize that aspirin can alleviate most
cases of headache”, if the token ”hypothesize”
is in the list of speculative cues, ”aspirin” is
marked as a drug and is subject, ”headache”
is a disease and predicate, and ”alleviate” is a
trigger word.
The matching algorithm iterates over la-
beled entities in each sentence and ﬁnds
all pairs that may constitute relationships.
It annotates sentences with potential trigger
8
Figure 1: Example of dictionaries, rules set and an example of sentence annotations in order to match relationship in
a sentence
phrases, speculative cues, and negations. Fi-
nally, the algorithm tries to match any pattern
from the grammar to the sequence in a sen-
tence. If the matching is successful, the rela-
tionship is extracted and mapped to the rela-
tionship type and metadata, such as mode of
action, negation, and speculation cues are ex-
tracted.
The conﬁdence score is calculated as a
proportion of words in sentences that ex-
actly match named phrases (Subject, Predi-
cate, Trigger, Speculation, Negation), divided
by all the words in the pattern (this includes
named phrases and tokens that were matched
as part of allowed distance tokens, e.g. up to
three words for each W3 statement in gram-
mar).
The rationale for this calculation is
based on the assumption that additional words
may change the semantics of the sentence and
therefore conﬁdence about the existence of the
relationship should be lower.
In addition, the method also extracts co-
occurrences, giving them ﬁxed conﬁdence
of 0.0001, and labeling them with a ”Co-
occurrence” label.
Each extracted relationship contains infor-
mation about entities (entity string, type, pre-
ferred term, internal ID), relationship type,
whether it is negated, whether it is speculative,
and conﬁdence score. Also, the evidence sen-
tence and Medline ID of the article where the
evidence was found are recorded.
3.4. Machine learning
We have developed machine learning meth-
ods for classifying relationship types.
The
task was modeled as a sentence-level classiﬁ-
cation task. The initial method is based on sen-
tence classiﬁcation using traditional machine
learning algorithms, such as Random Forests,
and Naive Bayes.
We have then advanced
the method by using ﬁne-tuned transformer-
based architectures for sentence-level relation-
ship type classiﬁcation, such as DistilBERT
(Sanh et al., 2019), and a text-to-text trans-
former called T5 (Raﬀel et al., 2020).
We
9
Relationship type
Unbalanced dataset
Balanced dataset
Biomarker
198
1243
No Explicit Relationship
446
446
Plays a role
7393
1532
Target->Causative
1460
1508
Target->General
656
1414
Target->Modulator->Decrease Disease
1108
1450
Target->Modulator->Increase Disease
720
1422
Table 2: Number of sentences for each relationship type in our balanced and unbalanced data sets
have also compared these methods to domain-
speciﬁc variants of BERT and T5 models,
called PubMedBERT (Gu et al., 2021) and Sci-
Five (Phan et al., 2021) respectively. We report
here results from all of the mentioned experi-
ments.
3.4.1. Training and testing data
The data are collected by using a rule-based
relationship extractor previously described for
the Gene-Disease relationship. We are evalu-
ating our approach to Gene-Disease data, as it
proved to have the most complex data model
and therefore is the most complex to correctly
extract relationships.
Also, this relationship
type is important from a biomedical perspec-
tive, as it may give insights on potential tar-
gets for treating respective diseases.
From
this dataset, 2000 sentences were reviewed and
corrected by human annotators. For this task,
a company called Molecular Connections was
contracted. Other 10 000 sentences were ob-
tained from the rule-based method, with conﬁ-
dence 1. These sentences would match cor-
rect sentences, as they do not allow for any
tokens that may change context, apart from
named phrases.
Therefore, the dataset con-
tained about 12 000 sentences. The data was
split as 90% training and 10% testing data for
training and testing of machine learning ap-
proaches.
In order to create a more balanced dataset,
we have generated the second dataset by tak-
ing 2000 manually annotated sentences, but
then adding sentences from the rule-based
method with conﬁdence 1 in such a way that
each relationship class had at least 1400 sen-
tences (for biomarkers, we could obtain 1243
sentences with conﬁdence 1, from a processed
portion of the data we had at the time of build-
ing the dataset). The statistics about the num-
ber of sentences per relationship class in our
datasets are presented in Table 2.
We have also created a dataset for the classi-
ﬁcation of a mode of action. We created again
one unbalanced and balanced dataset. Since
for the mutation class we had only 140 exam-
ples, we initially balanced the dataset by tak-
ing 140 examples from each class. This is a
fairly small dataset and may be improved by
adding examples. We have created an addi-
tional dataset taking 300 data samples from
each class, allowing duplication for classes
that did not have enough samples (e.g. mu-
tation). Since Tarawneh et al. (2022) argued
that oversampling with ﬁctitious data may re-
sult in the model failing when put to real-
world problems, we perform only duplica-
tion, keeping just real-world data.
Access
to the generated datasets can be requested
at https://zenodo.org/record/6466316#
.Ylw3T-dS9Ea.
10
3.4.2. Initial experiments using Random For-
est and Naive Bayes classiﬁers
Initially, we attempted to use traditional
machine learning algorithms, such as Naive
Bayes and Random Forests. For both of them,
sentences were tokenized and stemmed us-
ing Porter Stemmer (Porter, 1980). Since for
relationship extraction, it is important to ex-
amine the sequences of words, the features
for our classiﬁers were uni-grams, bi-grams,
tri-grams, and four-grams. Finally, data was
trained using Random Forest and Naive Bayes
Classiﬁer.
3.4.3. Transformer-based architectures: Dis-
tilBERT, PubMedBERT and T5 and
SciFive-based models
Transformer-based models are currently the
state-of-the-art machine learning methods that
perform well on a variety of tasks, ranging
from classiﬁcation to summarization and ques-
tion answering (Devlin et al., 2018; Raﬀel
et al., 2020). In the past few years, a num-
ber of language models were developed and
pre-trained on datasets such as common crawl.
Many of these models are based on the BERT
model, with various modiﬁcations to reduce
the size of the model or increase speed (Sanh
et al., 2019; Liu et al., 2019; Lan et al., 2019).
These models can be used for classiﬁcation by
using and training head - a feed-forward neu-
ral network on top of the transformer network
that outputs predictions. We will use a BERT-
based model that was optimized for size and
speed, called DistilBERT (Sanh et al., 2019),
whose authors claimed that has 40% fewer
parameters, runs 60% faster while preserving
over 97% of BERT’s performances as mea-
sured on the GLUE language understanding
benchmark. Both BERT and DistilBERT are
trained in the general domain. Since our task is
speciﬁc to the biomedical domain, we will also
use a BERT-based model trained on PubMed
and PubMed Central (PMC) articles released
by Microsoft, called PubMedBERT (Gu et al.,
2021). In 2020, Google released a text-to-text
transformer called T5. This model is generat-
ing textual output and a single model can be
trained to perform multiple tasks (specifying
task in the preﬁx of the input). In the original
paper, the authors of T5 claimed that the model
exhibits state-of-the-art performance and on
most of the tasks it outperformed BERT. In
this paper, we will evaluate that claim on the
sentence-level classiﬁcation of biomedical re-
lationships (gene-disease). We will also com-
pare it to the biomedical variant of the model
called SciFive (Phan et al., 2021).
The learning task was deﬁned as a sentence
classiﬁcation task. For a given sentence, con-
taining entities, the model is supposed to pro-
vide a normalized relationship type from our
data model. In the training and testing sen-
tences, the text was pre-prepared in such a way
that subject of the relationship (e.g. gene) was
masked with the keyword SUBJECT and the
predicated of the relationship (e.g.
disease)
was masked with the keyword PREDICATE.
In this way, we hypothesized that the internal
attention mechanism of the model would be
able to learn how to treat the vicinity of sub-
jects and the predicates of the relationships.
The DistilBERT model was based on the
DistilBERT base uncased model available on
HuggingFace5. This model was ﬁne-tuned for
the classiﬁcation task, and trained on our un-
balanced and balanced datasets for 5 epochs
(learning rate=0.00002). DistilBERT is an en-
coder model, to which a decoder can be cre-
ated using a pooling and feed-forward network
whose output layers dimension is equal to the
number of classes (in our case 8).
Similarly, PubMedBERT was based on the
base uncased version of the model trained on
5https://huggingface.co/distilbert-base-uncased
11
both PubMed and PMC data that is available
on HuggingFace6. The model was also trained
for 5 epochs and the same conﬁguration was
used for learning rate, batch size, and sequence
size as for DistilBERT model.
On the other hand, the T5 model has
encoder-decoder architecture, and therefore
we do not deﬁne additional layers. We have
ﬁne-tuned the T5 model that is readily avail-
able on HuggingFace7.
T5 is a multi-task
model that can be ﬁne-tuned and new tasks
can be added during the ﬁne-tuning of the
model. The multi-tasking nature of T5 is con-
venient since the same model can be deployed
once performing multiple tasks (e.g. question-
answering, summarization, translation, and re-
lationship extraction within the same API).
During the ﬁne-tuning of the model, we have
added new preﬁxes for gene-disease relation-
ship classiﬁcation and gene mode-of-action
classiﬁcation (with four classes - activation,
inhibition, mutation, and not reported).
We
have ﬁne-tuned the model on our dataset us-
ing Adafactor optimizer (Shazeer and Stern,
2018). The model was trained for 5 epochs
(learning rate=0.00002).
Same ﬁne-tuning
was performed on the domain-speciﬁc SciFive
base model available on HuggingFace8.
Encoder layers of T5, SciFive, and PubMed-
BERT have a size of 512 tokens, while Distil-
BERT has an encoder size of 728 tokens. Se-
quence sizes are longer than the longest sen-
tence in our dataset, therefore the size dif-
ference should not aﬀect the training and we
used padding to ﬁll the sequence with special
padding tokens.
6https://huggingface.co/microsoft/BiomedNLP-
PubMedBERT-base-uncased-abstract-fulltext
7https://huggingface.co/t5-base
8https://huggingface.co/razent/SciFive-base-
Pubmed PMC
4. Results
4.1. Rule-based relationship extraction
We have processed base Medline data un-
til January 2021, containing about 35 mil-
lion abstracts. The processing with both Lin-
naeus and the relationship extraction engine
took about 7 days on a single machine. We
managed to extract in total 4,784,985 rela-
tionships (with co-occurrences 35,900,521).
There were 631,573 named relationships
found between Drug-Genes (6,885,810 includ-
ing co-occurrences), 1,468,639 relationships
between Drug-Diseases (8,378,599 including
co-occurrences), and 2,684,742 relationships
between Genes and Diseases (20,065,385 in-
cluding co-occurrences).
Extracted relationships can be loaded into a
graph or relational database, where these re-
lationships can answer complex medical ques-
tions with evidence. By summing conﬁdence
scores, it is possible to retrieve genes interact-
ing with a given drug (e.g. top results for drug
Tolvaptan was inhibition of AVPR2, while the
second one was inhibition of vasopressin re-
ceptor family), drugs that have an eﬀect on
certain disease (e.g. for autosomal dominant
polycystic kidney disease retrieved Tolvaptan,
which is approved for autosomal dominant
polycystic kidney disease, Sirolimus, which
inhibits mTOR and as well have been often
used in polycystic kidney disease, and So-
matostatin, which was published as a hormone
having a potential role in the treatment of au-
tosomal dominant polycystic kidney disease
(Messchendorp et al., 2020)), or what genes
are important for a given disease (e.g. for auto-
somal dominant polycystic kidney disease re-
trieved PKD1 and PKD2 as targets that both
play a role and have a causative relationship
with disease, as well as mTOR, REN, CCL2).
We evaluated a case study related to autoso-
mal dominant polycystic kidney disease. We
12
Figure 2: Section of knowledge graph showing nodes that are in relationship with autosomal dominant polycys-
tic kidney disease (ADPKD). Orange entities are diseases (ADPKD), entities in blue are drugs and in green are
genes/proteins. Label on edges present relationship type, number of mentions and cumulative conﬁdence score for
the given relationship between two entities.
created a graph whose edges end in autosomal
dominant polycystic kidney disease. In order
to reduce noise, we consider only edges that
represent the relationship that was mentioned
at least 5 times in PubMed. We then evaluated
the graph and all entities were indeed known
from the literature to experts in the kidney dis-
ease team. A portion of the knowledge graph
with relationships ending in ADPKD is pre-
sented in Figure 2.
We have manually evaluated 100 abstracts
containing at least one relationship and calcu-
lated precision, recall, and F1-score. The eval-
uation is depending on the extent of trigger
phrases and completeness of grammar, which
is overtime improving. The measured perfor-
mance was 0,88 precision, 0,74 recall and 0,80
F1-score. It is expected that the rule-based ap-
proach would have high precision and lower
recall, as it would miss some of the relation-
ships, but annotate relatively few false pos-
itives.
Despite making some false positive
relationships, generated data perform well in
answering relevant biomedical questions be-
13
tween genes, drugs, and disease. The cumu-
lative eﬀect is that noise can be ignored by
setting a threshold and manually validating re-
sults below the given threshold if necessary.
4.2. Naive Bayes and Random Forests-based
Relationship extraction
The machine learning method was evaluated
on two sets (2000 manually annotated rela-
tionships + 10,000 random relationships with
conﬁdence 1 - unbalanced set, and 2000 man-
ually annotated relationships + random rela-
tionships with conﬁdence 1, so there are at
least 1,500 examples for each class - balanced
set).
For both data sets, 90% of data was
used as training data, while 10% of data (about
1200 sentences) was used as a testing set. The
results of our evaluation can be seen in the ta-
ble 3.
Balancing data signiﬁcantly improves pre-
cision and recall in both classiﬁers. With un-
balanced data, Naive Bayes learned to always
pick the most probable class - the class with
the most results.
The random forest classi-
ﬁer was better at learning how to recognize
classes. However, balancing data, gained 26%
in F1-score for Naive Bayes and 14% for over-
all results in Random Forests. The worst per-
formance has a class which we were unable
to balance due to the lack of annotated exam-
ples - No Explicit relationships (477 sentences
in the unbalanced set, that was annotated by
annotators). Other classes performed with an
F1-score over 70%.
4.3. Transformer-based relationship extrac-
tion
We have ﬁne-tuned base T5 and SciFive
models for relationship extraction by adding
a new preﬁx (”Relationship extraction:”) on
both unbalanced and balanced data. We have
monitored the performance of the algorithm
over epochs. The results can be seen in Fig-
ure 3.
Likewise, we have trained the DistilBERT
and PubMedBERT models on both datasets.
Balancing data improves all transformer
models, although the increase in performance
is just 1-2% (F1-score increase from 0.86 to
0.88 after ﬁve epochs on the T5 model, or
0.89-0.91 in DistilBERT). However, certain
relationship types in the unbalanced dataset
had a large gap between precision and recall
(e.g. ”No Explicit relationship” in unbalanced
had P=0.88, R=0.26), while in the balanced
dataset precision and recall were closer (for the
same class P=0.88, R=0.72).
We present the results of relationship classi-
ﬁcation after ﬁve epochs using general domain
models in Table 4, while results for domain-
speciﬁc models are in Table 5. Overall, the
BERT-based models performed better on both
data sets, even though the performance diﬀer-
ence was just 2-3%. Also, the BERT-based
models performed better on the majority of
relationship types. The stronger performance
of DistilBERT, compared to T5-based models,
is surprising and interesting due to its much
smaller nature (66 million parameters in Dis-
tilBERT base compared to 220 million param-
eters in T5 base). This may be due to the multi-
task and text-to-text nature of the T5 model,
as a number of parameters need to be retained
for other tasks and preﬁxes, as well as en-
coding to textual output. The best perform-
ing model was PubMedBERT, achieving F1-
score of 0.92, followed by DistilBERT with
F1-score of 0.91. The performance diﬀerence
between PubMedBERT and DistilBERT is ex-
pected and within 3% loss between BERT-
based models and distilled version of it, that
authors of original paper reported.
We have added preﬁx into the T5 model
and trained it for the classiﬁcation of gene-
associated modes of action into four possible
14
Class
Precision
Recall
F1-score
Unbalanced dataset
Naive Bayes
0.39
0.62
0.48
Biomarker
0
0
0
No Explicit Relationship
0
0
0
Plays a role
0.62
1
0.77
Target->Causative
0
0
0
Target->General
0
0
0
Target->Modulator->Decrease Disease
0
0
0
Target->Modulator->Increase Disease
0
0
0
Random Forests
0.74
0.71
0.66
Biomarker
0.80
0.16
0.27
No Explicit Relationship
0.89
0.30
0.45
Plays a role
0.70
0.99
0.82
Target->Causative
0.81
0.34
0.48
Target->General
0.75
0.24
0.37
Target->Modulator->Decrease Disease
0.76
0.31
044
Target->Modulator->Increase Disease
0.81
0.17
0.28
Balanced dataset
Naive Bayes
0.73
0.75
0.74
Biomarker
0.94
0.91
0.92
No Explicit Relationship
0
0
0
Plays a role
0.66
0.75
0.70
Target->Causative
0.66
0.89
0.76
Target->General
0.83
0.73
0.78
Target->Modulator->Decrease Disease
0.74
0.72
0.73
Target->Modulator->Increase Disease
0.84
0.76
0.80
Random Forests
0.79
0.79
0.78
Biomarker
0.97
0.85
0.91
No Explicit Relationship
0.64
0.15
0.24
Plays a role
0.65
0.80
0.72
Target->Causative
0.81
0.87
0.84
Target->General
0.76
0.84
0.80
Target->Modulator->Decrease Disease
0.75
0.81
0.78
Target->Modulator->Increase Disease
0.91
0.81
0.86
Table 3: Results of Naive Bayes and Random Forests classiﬁers
15
Figure 3: F1-score by epoch in ﬁne-tuned DistilBERT and T5 models on both unbalanced and balanced datasets
T5
DistilBERT
Class
Precision
Recall
F1-score
Precision
Recall
F1-score
Unbalanced dataset
Overall (weighted average)
0.87
0.87
0.86
0.89
0.89
0.89
Biomarker
1.00
0.52
0.69
0.75
0.63
0.69
No Explicit Relationship
0.88
0.26
0.40
0.57
0.52
0.54
Plays a role
0.91
0.95
0.93
0.96
0.95
0.95
Target->Causative
0.84
0.90
0.87
0.89
0.94
0.91
Target->General
0.75
0.79
0.77
0.72
0.75
0.74
Target->Decrease Disease
0.77
0.79
0.78
0.74
0.82
0.78
Target->Increase Disease
0.85
0.82
0.83
0.78
0.79
0.79
Balanced dataset
Overall (weighted average)
0.88
0.88
0.88
0.91
0.91
0.91
Biomarker
0.97
0.95
0.96
0.91
0.93
0.92
No Explicit Relationship
0.88
0.72
0.79
0.92
0.86
0.89
Plays a role
0.86
0.80
0.83
0.86
.0.82
0.84
Target->Causative
0.90
0.96
0.93
0.97
0.95
0.96
Target->General
0.83
0.87
0.85
0.92
0.93
0.93
Target->Decrease Disease
0.83
0.91
0.87
0.84
0.93
0.88
Target->Increase Disease
0.91
0.95
0.93
0.91
0.91
0.91
Table 4: Results of the best performing ﬁne-tuned T5 and DistilBERT models (after 5 epochs)
classes: (1) activation, (2) inhibition, (3) mu-
tation, and (4) not reported. The utility of the
T5 model is that a single model can perform
both classiﬁcations of sentences by relation-
16
SciFive
PubMedBERT
Class
Precision
Recall
F1-score
Precision
Recall
F1-score
Unbalanced dataset
Overall (weighted average)
0.91
0.87
0.88
0.90
0.90
0.90
Biomarker
0.87
0.62
0.72
0.76
0.90
0.83
No Explicit Relationship
0.29
0.85
0.43
0.74
0.44
0.56
Plays a role
0.97
0.93
0.94
0.95
0.96
0.95
Target->Causative
0.87
0.93
0.90
0.87
0.84
0.85
Target->General
0.87
0.63
0.73
0.74
0.86
0.80
Target->Decrease Disease
0.89
0.68
0.77
0.83
0.88
0.85
Target->Increase Disease
0.73
0.80
0.76
0.86
0.84
0.85
Balanced dataset
Overall (weighted average)
0.90
0.88
0.89
0.92
0.92
0.92
Biomarker
1.00
0.91
0.95
0.97
0.97
0.97
No Explicit Relationship
0.70
0.86
0.77
0.95
0.88
0.92
Plays a role
0.71
0.92
0.80
0.92
0.88
0.90
Target->Causative
0.99
0.89
0.94
0.94
0.96
0.95
Target->General
0.96
0.89
0.92
0.91
0.92
0.91
Target->Decrease Disease
0.91
0.80
0.85
0.88
0.92
0.90
Target->Increase Disease
0.96
0.92
0.94
0.91
0.93
0.92
Table 5: Results of the best performing ﬁne-tuned SciFive and PubMedBERT models (after 5 epochs)
ship type as well as the mode of action, for
which we would need separate DistilBERT-
based models. The model was trained on the
unbalanced and balanced dataset (each class
containing 300 examples of each class). The
model was trained for 5 epochs. The results
are presented in Table 6.
Mode-of-action detection performs well
with quite a small amount of data. This is be-
cause terms used for mode-of-action are in a
relatively closed set (activation, inhibition, in-
hibitor, agonist, antagonist, mutation, modu-
lation, etc.), and the language model is able
to transfer and infer them from its pre-training
on the C4 dataset. However, adding data helps
improve it.
5. Discussion
The presented rule-based methodology is
currently the base of the developed knowledge
graph. With about 5 million typed relation-
ships and over 30 million co-occurrences, it
presents a powerful tool for drug discovery,
target identiﬁcation, indication expansion, and
even pharmacovigilance. The graph structure
allows for analysis over multiple hops. This
will be further improved by adding protein-
protein, drug-drug, and disease-disease inter-
actions, on which we are working. It enables
visualization of interaction pathways for dis-
eases, graph learning for ﬁnding potentially
missing relationships, and validating hypothe-
ses about weak relationships.
The current number of relationships in our
graph is comparable with other state-of-the-art
databases and graphs that consider the same or
17
Class
Precision
Recall
F1-score
Unbalanced dataset
Overall (weighted average)
0.95
0.95
0.94
Activation
1.00
1.00
1.00
Inhibition
0.87
1.00
0.93
Mutation
1.00
0.77
0.87
Not reported
0.94
1.00
0.97
Balanced dataset (300 examples per class)
Overall (weighted average)
0.98
0.97
0.97
Activation
1.00
1.00
1.00
Inhibition
0.93
1.00
0.96
Mutation
0.97
1.00
0.99
Not reported
1.00
0.90
0.95
Table 6: Results of the best performing mode-of-action T5 model
similar relationships. Yang et al. (2021) devel-
oped a method for creating a stroke knowledge
graph using PKDE4J based on 9 entity types
and 30 relation types.
The relationship ex-
traction method based on BioBERT performed
with an F1-score of 84.26% and they man-
aged to extract 157 000 relationships based on
stroke-only papers in Pubmed (about 130 000
abstracts). Lee et al. (2022) developed like-
wise a method based on PKDE4J for entity
identiﬁcation and SciBERT for classiﬁcation
of relationships between genes, diseases, and
compounds. The data model had 8 relationship
types based on whether the relationship is di-
rected, undirected, positive, negative, and has
an increasing or decreasing eﬀect on the object
entity. The best performance of their model
is an F1-score of 81.7%. We believe that our
model performed better (91% F1-score), be-
cause the data model is more granular and
crafted for particular entity pair relationships,
therefore easier to learn than relationship types
generic for any biomedical entity pair.
Kim et al. (2017) focused on extracting
gene-disease evidence sentences, using exist-
ing tools extracting genetic events, but did not
classify the relationship between genes and
diseases. They managed to extract about 7.3
million evidence sentences from PubMed. Our
system is extracting mode of action, which
partially compares to biological events, on
top of which it also extracts typed relation-
ships. Our system extracted 2.7 million typed
relationships and 20 million co-occurrences,
therefore both wider (co-occurrences) and
more detailed (typed relationships with modes
of action) evidence.
Bhasuran and Natarajan (2018) used SVM
on word embeddings to classify the existence
of gene-disease relationships. The method was
evaluated with an F1-score of about 83%-87%.
RENET2 achieved 72.13% F1-score for ex-
tracting gene-disease relationships from Pub-
MedCentral articles (Su et al., 2021). How-
ever, gene-disease association types were not
classiﬁed.
Only the existence of the asso-
ciation was annotated.
A number of meth-
ods were proposed for similar gene-disease
association extraction without naming the re-
lationship type based on DisGeNet dataset
(Pi˜nero et al., 2016; Hebbar and Xie, 2021;
Parmar et al., 2020).
Since the publication
of DisGeNet, the research in this area accel-
erated.
Publicly available datasets with an-
18
notated biomedical relationship types are rare.
Therefore, there was a need for the creation
of new gene-disease relationship data set with
our described data model.
Our method based on PubMedBERT, as
well as DistilBERT, has better results than
most methods reported in the literature. How-
ever, the model relies on the relationship types,
consistency of annotators, and size of the
dataset. It is interesting to note that domain-
speciﬁc models added a small increase in per-
formance (1% in both cases). While the diﬀer-
ence between DistilBERT and PubMedBERT
may be attributed to the knowledge distillation
process, it is not the case for SciFive and T5
models, where increase certainly comes from
domain speciﬁcity. However, since the mean-
ing of a biomedical relationship is often de-
scribed by terms and phrases also often used
in the general domain, the eﬀect of domain-
speciﬁc models is not signiﬁcant.
6. Future work
The creation of a comprehensive biomedical
knowledge graph for target identiﬁcation, indi-
cation expansion, and drug discovery is a long-
term task.
Some of the future activities on
utilizing and improving our knowledge graph
will involve:
• Develop
machine
learning,
transformer-based models for other
relationship types (drug-gene, drug-
disease, drug-drug, gene-gene, disease-
disease).
This may involve further
annotation of data for other relationship
pairs and creating a model based on these
annotations.
• Unifying relationships obtained from
unstructured (literature, clinical tri-
als, expert reports, grant proposals)
and structured data sources - Combin-
ing structured and unstructured data pro-
vides better quality of results and opens
the possibility for a more detailed and
comprehensive evaluation of links in the
graph.
• Developing an interface for explor-
ing relationships and their evidence -
graphical user interface would enable a
wider scientiﬁc audience to utilize the
graph. This is especially important due to
the fact that the majority of pharmacolo-
gists and biologists working in the phar-
maceutical industry do not have an ex-
tensive computational background. This
would allow them to be more eﬃcient in
generating and evaluating hypotheses be-
fore going to the laboratory.
• Predicting novel target candidates us-
ing graph and temporal graph learn-
ing methods - based on the chronology
of the appearance of relationships in the
graph, it may be possible to learn pat-
terns and predict relationships between
entities that would be identiﬁed in future.
Based on the year of publication, we can
track when certain relationships appeared
in literature and how evidence around it
mounted. Therefore, it would be possi-
ble to automatically generate a hypothe-
sis about the existence of yet undiscov-
ered biological relationships using tem-
poral graph neural networks (Wang et al.,
2020).
7. Conclusions
In this paper, we have presented one rule-
based approach, that mainly served as a start-
ing point for obtaining biomedical relation-
ship data.
Further, we have compared tra-
ditional machine learning approaches, with
19
modern, state-of-the-art language models and
transformer approaches (DistilBERT, Pub-
MedBERT, T5 and SciFive models).
In all approaches, the improvement was no-
ticeable with balanced datasets, however, ﬁne-
tuned transformer-based models (DistilBERT,
PubMedBERT, T5, and SciFive) were more
resilient and did not depend so much on bal-
anced data sets as some older and traditional
approaches would (Naive Bayer and Random
Forests). Also, transformer-based models, due
to their pre-training on larger data, are able to
generalize well from a fairly small amount of
data. BERT-based models (PubMedBERT and
DistilBERT) performed slightly better than the
T5 models (T5 base and SciFive base), which
was a surprising ﬁnding since T5 has about 4
times more parameters than DistilBERT and
about 2 times more than PubMedBERT. How-
ever, this may be due to the multi-task nature
of T5, as well as the fact that part of the model
has to be used for text-to-text generation.
Developing machine learning data sets for
tasks such as relationship extraction can be
quite expensive. On the market, the pricing of
a single annotated sentence can range between
1-3 euros, depending on the complexity of the
task. However, this quickly scales, once the
data set has 7 annotation classes and there is
a need for over 1000 examples per annotation
class in the data set. The commissioned man-
ual annotations of our data set (around 7,000
sentences in total) cost 16,200 euros. The fur-
ther cost comes from cloud infrastructure and
machine learning engineering. Costs in devel-
oping relationship extraction models and ap-
proaches remain one of the main challenges.
Nevertheless, ﬁne-tuning transformer mod-
els proved to be a promising approach. First
of all, the performance of the model outper-
formed all other approaches, with over 92%
F1-score overall in the case of PubMedBERT,
and with the majority of annotation classes
breaching 85% F1-score. A review of the lit-
erature showed that the model performance is
state-of-the-art for biomedical typed relation-
ship extraction. Also, the model showed sta-
bility in terms of both precision and recall (un-
like the rule-based approach, which has high
precision but fairly low recall). On the other
hand, T5 models are multi-task models, where
it is possible to successfully address multiple
problems with a single model, which makes
valuable savings in terms of managing and up-
dating the models. Lastly, ﬁne-tuned T5 mod-
els, as they are text-to-text models, are easy to
use and data preparation is kept simple. Our
evaluation also showed, that in the particular
case of gene-disease relationship extraction,
domain-speciﬁc models add little performance
boost.
In terms of limitation, T5 models are large,
multi-task models, whose base model contains
220 million parameters.
This is, for exam-
ple, twice the size of the PubMedBERT base
and over four times the size of the DistilBERT
model, and it contributes heavily to the speed
of ﬁne-tuning and execution, making the pro-
cessing slow. Despite the fact that DistilBERT
can be trained only on a single task and there
is a need for post-processing of outputs, the
model has both performance and speed advan-
tages compared to the T5-based models. Dis-
tilBERT was outperformed by PubMedBERT
by 1%.
However, due to DistilBERTs size
and speed advantages, it may be preferable for
many productional use-cases.
References
Abacha, A. B., Zweigenbaum, P., 2011. Automatic ex-
traction of semantic relations between medical enti-
ties: a rule based approach. Journal of biomedical
semantics 2 (5), 1–11.
Arnold, P., Rahm, E., 2015. Semrep: A repository for
semantic mapping. Datenbanksysteme f¨ur Business,
Technologie und Web (BTW 2015).
20
Becker, K. G., Barnes, K. C., Bright, T. J., Wang, S. A.,
2004. The genetic association database. Nature ge-
netics 36 (5), 431–432.
Belousov, M., Milosevic, N., Dixon, W., Nenadic,
G., 2019. Extracting adverse drug reactions and
their context using sequence labelling ensembles in
tac2017. arXiv preprint arXiv:1905.11716.
Bhasuran, B., Natarajan, J., 2018. Automatic extraction
of gene-disease associations from literature using
joint ensemble learning. PloS one 13 (7), e0200699.
Canese, K., Weis, S., 2013. Pubmed: the bibliographic
database. In: The NCBI Handbook [Internet]. 2nd
edition. National Center for Biotechnology Informa-
tion (US).
Carvalho-Silva, D., Pierleoni, A., Pignatelli, M., Ong,
C.,
Fumis,
L.,
Karamanis,
N.,
Carmona,
M.,
Faulconbridge, A., Hercules, A., McAuley, E., et al.,
2019. Open targets platform:
new developments
and updates two years on. Nucleic acids research
47 (D1), D1056–D1065.
Cho, H., Choi, W., Lee, H., 2017. A method for named
entity normalization in biomedical articles: appli-
cation to diseases and plants. BMC bioinformatics
18 (1), 1–12.
Cohen, A., 2005. Unsupervised gene/protein named en-
tity normalization using automatically extracted dic-
tionaries. In:
Proceedings of the acl-ismb work-
shop on linking biological literature, ontologies and
databases: Mining biological semantics. pp. 17–24.
Collier, N., Nobata, C., Tsujii, J., 2000. Extracting
the names of genes and gene products with a hid-
den markov model. In: COLING 2000 Volume 1:
The 18th International Conference on Computational
Linguistics.
Coppernoll-Blach, P., 2011. Quertle: the conceptual
relationships alternative search engine for pubmed.
Journal of the Medical Library Association: JMLA
99 (2), 176.
Deng, P., Chen, H., Huang, M., Ruan, X., Xu, L., 2019.
An ensemble cnn method for biomedical entity nor-
malization. In: Proceedings of the 5th workshop on
BioNLP open shared tasks. pp. 143–149.
Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.,
2018. Bert: Pre-training of deep bidirectional trans-
formers for language understanding. arXiv preprint
arXiv:1810.04805.
Erkan, G., ¨Ozg¨ur, A., Radev, D., 2007. Semi-supervised
classiﬁcation for extracting protein interaction sen-
tences using dependency parsing. In: Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL). pp.
228–237.
Fukuda, K.-i., Tsunoda, T., Tamura, A., Takagi, T.,
et al., 1998. Toward information extraction: identi-
fying protein names from biological papers. In: Pac
symp biocomput. Vol. 707. Citeseer, pp. 707–718.
Gerner, M., Nenadic, G., Bergman, C. M., 2010. Lin-
naeus:
a species name identiﬁcation system for
biomedical literature. BMC bioinformatics 11 (1),
85.
Goertzel, B., Pinto, H., Heljakka, A., Ross, M., Pen-
nachin, C., Goertzel, I., 2006. Using dependency
parsing and probabilistic inference to extract rela-
tionships between genes, proteins and malignan-
cies implicit among multiple biomedical research ab-
stracts. In: Proceedings of the HLT-NAACL BioNLP
Workshop on Linking Natural Language and Biol-
ogy. pp. 104–111.
Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama,
N., Liu, X., Naumann, T., Gao, J., Poon, H.,
2021. Domain-speciﬁc language model pretrain-
ing for biomedical natural language processing.
ACM Transactions on Computing for Healthcare
(HEALTH) 3 (1), 1–23.
Gurulingappa, H., Rajput, A. M., Roberts, A., Fluck,
J., Hofmann-Apitius, M., Toldo, L., 2012. Develop-
ment of a benchmark corpus to support the automatic
extraction of drug-related adverse eﬀects from med-
ical case reports. Journal of biomedical informatics
45 (5), 885–892.
Hakala, K., Kaewphan, S., Salakoski, T., Ginter, F.,
2016. Syntactic analyses and named entity recog-
nition for pubmed and pubmed central—up-to-the-
minute. In: Proceedings of the 15th Workshop on
Biomedical Natural Language Processing. pp. 102–
107.
Hebbar, S., Xie, Y., 2021. Covidbert-biomedical rela-
tion extraction for covid-19. In: The International
FLAIRS Conference Proceedings. Vol. 34.
Henry, S., Buchan, K., Filannino, M., Stubbs, A.,
Uzuner, O., 2020. 2018 n2c2 shared task on adverse
drug events and medication extraction in electronic
health records. Journal of the American Medical In-
formatics Association 27 (1), 3–12.
Herrero-Zazo, M., Segura-Bedmar, I., Mart´ınez, P., De-
clerck, T., 2013. The ddi corpus:
An annotated
corpus with pharmacological substances and drug–
drug interactions. Journal of biomedical informatics
46 (5), 914–920.
Hogan, A., Blomqvist, E., Cochez, M., d’Amato, C.,
Melo, G. d., Gutierrez, C., Kirrane, S., Gayo, J. E. L.,
21
Navigli, R., Neumaier, S., et al., 2021. Knowledge
graphs. Synthesis Lectures on Data, Semantics, and
Knowledge 12 (2), 1–257.
Ji, Z., Wei, Q., Xu, H., 2020. Bert-based ranking for
biomedical entity normalization. AMIA Summits on
Translational Science Proceedings 2020, 269.
Jonnagaddala, J., Chang, N.-W., Jue, T. R., Dai, H.-
J., 2015. Recognition and normalization of disease
mentions in pubmed abstracts. In: Proceedings of
the ﬁfth BioCreative challenge evaluation workshop,
Sevilla, Spain. pp. 9–11.
Jonnalagadda, S., Topham, P., 2010. Nemo: Extrac-
tion and normalization of organization names from
pubmed aﬃliation strings. Journal of Biomedical
Discovery and Collaboration 5, 50.
Khan, M. R., Ziyadi, M., AbdelHady, M., 2020. Mt-
bioner: Multi-task learning for biomedical named
entity recognition using deep bidirectional trans-
formers. arXiv preprint arXiv:2001.08904.
Kim, J., Kim, J.-j., Lee, H., 2017. An analysis of
disease-gene relationship from medline abstracts by
digsee. Scientiﬁc reports 7 (1), 1–13.
Krallinger, M., Miranda, A., Mehryary, F., Luoma, J.,
Pyysalo, S., Valencia, A., 2020. Drugprot shared
task (biocreative vii track 1-2021) text mining drug-
protein/gene interactions (drugprot) shared task.
Krallinger, M., Rabal, O., Akhondi, S. A., P´erez, M. P.,
Santamar´ıa, J., Rodr´ıguez, G. P., Tsatsaronis, G., In-
txaurrondo, A., 2017. Overview of the biocreative vi
chemical-protein interaction track. In: Proceedings
of the sixth BioCreative challenge evaluation work-
shop. Vol. 1. pp. 141–146.
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma,
P., Soricut, R., 2019. Albert:
A lite bert for
self-supervised learning of language representations.
arXiv preprint arXiv:1909.11942.
Leaman, R., Wei, C.-H., Lu, Z., 2015. tmchem: a
high performance approach for chemical named en-
tity recognition and normalization. Journal of chem-
informatics 7 (1), 1–10.
Lee, Y., Son, J., Song, M., 2022. Bertsrc: Bert-based
semantic relation classiﬁcation.
Li, H., Chen, Q., Tang, B., Wang, X., Xu, H., Wang, B.,
Huang, D., 2017. Cnn-based ranking for biomedical
entity normalization. BMC bioinformatics 18 (11),
79–86.
Li, J., Sun, Y., Johnson, R. J., Sciaky, D., Wei, C.-H.,
Leaman, R., Davis, A. P., Mattingly, C. J., Wiegers,
T. C., Lu, Z., 2016. Biocreative v cdr task corpus:
a resource for chemical disease relation extraction.
Database 2016.
Liu, S., Shen, F., Wang, Y., Rastegar-Mojarad, M.,
Elayavilli, R. K., Chaudhary, V., Liu, H., 2017.
Attention-based neural networks for chemical pro-
tein relation extraction. Training 1020 (25.247),
4157.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.,
2019. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692.
Luo, L., Lai, P.-T., Wei, C.-H., Arighi, C. N., Lu, Z.,
2022. Biored: a rich biomedical relation extraction
dataset. Brieﬁngs in Bioinformatics.
McGuinness, D. L., Van Harmelen, F., et al., 2004. Owl
web ontology language overview. W3C recommen-
dation 10 (10), 2004.
Messchendorp, A. L., Casteleijn, N. F., Meijer, E., Gan-
sevoort, R. T., 2020. Somatostatin in renal physiol-
ogy and autosomal dominant polycystic kidney dis-
ease. Nephrology Dialysis Transplantation 35 (8),
1306–1316.
Messina, A., Pribadi, H., Stichbury, J., Bucci, M., Klar-
man, S., Urso, A., 2017. Biograkn: A knowledge
graph-based semantic database for biomedical sci-
ences. In: Conference on Complex, Intelligent, and
Software Intensive Systems. Springer, pp. 299–309.
Miller, J. J., 2013. Graph database applications and con-
cepts with neo4j. In: Proceedings of the Southern As-
sociation for Information Systems Conference, At-
lanta, GA, USA. Vol. 2324.
Milosevic, N., Kalappa, G., Dadafarin, H., Azimaee,
M., Nenadic, G., 2020. Mask: A ﬂexible framework
to facilitate de-identiﬁcation of clinical texts. arXiv
preprint arXiv:2005.11687.
Mintz, M., Bills, S., Snow, R., Jurafsky, D., 2009. Dis-
tant supervision for relation extraction without la-
beled data. In: Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP. pp. 1003–1011.
Miranda, A., Mehryary, F., Luoma, J., Pyysalo, S., Va-
lencia, A., Krallinger, M., 2021. Overview of drug-
prot biocreative vii track:
quality evaluation and
large scale text mining of drug-gene/protein rela-
tions. In: Proceedings of the seventh BioCreative
challenge evaluation workshop.
Muzaﬀar, A. W., Azam, F., Qamar, U., 2015. A rela-
tion extraction framework for biomedical text using
hybrid feature set. Computational and mathematical
methods in medicine 2015.
of
Medicine,
N.
L.,
2020.
Citations
added
to
medline
by
ﬁscal
year
@online
available
at
22
https://www.nlm.nih.gov/bsd/stats/cit added.html.
URL
https://www.nlm.nih.gov/bsd/stats/
cit_added.html
Parmar,
J.,
Koehler,
W.,
Bringmann,
M.,
Volz,
K. S., Kapicioglu, B., 2020. Biomedical informa-
tion extraction for disease gene prioritization. arXiv
preprint arXiv:2011.05188.
Parsia, B., Sirin, E., 2004. Pellet: An owl dl reasoner. In:
Third international semantic web conference-poster.
Vol. 18. Citeseer, p. 13.
Peng, N., Poon, H., Quirk, C., Toutanova, K., Yih,
W.-t., 2017. Cross-sentence n-ary relation extraction
with graph lstms. Transactions of the Association for
Computational Linguistics 5, 101–115.
Peng, Y., Rios, A., Kavuluru, R., Lu, Z., 2018.
Chemical-protein relation extraction with ensem-
bles of svm, cnn, and rnn models. arXiv preprint
arXiv:1802.01255.
Phan, L. N., Anibal, J. T., Tran, H., Chanana, S., Ba-
hadroglu, E., Peltekian, A., Altan-Bonnet, G., 2021.
Sciﬁve: a text-to-text transformer model for biomed-
ical literature. arXiv preprint arXiv:2106.03598.
Pi˜nero, J., Bravo, `A., Queralt-Rosinach, N., Guti´errez-
Sacrist´an, A., Deu-Pons, J., Centeno, E., Garc´ıa-
Garc´ıa, J., Sanz, F., Furlong, L. I., 2016. Disgenet: a
comprehensive platform integrating information on
human disease-associated genes and variants. Nu-
cleic acids research, gkw943.
Porter, M. F., 1980. An algorithm for suﬃx stripping.
Program.
Pyysalo, S., Ginter, F., Heimonen, J., Bjorne, J., Boberg,
J., Jarvinen, J., Salakoski, T., 2007. Bioinfer: a cor-
pus for information extraction in the biomedical do-
main. BMC bioinformatics 8 (1), 1–24.
Qu, M., Bengio, Y., Tang, J., 2019. Gmnn: Graph
markov neural networks. In: International confer-
ence on machine learning. PMLR, pp. 5241–5250.
Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
S., Matena, M., Zhou, Y., Li, W., Liu, P. J., 2020. Ex-
ploring the limits of transfer learning with a uniﬁed
text-to-text transformer. Journal of Machine Learn-
ing Research 21, 1–67.
Ravikumar, K., Rastegar-Mojarad, M., Liu, H., 2017.
Belminer:
adapting a rule-based relation extrac-
tion system to extract biological expression language
statements from bio-medical literature evidence sen-
tences. Database 2017.
Rindﬂesch, T. C., Tanabe, L., Weinstein, J. N., Hunter,
L., 1999. Edgar: extraction of drugs, genes and rela-
tions from the biomedical literature. In: Biocomput-
ing 2000. World Scientiﬁc, pp. 517–528.
Roberts, R. J., 2001. Pubmed central: The genbank of
the published literature.
Sanh,
V.,
Debut,
L.,
Chaumond,
J.,
Wolf,
T.,
2019.
Distilbert,
a
distilled
version
of
bert:
smaller, faster, cheaper and lighter. arXiv preprint
arXiv:1910.01108.
Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner,
M., Monfardini, G., 2008. The graph neural network
model. IEEE transactions on neural networks 20 (1),
61–80.
Schneider, E. W., 1973. Course modularization applied:
The interface system and its implications for se-
quence control and data analysis.
Shazeer, N., Stern, M., 2018. Adafactor:
Adaptive
learning rates with sublinear memory cost. In: Inter-
national Conference on Machine Learning. PMLR,
pp. 4596–4604.
Shearer, R., Motik, B., Horrocks, I., 2008. Hermit: A
highly-eﬃcient owl reasoner. In: Owled. Vol. 432.
p. 91.
Sirin, E., Parsia, B., Grau, B. C., Kalyanpur, A., Katz,
Y., 2007. Pellet: A practical owl-dl reasoner. Journal
of Web Semantics 5 (2), 51–53.
Speer, R., Chin, J., Havasi, C., 2017. Conceptnet 5.5:
An open multilingual graph of general knowledge.
In: Thirty-ﬁrst AAAI conference on artiﬁcial intelli-
gence.
Speer, R., Havasi, C., 2013. Conceptnet 5: A large se-
mantic network for relational knowledge. In: The
People’s Web Meets NLP. Springer, pp. 161–176.
Su, J., Wu, Y., Ting, H.-F., Lam, T.-W., Luo, R., 2021.
Renet2: high-performance full-text gene–disease re-
lation extraction with iterative training data expan-
sion. NAR Genomics and Bioinformatics 3 (3),
lqab062.
Szklarczyk, D., Gable, A. L., Lyon, D., Junge,
A., Wyder, S., Huerta-Cepas, J., Simonovic, M.,
Doncheva, N. T., Morris, J. H., Bork, P., et al., 2019.
String v11:
protein–protein association networks
with increased coverage, supporting functional dis-
covery in genome-wide experimental datasets. Nu-
cleic acids research 47 (D1), D607–D613.
Tarawneh, A. S., Hassanat, A. B., Altarawneh, G. A.,
Almuhaimeed, A., 2022. Stop oversampling for class
imbalance learning: A review. IEEE Access.
Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A.,
Lio, P., Bengio, Y., 2017. Graph attention networks.
arXiv preprint arXiv:1710.10903.
Wang, X., Ma, Y., Wang, Y., Jin, W., Wang, X., Tang, J.,
Jia, C., Yu, J., 2020. Traﬃc ﬂow prediction via spa-
tial temporal graph neural network. In: Proceedings
23
of The Web Conference 2020. pp. 1082–1092.
Yang, X., Wu, C., Nenadic, G., Wang, W., Lu, K.,
2021. Mining a stroke knowledge graph from liter-
ature. BMC bioinformatics 22 (10), 1–19.
Zhang, Y., Dai, H., Kozareva, Z., Smola, A. J., Song,
L., 2018. Variational reasoning for question answer-
ing with knowledge graph. In: Thirty-Second AAAI
Conference on Artiﬁcial Intelligence.
Zhou, H., Ning, S., Liu, Z., Lang, C., Liu, Z., Lei,
B., 2020. Knowledge-enhanced biomedical named
entity recognition and normalization: application to
proteins and genes. BMC bioinformatics 21 (1), 35.
Zitnik, M., Agrawal, M., Leskovec, J., 2018. Modeling
polypharmacy side eﬀects with graph convolutional
networks. Bioinformatics 34 (13), i457–i466.
24
