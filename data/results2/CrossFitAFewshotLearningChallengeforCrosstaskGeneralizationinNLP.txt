Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7163–7189
November 7–11, 2021. c⃝2021 Association for Computational Linguistics
7163
CROSSFIT
: A Few-shot Learning Challenge for
Cross-task Generalization in NLP
Qinyuan Ye
Bill Yuchen Lin
Xiang Ren
University of Southern California
{qinyuany, yuchen.lin, xiangren}@usc.edu
Abstract
Humans can learn a new language task efﬁ-
ciently with only few examples, by leveraging
their knowledge obtained when learning prior
tasks. In this paper, we explore whether and
how such cross-task generalization ability can
be acquired, and further applied to build bet-
ter few-shot learners across diverse NLP tasks.
We introduce CROSSFIT
, a problem setup
for studying cross-task generalization ability,
which standardizes seen/unseen task partitions,
data access during different learning stages,
and the evaluation protocols.
To instantiate
different seen/unseen task partitions in CROSS-
FIT and facilitate in-depth analysis, we present
the NLP Few-shot Gym, a repository of 160
diverse few-shot NLP tasks created from open-
access NLP datasets and converted to a uni-
ﬁed text-to-text format. Our analysis reveals
that the few-shot learning ability on unseen
tasks can be improved via an upstream learn-
ing stage using a set of seen tasks. We also
observe that the selection of upstream learning
tasks can signiﬁcantly inﬂuence few-shot per-
formance on unseen tasks, asking further anal-
ysis on task similarity and transferability.1
1
Introduction
Pre-trained language models ﬁne-tuned with abun-
dant task-speciﬁc data have become the predomi-
nant recipe for state-of-the-art results in NLP. How-
ever, these approaches are heavily dependent on
large-scale labeled datasets that are expensive to
create, and the resulting models still generalize
poorly to out-of-distribution inputs created with
small, harmless perturbations (Ribeiro et al., 2020).
In retrospect, researchers have advocated for build-
ing more human-like, general linguistic intelli-
gence that can “reuse previously acquired knowl-
edge about a language and adapt to a new task
quickly” (Yogatama et al., 2019; Linzen, 2020).
1Our code is at https://github.com/INK-USC/CrossFit.
Others 
(regression, etc.)
NLP Few-shot Gym
A repository of 160 diverse 
few-shot tasks in NLP
Stage 1. Upstream Learning
(Multitask Learning, Meta-learning, etc.)
Text-to-text 
Transformer
Text-to-text 
Transformer
Stage 2. Downstream Fine-tuning
Training Tasks
(e.g., non-cls. tasks)
Test Tasks
(e.g., classification)
Classification
Question 
Asnwering 
Conditional 
Generation
The Cr ossFit  Challenge
Can we build a few-shot learner to 
generalize beyond task boundaires?
Figure 1: We present the CROSSFIT Challenge to study
cross-task generalization in a diverse task distribution.
To support this problem setting, we introduce the NLP
Few-shot Gym, a repository of 160 diverse few-shot,
text-to-text tasks in NLP.
Existing work has approached this problem via
better few-shot ﬁne-tuning, by re-formulating tar-
get tasks into cloze questions that resembles the pre-
training objective (Schick and Schütze, 2020a,b),
generating prompts and using demonstrations (Gao
et al., 2020). Such progress primarily focus on
improving instance-level generalization, i.e., how
to better generalize from few labeled instances to
make predictions about new instances, within the
scope of one individual task. From a broader per-
spective, human-like learning ability also beneﬁts
from task-level generalization, or cross-task gener-
alization, i.e., how to learn a new task efﬁciently
given experiences of learning previous tasks.
Such ability has been widely studied in computer
vision and robotics community (Yu et al., 2020;
Triantaﬁllou et al., 2020), but is relatively under-
explored in NLP. Pruksachatkun et al. (2020) and
Vu et al. (2020) study transferability between one
intermediate task and a given target task, while it’s
possible to further improve performance with multi-
ple intermediate tasks. Han et al. (2018) and Bansal
et al. (2020a) focus on cross-task generalization
within the scope of classiﬁcation tasks, whereas hu-
7164
mans can generalize across different task formats
(classiﬁcation, multiple choice, generation, etc.),
goals (question answering, fact checking, etc.) and
domains (biomedical, social media, etc.).
Towards developing general linguistic intelli-
gence, we present CROSSFIT, a few-shot learning
challenge to acquire, evaluate and analyze cross-
task generalization in a realistic setting, with stan-
dardized training pipeline, data access and evalua-
tion protocol. The CROSSFIT challenge requires
a model to ﬁrst learn from a set of seen tasks in
an upstream learning stage, and then perform few-
shot learning on a set of unseen tasks, as illustrated
in Fig. 1. In accompany, we introduce the NLP
Few-shot Gym, a repository of 160 few-shot NLP
tasks gathered from open-access resources, cov-
ering a wide range of capabilities and goals, and
formulated into a uniﬁed text-to-text format. To
analyze the capability and limitation of existing
approaches to the CROSSFIT challenge, we design
eight speciﬁc seen/unseen task partitions.
With the CROSSFIT Challenge and the NLP Few-
shot Gym, we aim to investigate the following re-
search questions:
• Q1. Can we teach cross-task generalization abil-
ity to pre-trained models with existing methods?
• Q2. During upstream learning, is it better to
be “well-rounded” (learning from diverse tasks)
or be “specialized and targeted” (learning from
tasks in the same category with unseen tasks)?
• Q3. Does it help if we have more labelled data
for seen tasks during upstream learning?
To address the above questions, we empirically
analyze the performance of multi-task learning
and three meta-learning algorithms (MAML (Finn
et al., 2017), ﬁrst-order MAML and Reptile (Nichol
et al., 2018)). We observe that these approaches
can indeed lead to better few-shot performance
on unseen tasks. Interestingly, simple multi-task
learning outperforms existing meta-learning meth-
ods in many cases, encouraging future research on
identifying the reasons and developing improved
meta-learning methods. For Q2, we observe that
performance of individual unseen tasks varies with
different selection of seen tasks, calling for more
thorough investigation of the relationship between
task similarity and transferability. As for Q3, we
ﬁnd that enlarging the size of upstream data does
not necessitate better cross-task generalization abil-
ities. We envision cross-task generalization to be
an integral component towards general linguistic
intelligence, and we hope CROSSFIT serves as a
useful testbed for driving related progress.
2
Related Work
Few-shot Fine-tuning.
Few-shot learning refers
to teaching models a new task with a small num-
ber of annotated examples. Large-scale pre-trained
language models (e.g., BERT (Devlin et al., 2019))
have demonstrated great ability to learn new tasks
efﬁciently via ﬁne-tuning (Zhang et al., 2021).
Schick and Schütze (2020a,b) proposed pattern-
exploiting training (PET), which formulates text
classiﬁcation and NLI tasks into cloze questions (or
“prompts”) that resemble masked language model-
ing. PET can be further improved by generating
prompts automatically and incorporating demon-
strations into the input (Gao et al., 2020); and by
densifying the supervision signal with label con-
ditioning (Tam et al., 2021). While successful, in
these approaches the downstream tasks are learned
in isolation. Our work aims to boost few-shot learn-
ing ability on unseen tasks via acquiring cross-task
generalization ability from diverse seen tasks.
Meta-learning in NLP.
Recent works have ex-
plored meta-learning methods for relation classi-
ﬁcation (Han et al., 2018; Gao et al., 2019), gen-
eral text classiﬁcation (Dou et al., 2019; Bansal
et al., 2020a,b), low-resource machine transla-
tion (Gu et al., 2018), cross-lingual NLI/QA
(Nooralahzadeh et al., 2020). In general, these
works apply meta-learning algorithms to a set of
sub-tasks; however the sub-tasks are either syn-
thetic (e.g., classifying a new set of ﬁve relations
is a new sub-task) or drawn from a rather narrow
distribution (e.g., QA in one language is a sub-task).
In our work, we explore a more realistic setting –
learning from a set of NLP tasks with diverse goals:
classiﬁcation, question answering, conditional gen-
eration, etc. This setting is attracting attention in
NLP community rapidly and is also explored in
very recent work (Zhong et al., 2021; Mishra et al.,
2021; Bragg et al., 2021; Wei et al., 2021).
Unifying NLP Task Formats.
Researchers have
explored unifying the formats of different tasks,
in order to better enable knowledge transfer, e.g.,
DecaNLP (McCann et al., 2018), UFO-Entail (Yin
et al., 2020) and EFL (Wang et al., 2021). Fol-
lowing T5 (Raffel et al., 2020), we adopt a uni-
ﬁed text-to-text format that subsumes all text-based
tasks of interest. Related to our work, UniﬁedQA
7165
(Khashabi et al., 2020) examines the feasibility
of training a general cross-format QA model with
multi-task learning. Our work extends from these
ideas, and we signiﬁcantly enlarge the task repos-
itory to 160 to broaden the coverage, in hopes to
build a general-purpose few-shot learner.
3
The CROSSFIT Challenge
In this section, we present the CROSSFIT Chal-
lenge, a problem setting for acquiring and evalu-
ating cross-task generalization. Ideally, a strong
CROSSFIT system can capture cross-task general-
ization ability from a set of seen tasks and thus
adapts to new unseen tasks efﬁciently.
3.1
Preliminaries
The meaning of “task” is overloaded: “tasks” can
be categorized at different granularity (e.g., text
classiﬁcation vs. QA, yes/no QA vs. machine read-
ing comprehension), and from different aspects
(e.g., domain, label space). Herein we take a gen-
eral formulation by deﬁning a “task” with its train-
ing and testing examples. We deﬁne a task T as
a tuple of (Dtrain, Ddev, Dtest). Each set D is a
set of annotated examples {(xi, yi)} in text-to-text
format. In few-shot setting, the size of Dtrain and
Ddev are required to be small (e.g., 16 example per
class for classiﬁcation tasks).
Existing work mostly focuses on improving
instance-level generalization for individual task by
using task-speciﬁc templates. Performance on in-
dividual tasks is used as the measure of success.
For the CROSSFIT Challenge, we aim to acquire
cross-task generalization and build better general-
purpose few-shot learners, which calls for a differ-
ent problem setting with distinct training procedure
and evaluation protocol.
3.2
Problem Setting
Tasks and Data.
To acquire and evaluate cross-
task generalization, we ﬁrst gather a large reposi-
tory of few-shot tasks T , and partition them into
three non-overlapping sets Ttrain, Tdev, Ttest. In
hopes to examine the capability and limitation of an
approach in different settings, and to answer our re-
search questions, we design multiple task partitions
with different focuses. Details of the repository and
partitions, or as we name them, the NLP Few-shot
Gym, are deferred to §4.
Learning Stages.
A CROSSFIT method may
learn from Ttrain and perform necessary tuning
with Tdev in the upstream learning stage; it is then
evaluated with few-shot tasks in Ttest:
• Upstream learning stage. Here, the algorithm
has access to the Dtrain and Ddev for each train-
ing task in Ttrain, while Dtest is unavailable. The
algorithm also has access to all data in Tdev, but
for validation purpose only (i.e., it is not allowed
to use Tdev to update model weights).
• Few-shot learning stage. In this stage, Ttest be-
came available. Models resulting from the up-
stream learning stage are required to learn from
Dtrain via a particular few-shot learning method
(e.g., direct ﬁne-tuning). The ﬁnal few-shot learn-
ing performance is evaluated on Dtest. 2
Evaluation Metric.
Evaluating the performance
of a model on a diverse collection of NLP tasks is
inherently challenging, as different tasks use dif-
ferent metrics. It is thus not reasonable to simply
aggregate performance of classiﬁcation tasks (e.g.,
accuracy, F1) and generation tasks (e.g., ROUGE,
BLEU) by taking the average.
To address this problem, we ﬁrst narrow down to
a collection of 7 evaluation metrics: classiﬁcation
F1, accuracy, QA F1, exact match (EM), Rogue-
L, Matthew correlation, and Pearson correlation,
which cover all tasks in our experiments. Then, we
deﬁne Average Relative Gain (ARG), a metric that
computes relative performance changes before and
after the upstream learning stage for each test task,
and ﬁnally take the average across all test tasks.
For example,
suppose we have Ttest
=
{TA, TB}. If an upstream learning algorithm helps
improve the few-shot learning performance from
50% F1 score to 70% on task TA (i.e., a 40%
relative improvement), and from 40% accuracy
to 30% on task TB (i.e., −25% relative improve-
ment), the ﬁnal ARG on Ttest would be computed
as 40%+(−25%)
2
= 7.5%.
The ARG metric reﬂects the overall performance
gain on all tasks in Ttest, no matter what speciﬁc
metrics each task uses. We use ARG for a high-
level comparison, and we still analyze the perfor-
mance for each task (e.g., absolute performance
metrics, performance growth with “more shots”,
sensitivity to different selection of Ttrain) in our
in-depth analysis.
2For clariﬁcation, the performance on the Ddev of a task
in Tdev or Ttest will be used for tuning hyper-parameters
during ﬁne-tuning. The overall performance on Tdev is used
for tuning tuning hyper-parameters during upstream learning.
7166
4
NLP Few-shot Gym
Towards learning to generalize across tasks in
CROSSFIT challenge, we need a resource that con-
tains sufﬁcient number of tasks, covering a wide
range of NLP applications, and presented in a uni-
ﬁed text-to-text format. Herein, we introduce the
NLP Few-shot Gym, a repository of 160 few-shot
tasks gathered from existing open-access datasets.
4.1
Dataset Selection
We choose to use Huggingface Datasets3 (Lhoest
et al., 2021) as the pool of our candidate tasks.
We ﬁlter these datasets on a case-by-case basis,
mainly using the following criteria: (1) We focus
on English monolingual datasets. (2) We exclude
datasets that require information retrieval, as they
require a separate retriever. (3) We exclude se-
quence labeling tasks (e.g., dependency parsing,
NER), which are highly dependent on tokenization,
and are hard to evaluate in text-to-text format. (4)
We exclude datasets dealing with extremely long
documents (e.g., a scientiﬁc paper) as input, as
most pre-trained models cannot process such long
input sequences. We ﬁnalize our selection with 160
datasets which are detailed in Appendix A.
4.2
A Uniﬁed Text-to-Text Format
We follow Raffel et al. (2020) and convert all of
our datasets into a uniﬁed text-to-text format. For
example, the task of natural language inference
(originally a sentence-pair classiﬁcation problem)
becomes: premise:
<premise> hypothesis:
<hypothesis>, and the target sequence is either the
word entailment, contradiction or neutral.
As for machine reading comprehension tasks, the
input format is question: <question> context:
<context> and the target sequence is the correct
answer span. We also reference the format for QA
tasks from UniﬁedQA (Khashabi et al., 2020).
4.3
Formulating Few-shot Tasks
We mainly follow the practice in (Gao et al., 2020)
for few-shot sampling. For classiﬁcation and re-
gression tasks, we include 16 training examples
per class in Dtrain. For other types of tasks, we
include 32 examples in Dtrain. In conformity with
real-world situations where labeled data are scarce,
3https://huggingface.co/datasets. It is an extensible library
that provides access to 626 open-access NLP datasets (as of
Feb 25th, 2021) with a uniﬁed, open-source API.
 
Sentiment Analysis 
Amazon_Polarity (McAuley et al. 2013) 
IMDB (Maas et al. 2011)
Poem_Sentiment (Sheng et al. 2020) ...
Paraphrase Identification
Quora Question Paraphrases (Quora)
MRPC (Dolan et al. 2005)
PAWS (Zhang et al. 2019) ...
Natural Language Inference
MNLI (Williams et al. 2018)
QNLI (Rajpurkar et al. 2016)
SciTail (Knot et al. 2018) ...
Others (topic, hate speech, ...)
Reading Comprehension
SQuAD (Rajpurkar et al. 2016)
QuoRef (Dasigi et al. 2019)
TweetQA (Xiong et al. 2019) ...
Multiple-Choice QA 
CommonsenseQA (Talmor et al. 2019)
OpenbookQA (Mihaylov et al. 2018)
AI2_ARC (Clark et al. 2018) ...
Closed-book QA
WebQuestions (Berant et al. 2013)
FreebaseQA (Jiang et al. 2019)
KILT-NQ (Kwiatkowski et al. 2019) ...
Others (yes/no, long-form QA)
Summarization
Gigaword (Napoles et al. 2012)
XSum (Narayan et al. 2018) ...
Dialogue
Empathetic Dialog (Rashkin et al. 2019)
KILT-Wow (Dinan et al. 2019) ...
Others (text2SQL, table2text ...)
Regression
Mocha (Chen et al. 2020) 
Yelp Review Full (Yelp Open Dataset) ...
Others
Acronym Identification 
Sign Language Translation 
Autoregressive Entity Linking 
Motion Recognition 
Pronoun Resolution  ...
Classification
Question Answering
Conditional Generation
Others
Figure 2: Task Ontology for the NLP Few-shot Gym.
Full information is listed in Appendix A.
we assume a development set Ddev which shares
the same size with Dtrain.
We sample Dtrain and Ddev splits from each
dataset’s original train set with 5 different random
seeds. This helps us reduce variance during few-
shot evaluation, and also enlarges the number of
few-shot tasks used for learning. Consequently,
the “effective size” of our NLP Few-shot Gym is
160 × 5 = 800, while we use the number 160
throughout the paper to avoid possible confusion.
We use the original development set for each
dataset as Dtest, or withhold 20% of the dataset
when the ofﬁcial development split is not available.
The held-out test examples are sampled once before
sampling Dtrain and Ddev.
4.4
Task Ontology and Partitions
As mentioned in §3.2, a CROSSFIT method is ex-
pected to ﬁrst acquire cross-task generalization on
a set of Ttrain and evaluate such ability on Ttest. To
comprehensively analyze to what extent a trained
model can generalize, and how its behavior differs
in different scenarios, we need to build different
partitions of (Ttrain, Tdev, Ttest).
Towards this goal, we ﬁrst manually classify
the 160 tasks and form a task ontology with cate-
gories and sub-categories, as shown in Fig. 2. The
ﬁrst-level categories include classiﬁcation, ques-
tion answering, conditional generation, and oth-
7167
ers.4 Further, we design eight different partitions
of (Ttrain, Tdev, Ttest). We illustrate four partitions
in Fig. 3 and provide more details in Table 1.
Our Partition 1 randomly split all 160 few-shot
tasks into the three sets, where |Ttrain| = 120 and
|Tdev| = |Ttest| = 20. The design of Partition 1
mimics the real-world language learning environ-
ment where the goal is to build a general-purpose
few-shot learner, and a set of diverse tasks (Ttrain)
are used to train the learner. Our Partition 2.1-2.3
withhold 10 classiﬁcation tasks for development
and 10 more for testing. The Ttrain is controlled
to have either 100% classiﬁcation tasks, 100%
non-classiﬁcation tasks, or half-and-half. These
three partitions help us to understand the inﬂuence
brought by different task distribution in Ttrain. The
remaining four partitions still focus on crossing
task boundaries, but in a ﬁner granularity: seen
and unseen tasks are in the same category, but not
the same sub-category. For example, Partition 3.1
has 57 non-NLI classiﬁcation tasks as Ttrain, and
8 NLI tasks as Ttest. These partitions help us to un-
derstand whether cross-task generalization in this
ﬁner granularity is easier for model to acquire.
5
Methods to CROSSFIT
We mainly use BART-Base (Lewis et al., 2020) as
the text-to-text transformer for our analysis in the
CROSSFIT setup. We leave conﬁrmatory experi-
ments with T5-v1.1-Base and BART-Large model
in Appendix C.
Direct Fine-tuning on Test Tasks.
This serves
as the basic baseline method for the CROSSFIT
challenge, which does not make use of Ttrain or
Tdev, or go through the upstream learning stage.
For each task T ∈Ttest, we directly ﬁne-tune the
text-to-text model with its Dtrain, tune the hyper-
parameters with Ddev, and assess its performance
with the test set Dtest. We use the performance of
direct ﬁne-tuning as the base for computing ARG
scores of other CROSSFIT approaches. We expect
a model trained with upstream learning would cap-
ture cross-task generalization ability and thus have
better ARG scores.
Multi-task
Learning
(MTL).
A
straight-
forward yet effective method is to combine the
data5 in the training tasks to learn a multi-task
4We later discuss the limitation of this design in §6-Q2
5Both Dtrain and Ddev are used, as Ddev is used for gra-
dient updates in meta-learning algorithm. We do so to make
sure that the data access for the two methods is fair.
Classification
Question 
Answreing
Conditional 
Generation
Others
Classification
Question 
Answreing
Conditional 
Generation
Others
Classification
Question 
Answreing
Conditional 
Generation
Others
Training Task
Dev Task 
Test Task
Unused Task
(a) Random Split
(b) 45non-class
(c) Held-out-NLI
Classification
Question 
Answreing
Conditional 
Generation
Others
(d) Held-out-MRC
Figure 3: Illustration for different task partitions.
We evaluate a CROSSFIT approach on different task
partitions to examine its generalization ability in dif-
ferent scenarios. Full details in Table 1. The locations
and distances in this ﬁgure are hypothetical and for il-
lustrative purposes only.
model, before ﬁne-tuning it on each test task.
Speciﬁcally, we gather source-target examples for
all tasks in Ttrain and ﬁne-tune the text-to-text
model with these examples.
Then we use the
resulting checkpoint as initialization and perform
the same procedure in “direct ﬁne-tuning” for each
test task in Ttest. The performance gain over the
direct ﬁne-tuning is used for computing its overall
ARG score.
Model-Agnostic
Meta-learning
(MAML).
Cross-task generalization ability, closely aligns
with the concept of learning to learn. Hence, we
use MAML (Finn et al., 2017), a representative
meta-learning approach during upstream learning.
The core concept of MAML is to learn a set of
initialization weight, from which the model adapts
fast to a new task within few gradient updates.
In MAML training, we iterate through tasks in
Ttrain to update the model. For each train task
(Dtrain, Ddev), we ﬁrst sample a support batch
Bsupport from Dtrain and a query batch Bquery
from Ddev. We use fθ to denote the text-to-text
model with parameters θ.
Using Bsupport, we
ﬁrst compute the updated parameters θ′ with
gradient descent (i.e., the inner loop). Due to the
large size of pre-trained text-to-text models, we
7168
No.
Shorthand
Ttrain
Tdev
Ttest
ARG(Multi)
ARG(MAML)
ARG(FoMAML)
ARG(Rept.)
Details
1
Random
120
20
20
35.06%
28.50%
22.69%
25.90%
Fig. 4(a)
2.1
45cls
45 cls.
10 cls.
10 cls.
11.68%
9.37%
10.28%
13.36%
Fig. 5
2.2
23cls+22non-cls
23 cls. + 22 non-cls.
10 cls.
10 cls.
11.82%
9.69%
13.75%
14.34%
2.3
45non-cls
45 non-cls.
10 cls.
10 cls.
11.91%
9.33%
11.20%
14.14%
3.1
Held-out-NLI
57 non-NLI cls.
/
8 NLI
16.94%
12.30%
12.33%
14.46%
Fig. 4(b)
3.2
Held-out-Para
61 non-Paraphrase cls.
/
4 Para. Iden.
18.21%
17.90%
21.57%
19.72%
Fig. 4(c)
4.1
Held-out-MRC
42 non-MRC QA
/
9 MRC
32.81%
27.28%
28.85%
28.85%
Fig. 4(d)
4.2
Held-out-MCQA
29 non-MC QA
/
22 MC QA
12.20%
4.69%
6.73%
7.67%
Fig. 4(e)
Table 1: (Ttrain,Tdev,Ttest) partitions used in the study (full lists in Appendix B), and their ARG scores when
upstream learning methods are applied. “cls.” stands for “classiﬁcation”, “Para. Iden.” for “paraphrase identiﬁca-
tion”, “MRC” for “machine reading comprehension” and “MCQA” for “multiple-choice QA”.
use one gradient update in the inner loop, i.e.,
θ′ = θ −α∇θL(fθ, Bsupport). Then we apply the
updated text-to-text model fθ′ to Bquery, and do
one step of meta-optimization (i.e., the outer loop),
with θ ←θ −β∇θL(fθ′, Bquery).
First-order MAML.
First-order MAML (Finn
et al., 2017) avoids second-order optimization and
improves training stability using the ﬁrst-order
approximation by differentiating with respect to
the fast weights θ′ instead of the original parame-
ters θ for the gradient ∇θL(fθ′, Bquery), i.e., θ ←
θ −β∇θ′L(fθ′, Bquery).
Reptile.
Reptile (Nichol et al., 2018) is another
memory-efﬁcient, ﬁrst-order meta-learning algo-
rithm that ﬁrst makes multiple gradient updates in
the inner loop, then directly uses θ′ −θ to approxi-
mate ∇θL(fθ′, Bquery), i.e., θ ←θ + β(θ′ −θ).
6
Empirical Analysis
In this section we look to interpret the results and
answer our research questions. We summarize the
ARG scores in Table 1 and plot the performance of
each test task (for each partition) in Fig. 4-5.
Q1. Can we teach pre-trained LMs to gener-
alize across tasks with existing methods?
Overall Performance.
From Table 1, we ob-
serve that, on average, the tested upstream learning
methods indeed improve cross-task generalization:
their ARG scores are positive, meaning that they
are better than direct ﬁne-tuning (ARG=0%). Fur-
ther, by aggregating results from all upstream learn-
ing methods and task partitions, we ﬁnd that the
performance on 51.47% test tasks are signiﬁcantly
improved (> 5% relative improvement compared
to direct ﬁne-tuning); 35.93% tasks are relatively
unaffected (between ±5%); and 12.60% tasks suf-
fer from worse performance (< −5%).
Correlated Performance Gains.
The perfor-
mance gain obtained with different upstream learn-
ing methods are correlated with each other – i.e.,
tasks that beneﬁt from multi-task learning is likely
to also beneﬁt from meta-learning. For the Ran-
dom partition, the Spearman Correlation between
the relative improvement brought by MTL and
MAML is 0.66, with p value equals to 0.0015. This
suggests that different upstream learning methods,
while taking different optimization objectives, cap-
ture similar inductive bias from Ttrain.
MTL is a strong baseline.
Surprisingly, the
most straight-forward multi-task learning method
is hard to beat. This could be counter-intuitive, as
meta-learning methods are speciﬁcally designed
for rapid generalization to unseen tasks, sharing
the same goal with our CROSSFIT challenge. We
think there are three possible reasons: (1) Due to
memory constraints, we limit the number of inner-
loop updates to be one, which may be insufﬁcient.
Also, meta-learning methods are highly sensitive
to hyper-parameters and even random seeds (An-
toniou et al., 2019), which we do not tune exhaus-
tively for practical reasons. (2) Text-to-text trans-
formers have much more complex architectures,
while most meta-learning methods are typically
applied to small feed-forward/convolutional net-
works. (3) The CROSSFIT challenge has a highly
diverse set upstream tasks, which may introduce
under-explored difﬁculties. That being said, we
believe it is important to identify the true cause,
and to develop improved meta-learning methods
for the CROSSFIT challenge as future work.
Forgetting Pre-Trained Knowledge.
A few test
tasks have negative performance gain after up-
stream learning, including Glue-COLA (measuring
linguistic acceptability) and Domain Crawl (sepa-
rating domain names into tokens) in the Random
7169
glue­cola
crawl_domain
ag_news
ai2_arc
wiki_split
amazon_polarity
blimp­...licensor_present
tweet_eval­irony
ethos­disability
sglue­rte
circa
ethos­sexual_orient.
hatexplain
race­high
glue­qnli
quoref
blimp­..._npi_scope
break­QDMR
yelp_polarity
average
­25%
0%
25%
50%
75%
100%
Relative Performance Gain (%)
(a) Random
direct fine­tuning
multi­task learning
maml
first­order maml
reptile
anli
glue­wnli
glue­qnli
sick
glue­rte
sglue­cb
scitail
glue­mnli
average
­10%
0%
10%
20%
30%
40%
50%
Relative Performance Gain (%)
(b) Held­Out­NLI
glue­mrpc
medical_q_pairs
glue­qqp
paws
average
­10%
0%
10%
20%
30%
40%
50%
(c) Held­Out­Para
biomrc
quoref
sglue­record
tweet_qa
squad­w_context
ropes
adversarialqa
duorc
hotpot_qa
average
­20%
0%
20%
40%
60%
80%
(d) Held­Out­MRC
wiqa
codah
social_i_qa
quartz­wo_knowledge
wino_grande
quartz­w_knowledge
sglue­copa
hellaswag
ai2_arc
quarel
race­high
sciq
swag
math_qa
dream
openbookqa
quail
race­middle
aqua_rat
cosmos_qa
qasc
commonsense_qa
average
­30%
­20%
­10%
0%
10%
20%
30%
40%
50%
60%
Relative Performance Gain (%)
(e) Held­Out­Multiple­Choice
Figure 4: Experimental results for the CROSSFIT challenge with different task partitions. The details of each
partition is shown in Table 1. Relative performance gain is computed based on the results of direct ﬁne-tuning.
Best viewed in color. Green color is used to highlight the Average Relative Gain (ARG) for each method.
Partition setting. For Glue-COLA, similar observa-
tions are reported by Pruksachatkun et al. (2020)
in an intermediate-task transfer learning setting,
where the authors conjecture catastrophic forget-
ting of the masked language modeling (MLM)
tasks may be the cause. BART uses denoising pre-
training objective, a variant of MLM. Intuitively,
Domain Crawl is also one of the most similar tasks
to denoising in all test tasks, which further sup-
ports this hypothesis. We thus conjecture that for
test tasks that resemble pre-training objectives, up-
stream learning could hurt performance due to the
catastrophic forgetting phenomena.
Understanding negative transfer (Wu et al., 2020)
and selecting source tasks to avoid negative transfer
(Vu et al., 2020) are also growing research topics.
In this work we refrain from further investigation;
however we believe combating negative transfer
and thus improving CROSSFIT performance is a
promising future direction.
Q2. Well-rounded or specialized? Which is
a better strategy of upstream learning?
“Learning to be well-rounded vs. learning to
be specialized” is a common dilemma that human
learners struggles with. For the CROSSFIT chal-
lenge, the former refers to learning from a set of
diverse tasks in upstream learning; the latter refers
to learning from a set of tasks closer to target few-
shot tasks. To study this research question, we want
to ﬁnd out which option works better in upstream
learning. Put differently, we aim to analyze the
inﬂuence of upstream task selection for a ﬁxed
set of the downstream tasks.
Setup.
We ﬁrst conduct controlled experiments
with Partition 2.1-2.3, where Ttest is a ﬁxed set
of classiﬁcation tasks, and Ttrain varies. In Par-
tition 2.1, all tasks in Ttrain are classiﬁcation
tasks (i.e., “specialized and targeted”); in Partition
7170
emo
anli
ethos­race
financial_phrasebank
dbpedia_14
wiki_qa
ethos­religion
superglue­cb
tab_fact
yelp_polarity
average
­25%
0%
25%
50%
75%
100%
Relative Performance Gain (%)
11.68%
11.82%
11.91%
(a) Multi­task Learning
45 classification tasks
23 classification + 22 non­classification tasks
45 non­classification tasks
ethos­race
anli
emo
wiki_qa
dbpedia_14
financial_phrasebank
ethos­religion
tab_fact
superglue­cb
yelp_polarity
average
­25%
0%
25%
50%
75%
100%
Relative Performance Gain (%)
9.37%
9.69%
9.33%
(b) Meta­Learning
Figure 5: Comparison for the controlled experiment on
Partition 2.1-2.3. Ttest is a ﬁxed set of 10 classiﬁcation
tasks, while Ttrain varies.
2.2, half of the tasks are classiﬁcation tasks (i.e.,
“well-rounded”); in Partition 2.3, all tasks are non-
classiﬁcation tasks (i.e., “specialized in an opposite
direction”, for a controlled experiment).
Analysis and Discussion.
It is surprising at ﬁrst
that non-classiﬁcation tasks and classiﬁcation tasks
are equivalently helpful in terms of ARG scores
(see Fig. 5). On a second thought, this observation
is encouraging as it demonstrates that acquiring
cross-task generalization is feasible and promising,
even when Ttrain and Ttest are drastically differ-
ent. It also suggests that our categorization of tasks
(§4.4) may not align with how models learn trans-
ferable skills: selecting Ttrain tasks that have the
same format and goal as the test task may not lead
to optimal transfer.
In retrospect, we acknowledge that our design of
ontology and partitions based on task format and
goal is ﬂawed. This is merely one aspect of “task
similarity”. However, understanding the complex
relationship between tasks is another challenging
and under-explored problem. We consider our on-
tology as a starting point, rather than a ﬁxed ﬁnal
one. We use the current ontology to guide our ex-
periment and analysis, and we hope future analysis
could help build a more informative ontology.
Case Studies.
We further look at cases where a
test task appear in Ttest of multiple partitions. For
example, AI2_ARC and Race-High are in the Ttest
of both Random partition and Held-out-MCQA
partition. We present the results in Table 2. In
general, the performance of these tasks varies when
Test Task
Partition
∆multi
∆meta
Glue-QNLI
Random
15.89%
11.55%
Held-Out-NLI
10.88%
10.94%
AI2_ARC
Random
1.30%
4.22%
Held-Out-MCQA
6.49%
−6.22%
Race-High
Random
26.71%
6.59%
Held-Out-MCQA
7.27%
−6.28%
QuoRef
Random
25.47%
3.99%
Held-Out-MRC
12.25%
4.64%
Table 2: Performance comparison of test task perfor-
mance when different Ttrain sets are used in upstream
learning. See text in Q2 for in-depth analysis.
glue­mrpc
medical_qpairs
glue­qqp
paws
average
­10%
0%
10%
20%
30%
40%
50%
Relative Performance Gain (%)
1x
2x
4x
8x
Figure 6: Controlling upstream learning data size in
with Held-out-Para Partition. Enlarging the size of data
during upstream learning does not necessitate better
cross-task generalization ability.
different Ttrain sets are used. However, we have
not found consistent patterns of what type of Ttrain
lead to better performance for a speciﬁc test task.
Q3. Does it help if we have more labelled
data for upstream tasks?
As described in §4.3, we limit our upstream tasks
to be also few-shot: classiﬁcation tasks have 16 ex-
amples per class, and non-classiﬁcation tasks have
32 examples. This decision is empirically deter-
mined following prior works (Schick and Schütze,
2020a,b; Gao et al., 2020) and makes our exten-
sive analysis practical and efﬁcient. It is possible
that using more data for each upstream task can
signiﬁcantly improve cross-task generalization. To
investigate this, we conduct a set of controlled ex-
periments where the number of examples in up-
stream tasks are changed to [2, 4, 8] times of the
original size. We use the Held-out-Para Partition
and multi-task learning for the experiments, and
present the result in Fig. 6. Surprisingly, we ﬁnd
that the effect from using more upstream data is
inconsistent on different target tasks. The overall
ARG for all sizes are close: even 8x larger up-
7171
stream data leads to only 4% improvement in ARG.
We conclude that enlarging the size of data during
upstream learning does not necessitate better cross-
task generalization ability. This also justiﬁes our
decision to keep upstream tasks few-shot.
Q4-Q6. Additional Analysis
Due to space limit, we summarize our other ﬁnd-
ings below and defer the details to Appendix C.
Few-Shot →More-Shot (Q4).
In practice, users
may continue to collect data over time. We wonder
if cross-task generalization ability is still helpful
for medium/high-resource target tasks. We ﬁnd
that the performance gain from upstream learning
is still evident when 1024 shots are available. The
performance gap diminishes with millions of train-
ing examples.
Using Different Base Models (Q5).
We extend
our analysis on BART-base (139M) to larger pre-
trained text-to-text Transformers: BART-Large
(406M) and T5-v1.1-Base (248M). Generally, the
performance grows with models sizes with only
few exceptions, which suggests that upstream learn-
ing methods we use are model-agnostic, and can
be applied to larger models to further improve few-
shot performance.
Integration with PET Training (Q6).
Pattern-
exploiting training (PET) (Schick and Schütze,
2020a,b) was originally proposed for classiﬁcation
tasks and encoder language models. We test a few
variants of PET training with BART-Base and try
applying PET training after upstream learning. In
general we observe deteriorated performance com-
pared to direct ﬁne-tuning. We hypothesize that
PET methods are not directly applicable to encoder-
decoder language models used in our study.
7
Conclusion and Future Work
In this paper, we study the problem of building
better few-shot learners via acquiring cross-task
generalization ability from diverse NLP tasks. To-
wards our goal, we introduce the CROSSFIT Chal-
lenge, an task setup that standardizes the training
pipeline, data access and evaluation protocol. We
also present the NLP Few-shot Gym, a reposi-
tory of 160 diverse few-shot NLP tasks, to sup-
port CROSSFIT learning in different scenarios. We
empirically demonstrated that cross-task general-
ization can be acquired via multi-task learning and
meta-learning; conﬁrmed that the selection of seen
tasks would inﬂuence the few-shot performance on
unseen tasks.
We have highlighted several unexpected or un-
desired observations in our analysis, for which
we invite future work in understanding and com-
bating related issues.
In addition, we envision
the CROSSFIT Challenge and the NLP Few-shot
Gym to serve as the testbed for many interesting
“meta-problems”, such as (1) learning to generate
prompt for diverse task formats and further improve
learning efﬁciency (Shin et al., 2020; Gao et al.,
2020); (2) learning to select appropriate source
tasks to learn from during upstream learning (Za-
mir et al., 2018; Standley et al., 2020), potentially
with task2vec methods (Achille et al., 2019; Vu
et al., 2020); (3) applying task augmentation strate-
gies to prevent over-ﬁtting (Murty et al., 2021); (4)
learning to accumulate knowledge and avoid catas-
trophic forgetting in an continual learning setup
(Jin et al., 2021); (5) decomposing complex tasks
into atomic tasks and exploring cross-task general-
ization through the lens of compositionality (An-
dreas et al., 2016; Khot et al., 2021).
Acknowledgments
We thank authors and crowd-workers of all datasets
used in our study. We thank huggingface datasets
team for making datasets more accessible. We
thank anonymous reviewers and members of USC
INK Lab for their valuable feedback.
This
work is supported in part by the Ofﬁce of the
Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity
(IARPA), via Contract No. 2019-19051600007;
the DARPA MCS program under Contract No.
N660011924033; the Defense Advanced Research
Projects Agency with award W911NF-19-20271;
NSF IIS 2048211.
References
A. Achille, Michael Lam, Rahul Tewari, A. Ravichan-
dran, Subhransu Maji, Charless C. Fowlkes, Stefano
Soatto, and P. Perona. 2019. Task2vec: Task em-
bedding for meta-learning. 2019 IEEE/CVF Inter-
national Conference on Computer Vision (ICCV),
pages 6429–6438.
Tiago A. Almeida, José María G. Hidalgo, and Akebo
Yamakami. 2011. Contributions to the study of sms
spam ﬁltering: New collection and results. In Pro-
ceedings of the 11th ACM Symposium on Document
7172
Engineering, DocEng ’11, page 259–262, New York,
NY, USA. Association for Computing Machinery.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019.
MathQA: Towards interpretable
math word problem solving with operation-based
formalisms. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 2357–2367, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016. Learning to compose neural net-
works for question answering.
In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1545–
1554, San Diego, California. Association for Com-
putational Linguistics.
Antreas Antoniou,
Harrison Edwards,
and Amos
Storkey. 2019. How to train your MAML. In Inter-
national Conference on Learning Representations.
Trapit Bansal, Rishikesh Jha, and Andrew McCallum.
2020a.
Learning to few-shot learn across diverse
natural language classiﬁcation tasks.
In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics, pages 5108–5123, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.
Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,
and Andrew McCallum. 2020b.
Self-supervised
meta-learning for few-shot natural language classiﬁ-
cation tasks. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 522–534, Online. Association
for Computational Linguistics.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Proceedings of the sec-
ond PASCAL challenges workshop on recognising
textual entailment, volume 6, pages 6–4. Venice.
Francesco Barbieri, Jose Camacho-Collados, Luis Es-
pinosa Anke, and Leonardo Neves. 2020. TweetE-
val: Uniﬁed benchmark and comparative evaluation
for tweet classiﬁcation. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2020,
pages 1644–1650, Online. Association for Computa-
tional Linguistics.
Max Bartolo, Alastair Roberts, Johannes Welbl, Sebas-
tian Riedel, and Pontus Stenetorp. 2020. Beat the
AI: Investigating adversarial human annotation for
reading comprehension. Transactions of the Associ-
ation for Computational Linguistics, 8:662–678.
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo
Giampiccolo. 2009. The ﬁfth pascal recognizing tex-
tual entailment challenge. In TAC.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013.
Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA. Association for Computational Lin-
guistics.
Chandra Bhagavatula, Ronan Le Bras, Chaitanya
Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-
nah Rashkin, Doug Downey, Wen tau Yih, and Yejin
Choi. 2020. Abductive commonsense reasoning. In
International Conference on Learning Representa-
tions.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-
feng Gao, and Yejin Choi. 2020. Piqa: Reasoning
about physical commonsense in natural language. In
Thirty-Fourth AAAI Conference on Artiﬁcial Intelli-
gence.
Michael Boratko, Xiang Li, Tim O’Gorman, Rajarshi
Das, Dan Le, and Andrew McCallum. 2020. Pro-
toQA: A question answering dataset for prototypi-
cal common-sense reasoning. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1122–1136,
Online. Association for Computational Linguistics.
Jan A. Botha, Manaal Faruqui, John Alex, Jason
Baldridge, and Dipanjan Das. 2018.
Learning to
split and rephrase from Wikipedia edit history. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
732–737, Brussels, Belgium. Association for Com-
putational Linguistics.
Jonathan Bragg, Arman Cohan, Kyle Lo, and Iz Belt-
agy. 2021. Flex: Unifying evaluation for few-shot
nlp.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.
Ankush Chatterjee, Kedhar Nath Narahari, Meghana
Joshi, and Puneet Agrawal. 2019.
SemEval-2019
task 3: EmoContext contextual emotion detection in
text. In Proceedings of the 13th International Work-
shop on Semantic Evaluation, pages 39–48, Min-
neapolis, Minnesota, USA. Association for Compu-
tational Linguistics.
Anthony Chen, Gabriel Stanovsky, Sameer Singh, and
Matt Gardner. 2020a. MOCHA: A dataset for train-
ing and evaluating generative reading comprehen-
sion metrics. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 6521–6532, Online. Associa-
tion for Computational Linguistics.
7173
Michael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-
nandez, and Doug Downey. 2019.
CODAH: An
adversarially-authored question answering dataset
for common sense. In Proceedings of the 3rd Work-
shop on Evaluating Vector Space Representations
for NLP, pages 63–69, Minneapolis, USA. Associ-
ation for Computational Linguistics.
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020b. Tabfact: A large-scale
dataset for table-based fact veriﬁcation. In Interna-
tional Conference on Learning Representations.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. BoolQ: Exploring the surprising
difﬁculty of natural yes/no questions. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 2924–2936, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. ArXiv,
abs/1803.05457.
Arman Cohan, Waleed Ammar, Madeleine van Zuylen,
and Field Cady. 2019.
Structural scaffolds for ci-
tation intent classiﬁcation in scientiﬁc publications.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers), pages
3586–3596, Minneapolis, Minnesota. Association
for Computational Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005.
The pascal recognising textual entailment
challenge. In Machine Learning Challenges Work-
shop, pages 177–190. Springer.
Pradeep Dasigi,
Nelson F. Liu,
Ana Marasovi´c,
Noah A. Smith, and Matt Gardner. 2019. Quoref:
A reading comprehension dataset with questions re-
quiring coreferential reasoning. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 5925–5932, Hong Kong,
China. Association for Computational Linguistics.
Thomas Davidson, Dana Warmsley, Michael Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language. In
Proceedings of the 11th International AAAI Confer-
ence on Web and Social Media, ICWSM ’17, pages
512–515.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing.
In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
T. Diggelmann, Jordan L. Boyd-Graber, Jannis Bu-
lian, Massimiliano Ciaramita, and Markus Leippold.
2020. Climate-fever: A dataset for veriﬁcation of
real-world climate claims. ArXiv, abs/2012.00614.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2019. Wizard
of wikipedia: Knowledge-powered conversational
agents.
In International Conference on Learning
Representations.
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).
Zi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos.
2019.
Investigating meta-learning algorithms for
low-resource natural language understanding tasks.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 1192–
1197, Hong Kong, China. Association for Computa-
tional Linguistics.
Matthew Dunn, Levent Sagun, Mike Higgins, V. U.
Güney, Volkan Cirik, and Kyunghyun Cho. 2017.
Searchqa: A new q&a dataset augmented with con-
text from a search engine. ArXiv, abs/1704.05179.
Ondˇrej Dušek, David M. Howcroft, and Verena Rieser.
2019. Semantic noise matters for neural natural lan-
guage generation. In Proc. of the 12th International
Conference on Natural Language Generation, pages
421–426, Tokyo, Japan. Association for Computa-
tional Linguistics.
Ondˇrej Dušek, Jekaterina Novikova, and Verena Rieser.
2020. Evaluating the State-of-the-Art of End-to-End
Natural Language Generation: The E2E NLG Chal-
lenge. Computer Speech & Language, 59:123–156.
Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci,
Christophe Gravier,
Jonathon Hare,
Frederique
Laforest, and Elena Simperl. 2018. T-REx: A large
scale alignment of natural language with knowledge
base triples. In Proceedings of the Eleventh Inter-
national Conference on Language Resources and
Evaluation (LREC-2018), Miyazaki, Japan. Euro-
pean Languages Resources Association (ELRA).
Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and
Dragomir Radev. 2019. Multi-news: A large-scale
multi-document summarization dataset and abstrac-
tive hierarchical model. In Proceedings of the 57th
7174
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1074–1084, Florence, Italy.
Association for Computational Linguistics.
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
Long form question answering. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 3558–3567, Florence,
Italy. Association for Computational Linguistics.
Manaal Faruqui and Dipanjan Das. 2018. Identifying
well-formed natural language questions. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 798–803,
Brussels, Belgium. Association for Computational
Linguistics.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks.
In Proceedings of the 34th In-
ternational Conference on Machine Learning, vol-
ume 70 of Proceedings of Machine Learning Re-
search, pages 1126–1135. PMLR.
Tianyu Gao, A. Fisch, and Danqi Chen. 2020. Making
pre-trained language models better few-shot learn-
ers. ArXiv, abs/2012.15723.
Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng
Li, Maosong Sun, and Jie Zhou. 2019. FewRel 2.0:
Towards more challenging few-shot relation classiﬁ-
cation. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
6250–6255, Hong Kong, China. Association for
Computational Linguistics.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recognizing
textual entailment challenge. In Proceedings of the
ACL-PASCAL workshop on textual entailment and
paraphrasing, pages 1–9. Association for Computa-
tional Linguistics.
Ona de Gibert, Naiara Perez, Aitor García-Pablos, and
Montse Cuadros. 2018. Hate Speech Dataset from
a White Supremacy Forum. In Proceedings of the
2nd Workshop on Abusive Language Online (ALW2),
pages 11–20, Brussels, Belgium. Association for
Computational Linguistics.
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and
Aleksander Wawer. 2019.
SAMSum corpus:
A
human-annotated dialogue dataset for abstractive
summarization. In Proceedings of the 2nd Workshop
on New Frontiers in Summarization, pages 70–79,
Hong Kong, China. Association for Computational
Linguistics.
Andrew Gordon, Zornitsa Kozareva, and Melissa
Roemmele. 2012.
SemEval-2012 task 7: Choice
of plausible alternatives: An evaluation of common-
sense causal reasoning. In *SEM 2012: The First
Joint Conference on Lexical and Computational Se-
mantics – Volume 1: Proceedings of the main con-
ference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 394–398,
Montréal, Canada. Association for Computational
Linguistics.
Jiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li,
and Kyunghyun Cho. 2018. Meta-learning for low-
resource neural machine translation.
In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 3622–3631,
Brussels, Belgium. Association for Computational
Linguistics.
Harsha Gurulingappa, Abdul Mateen Rajput, Angus
Roberts, Juliane Fluck, Martin Hofmann-Apitius,
and Luca Toldo. 2012. Development of a benchmark
corpus to support the automatic extraction of drug-
related adverse effects from medical case reports.
Journal of Biomedical Informatics, 45(5):885–892.
Text Mining and Natural Language Processing in
Pharmacogenomics.
Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan
Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel:
A large-scale supervised few-shot relation classiﬁca-
tion dataset with state-of-the-art evaluation. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 4803–
4809, Brussels, Belgium. Association for Computa-
tional Linguistics.
Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.
Question-answer driven semantic role labeling: Us-
ing natural language to annotate natural language.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
643–653, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named en-
tities in text. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 782–792, Edinburgh, Scotland, UK. Asso-
ciation for Computational Linguistics.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-
Yew Lin, and Deepak Ravichandran. 2001. Toward
semantics-based answer pinpointing.
In Proceed-
ings of the First International Conference on Human
Language Technology Research.
Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and
Yejin Choi. 2019.
Cosmos QA: Machine reading
comprehension with contextual commonsense rea-
soning. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
2391–2401, Hong Kong, China. Association for
Computational Linguistics.
7175
Chao Jiang, Mounica Maddela, Wuwei Lan, Yang
Zhong, and Wei Xu. 2020. Neural CRF model for
sentence alignment in text simpliﬁcation.
In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 7943–
7960, Online. Association for Computational Lin-
guistics.
Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-
baseQA: A new factoid QA data set matching trivia-
style question-answer pairs with Freebase. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers), pages 318–323,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Xisen Jin, Mohammad Rostami, and Xiang Ren. 2021.
Lifelong learning of few-shot learners across nlp
tasks. ArXiv, abs/2104.08808.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018. Looking be-
yond the surface: A challenge set for reading com-
prehension over multiple sentences. In Proceedings
of the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 252–262, New Orleans, Louisiana. As-
sociation for Computational Linguistics.
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-
mat boundaries with a single QA system. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2020, pages 1896–1907, Online. As-
sociation for Computational Linguistics.
Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2020.
Qasc:
A
dataset for question answering via sentence compo-
sition. Proceedings of the AAAI Conference on Arti-
ﬁcial Intelligence, 34(05):8082–8090.
Tushar Khot, Daniel Khashabi, Kyle Richardson, Pe-
ter Clark, and Ashish Sabharwal. 2021. Text mod-
ular networks: Learning to decompose tasks in the
language of existing models. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1264–1279, On-
line. Association for Computational Linguistics.
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
SciTail: A textual entailment dataset from science
question answering. In AAAI.
Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim.
2019.
Abstractive summarization of Reddit posts
with multi-level memory networks.
In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 2519–2531, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Neema Kotonya and Francesca Toni. 2020.
Ex-
plainable automated fact-checking for public health
claims. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 7740–7754, Online. Associa-
tion for Computational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: A benchmark for question an-
swering research. Transactions of the Association
for Computational Linguistics, 7:453–466.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. RACE: Large-scale ReAd-
ing comprehension dataset from examinations. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
785–794, Copenhagen, Denmark. Association for
Computational Linguistics.
Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with
application to the biography domain.
In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1203–1213,
Austin, Texas. Association for Computational Lin-
guistics.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
D. Kontokostas, Pablo N. Mendes, Sebastian Hell-
mann, M. Morsey, Patrick van Kleef, S. Auer, and
C. Bizer. 2015. Dbpedia - a large-scale, multilingual
knowledge base extracted from wikipedia. Semantic
Web, 6:167–195.
Hector J. Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Proceedings of the Thirteenth International Confer-
ence on Principles of Knowledge Representation
and Reasoning, KR’12, page 552–561. AAAI Press.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In Proceedings of the 21st
Conference on Computational Natural Language
Learning (CoNLL 2017), pages 333–342, Vancou-
ver, Canada. Association for Computational Linguis-
tics.
Mike
Lewis,
Yinhan
Liu,
Naman
Goyal,
Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
7176
Linguistics, pages 7871–7880, Online. Association
for Computational Linguistics.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, A. Thakur, Patrick von Platen, Suraj Patil,
Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario vSavsko, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Clement Delangue, Th’eo Ma-
tussiere, Lysandre Debut, Stas Bekman, Pierric Cis-
tac, Thibault Goehringer, Victor Mustar, Franccois
Lagunas, Alexander M. Rush, and Thomas Wolf.
2021.
Datasets: A community library for natural
language processing.
Xin Li and Dan Roth. 2002. Learning question clas-
siﬁers. In COLING 2002: The 19th International
Conference on Computational Linguistics.
Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and
Xiang Ren. 2020a.
Birds have four legs?!
NumerSense:
Probing Numerical Commonsense
Knowledge of Pre-Trained Language Models.
In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 6862–6868, Online. Association for Computa-
tional Linguistics.
Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei
Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang
Ren. 2020b. CommonGen: A constrained text gen-
eration challenge for generative commonsense rea-
soning. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020, pages 1823–1840,
Online. Association for Computational Linguistics.
Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-
ner. 2019. Reasoning over paragraph effects in situ-
ations. In Proceedings of the 2nd Workshop on Ma-
chine Reading for Question Answering, pages 58–
62, Hong Kong, China. Association for Computa-
tional Linguistics.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 158–167, Vancou-
ver, Canada. Association for Computational Linguis-
tics.
Tal Linzen. 2020. How can we accelerate progress to-
wards human-like linguistic generalization? In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5210–
5217, Online. Association for Computational Lin-
guistics.
Annie Louis, Dan Roth, and Filip Radlinski. 2020. “I’d
rather just go to bed”: Understanding indirect an-
swers. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 7411–7425, Online. Associa-
tion for Computational Linguistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA. Association for Computational
Linguistics.
Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-
lenius, and Pyry Takala. 2014. Good debt or bad
debt: Detecting semantic orientations in economic
texts. J. Assoc. Inf. Sci. Technol., 65(4):782–796.
Irene Manotas, Ngoc Phuoc An Vo, and Vadim Sheinin.
2020. LiMiT: The literal motion in text dataset. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020, pages 991–1000, Online.
Association for Computational Linguistics.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014. A SICK cure for the evaluation of com-
positional distributional semantic models.
In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC-2014),
pages 216–223, Reykjavik, Iceland. European Lan-
guages Resources Association (ELRA).
Marie-Catherine de Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
Proceedings of Sinn und Bedeutung, 23(2):107–124.
Binny Mathew,
Punyajoy Saha,
Seid Muhie Yi-
mam, Chris Biemann, Pawan Goyal, and Ani-
mesh Mukherjee. 2020. Hatexplain: A benchmark
dataset for explainable hate speech detection. arXiv
preprint arXiv:2012.10289.
Julian McAuley and J. Leskovec. 2013. Hidden factors
and hidden topics: understanding rating dimensions
with review text. Proceedings of the 7th ACM con-
ference on Recommender systems.
Bryan McCann, N. Keskar, Caiming Xiong, and
R. Socher. 2018. The natural language decathlon:
Multitask learning as question answering.
ArXiv,
abs/1806.08730.
Clara H. McCreery, Namit Katariya, Anitha Kannan,
Manish Chablani, and Xavier Amatriain. 2020. Ef-
fective transfer learning for identifying similar ques-
tions: Matching user questions to covid-19 faqs.
In Proceedings of the 26th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data
Mining, KDD ’20, page 3458–3465, New York, NY,
USA. Association for Computing Machinery.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question an-
swering. In Proceedings of the 2018 Conference on
7177
Empirical Methods in Natural Language Processing,
pages 2381–2391, Brussels, Belgium. Association
for Computational Linguistics.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2021. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
arXiv preprint arXiv:2104.08773.
Ioannis Mollas, Zoe Chrysopoulou, Stamatis Kar-
los,
and Grigorios Tsoumakas. 2020.
Ethos:
an online hate speech detection dataset.
ArXiv,
abs/2006.08328.
Shikhar Murty, T. Hashimoto, and Christopher D. Man-
ning. 2021.
Dreca: A general task augmentation
strategy for few-shot natural language inference.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020.
CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1953–1967, Online. As-
sociation for Computational Linguistics.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012.
Annotated Gigaword.
In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction (AKBC-WEKEX), pages 95–100, Mon-
tréal, Canada. Association for Computational Lin-
guistics.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1797–1807, Brussels, Bel-
gium. Association for Computational Linguistics.
Alex Nichol, Joshua Achiam, and John Schulman.
2018.
On ﬁrst-order meta-learning algorithms.
ArXiv, abs/1803.02999.
Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-
versarial NLI: A new benchmark for natural lan-
guage understanding. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 4885–4901, Online. Association
for Computational Linguistics.
Farhad Nooralahzadeh, Giannis Bekoulis, Johannes
Bjerva, and Isabelle Augenstein. 2020.
Zero-shot
cross-lingual transfer with meta learning.
In Pro-
ceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 4547–4562, Online. Association for Compu-
tational Linguistics.
A. Othman and M. Jemni. 2012. English-asl gloss par-
allel corpus 2012: Aslg-pc12.
Bo Pang and Lillian Lee. 2005.
Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales.
In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL’05), pages 115–
124, Ann Arbor, Michigan. Association for Compu-
tational Linguistics.
Dimitris Pappas, Petros Stavropoulos, Ion Androut-
sopoulos, and Ryan McDonald. 2020. BioMRC: A
dataset for biomedical machine reading comprehen-
sion. In Proceedings of the 19th SIGBioMed Work-
shop on Biomedical Language Processing, pages
140–149, Online. Association for Computational
Linguistics.
Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim
Rocktäschel, Yuxiang Wu, Alexander H. Miller, and
Sebastian Riedel. 2020.
How context affects lan-
guage models’ factual predictions.
In Automated
Knowledge Base Construction.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases?
In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 2463–2473, Hong Kong, China. As-
sociation for Computational Linguistics.
Mohammad
Taher
Pilehvar
and
Jose
Camacho-
Collados. 2019. WiC: the word-in-context dataset
for evaluating context-sensitive meaning represen-
tations.
In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 1267–1273, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Amir
Pouran
Ben
Veyseh,
Franck
Dernoncourt,
Quan Hung Tran, and Thien Huu Nguyen. 2020.
What does this acronym mean? introducing a new
dataset for acronym identiﬁcation and disambigua-
tion. In Proceedings of the 28th International Con-
ference on Computational Linguistics, pages 3285–
3301, Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.
Yada Pruksachatkun,
Jason Phang,
Haokun Liu,
Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe
Pang, Clara Vania, Katharina Kann, and Samuel R.
Bowman. 2020. Intermediate-task transfer learning
with pretrained language models: When and why
does it work?
In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 5231–5247, Online. Association for
Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020.
Exploring
7178
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search, 21(140):1–67.
Altaf Rahman and Vincent Ng. 2012. Resolving com-
plex cases of deﬁnite pronouns:
The Winograd
schema challenge. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 777–789, Jeju Island, Korea.
Association for Computational Linguistics.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019. Explain yourself!
leveraging language models for commonsense rea-
soning.
In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 4932–4942, Florence, Italy. Association
for Computational Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.
Hannah Rashkin, Eric Michael Smith, Margaret Li, and
Y-Lan Boureau. 2019.
Towards empathetic open-
domain conversation models: A new benchmark and
dataset.
In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 5370–5381, Florence, Italy. Association
for Computational Linguistics.
Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,
and Sameer Singh. 2020.
Beyond accuracy: Be-
havioral testing of NLP models with CheckList. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4902–
4912, Online. Association for Computational Lin-
guistics.
Anna Rogers, Olga Kovaleva, Matthew Downey, and
Anna Rumshisky. 2020. Getting closer to ai com-
plete question answering: A set of prerequisite real
tasks. Proceedings of the AAAI Conference on Arti-
ﬁcial Intelligence, 34(05):8722–8731.
Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and
Karthik Sankaranarayanan. 2018. DuoRC: Towards
complex language understanding with paraphrased
reading comprehension. In Proceedings of the 56th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1683–1693, Melbourne, Australia. Association for
Computational Linguistics.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-
vatula, and Yejin Choi. 2020. Winogrande: An ad-
versarial winograd schema challenge at scale. Pro-
ceedings of the AAAI Conference on Artiﬁcial Intel-
ligence, 34(05):8732–8740.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019. Social IQa: Com-
monsense reasoning about social interactions.
In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 4463–
4473, Hong Kong, China. Association for Computa-
tional Linguistics.
Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,
Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-
textualized affect representations for emotion recog-
nition. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
pages 3687–3697, Brussels, Belgium. Association
for Computational Linguistics.
Timo Schick and Hinrich Schütze. 2020a. Exploiting
cloze questions for few-shot text classiﬁcation and
natural language inference.
Computing Research
Repository, arXiv:2001.07676.
Timo Schick and Hinrich Schütze. 2020b. It’s not just
size that matters: Small language models are also
few-shot learners. Computing Research Repository,
arXiv:2009.07118.
Emily Sheng and David Uthus. 2020.
Investigating
societal biases in a poetry composition system. In
Proceedings of the Second Workshop on Gender
Bias in Natural Language Processing, pages 93–106,
Barcelona, Spain (Online). Association for Compu-
tational Linguistics.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV,
Eric Wallace, and Sameer Singh. 2020. AutoPrompt:
Eliciting Knowledge from Language Models with
Automatically Generated Prompts.
In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
4222–4235, Online. Association for Computational
Linguistics.
Damien Sileo, Tim Van De Cruys, Camille Pradel,
and Philippe Muller. 2019. Mining discourse mark-
ers for unsupervised sentence representation learn-
ing.
In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
3477–3486, Minneapolis, Minnesota. Association
for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013.
Recursive deep models
for semantic compositionality over a sentiment tree-
bank.
In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1631–1642, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.
Trevor
Scott
Standley,
A.
Zamir,
Dawn
Chen,
L. Guibas, Jitendra Malik, and S. Savarese. 2020.
7179
Which tasks should be learned together in multi-task
learning? In ICML.
Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,
and Claire Cardie. 2019. DREAM: A challenge data
set and models for dialogue-based reading compre-
hension. Transactions of the Association for Com-
putational Linguistics, 7:217–231.
Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau
Yih, and Ashish Sabharwal. 2019a.
Quarel:
A
dataset and models for answering questions about
qualitative relationships. Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, 33(01):7063–
7071.
Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter
Clark. 2019b. QuaRTz: An open-domain dataset of
qualitative relationship questions. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 5941–5946, Hong Kong,
China. Association for Computational Linguistics.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4149–4158, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Derek Tam, R. R. Menon, M. Bansal, Shashank
Srivastava, and Colin Raffel. 2021.
Improving
and simplifying pattern exploiting training. ArXiv,
abs/2103.11955.
Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-
ter Clark, and Antoine Bosselut. 2019. WIQA: A
dataset for “what if...” reasoning over procedural
text.
In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
6076–6085, Hong Kong, China. Association for
Computational Linguistics.
James
Thorne,
Andreas
Vlachos,
Christos
Christodoulopoulos,
and
Arpit
Mittal.
2018.
FEVER: a large-scale dataset for fact extraction
and VERiﬁcation.
In Proceedings of the 2018
Conference of the North American Chapter of
the
Association
for
Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers), pages 809–819, New Orleans, Louisiana.
Association for Computational Linguistics.
Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pas-
cal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin,
Carles Gelada, Kevin Swersky, Pierre-Antoine Man-
zagol, and Hugo Larochelle. 2020. Meta-dataset: A
dataset of datasets for learning to learn from few ex-
amples.
In International Conference on Learning
Representations.
Sowmya Vajjala and Ivana Luˇci´c. 2018.
On-
eStopEnglish corpus: A new corpus for automatic
readability assessment and text simpliﬁcation.
In
Proceedings of the Thirteenth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 297–304, New Orleans, Louisiana. As-
sociation for Computational Linguistics.
Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-
dro Sordoni, Adam Trischler, Andrew Mattarella-
Micke, Subhransu Maji, and Mohit Iyyer. 2020. Ex-
ploring and predicting transferability across NLP
tasks.
In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 7882–7926, Online. Associa-
tion for Computational Linguistics.
Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,
and Hao Ma. 2021. Entailment as few-shot learner.
arXiv preprint arXiv:2104.14690.
Sinong Wang, Madian Khabsa, and Hao Ma. 2020. To
pretrain or not to pretrain: Examining the beneﬁts
of pretrainng on resource rich tasks. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 2209–2213, On-
line. Association for Computational Linguistics.
William Yang Wang. 2017. “liar, liar pants on ﬁre”: A
new benchmark dataset for fake news detection. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 422–426, Vancouver, Canada.
Association for Computational Linguistics.
Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-
hananey, Wei Peng, Sheng-Fu Wang, and Samuel R.
Bowman. 2020. Blimp: The benchmark of linguis-
tic minimal pairs for english. Transactions of the As-
sociation for Computational Linguistics, 8:377–392.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics, 7:625–641.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V. Le. 2021. Finetuned lan-
guage models are zero-shot learners.
Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.
Crowdsourcing multiple choice science questions.
In Proceedings of the 3rd Workshop on Noisy User-
generated Text, pages 94–106, Copenhagen, Den-
mark. Association for Computational Linguistics.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
7180
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language process-
ing. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing:
System Demonstrations, pages 38–45, Online. Asso-
ciation for Computational Linguistics.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
ner, Yoav Goldberg, Daniel Deutch, and Jonathan
Berant. 2020.
Break it down: A question under-
standing benchmark. Transactions of the Associa-
tion for Computational Linguistics, 8:183–198.
Sen Wu, Hongyang R. Zhang, and Christopher Ré.
2020.
Understanding and improving information
transfer in multi-task learning. In International Con-
ference on Learning Representations.
Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulka-
rni, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and
William Yang Wang. 2019. TWEETQA: A social
media focused question answering dataset. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5020–
5031, Florence, Italy. Association for Computa-
tional Linguistics.
Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
WikiQA: A challenge dataset for open-domain ques-
tion answering.
In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2013–2018, Lisbon, Portugal. As-
sociation for Computational Linguistics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D. Manning. 2018.
HotpotQA: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 2369–2380, Brussels, Belgium. Association
for Computational Linguistics.
Wenpeng Yin, Nazneen Fatema Rajani, Dragomir
Radev, Richard Socher, and Caiming Xiong. 2020.
Universal natural language processing with limited
annotations: Try few-shot textual entailment as a
start.
In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 8229–8239, Online. Associa-
tion for Computational Linguistics.
Dani Yogatama, Cyprien de Masson d’Autume, Jerome
Connor, Tomás Kociský, Mike Chrzanowski, Ling-
peng Kong, A. Lazaridou, Wang Ling, L. Yu,
Chris Dyer, and P. Blunsom. 2019. Learning and
evaluating general linguistic intelligence.
ArXiv,
abs/1901.11373.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao,
Shanelle Roman,
Zilin Zhang,
and Dragomir Radev. 2018.
Spider:
A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-SQL task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3911–3921, Brussels, Belgium. Association for
Computational Linguistics.
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,
Karol Hausman, Chelsea Finn, and Sergey Levine.
2020. Meta-world: A benchmark and evaluation for
multi-task and meta reinforcement learning. In Pro-
ceedings of the Conference on Robot Learning, vol-
ume 100 of Proceedings of Machine Learning Re-
search, pages 1094–1100. PMLR.
Amir R. Zamir, Alexander Sax, William B. Shen,
Leonidas J. Guibas, Jitendra Malik, and Silvio
Savarese. 2018.
Taskonomy: Disentangling task
transfer learning. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). IEEE.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and
Yejin Choi. 2018. SWAG: A large-scale adversar-
ial dataset for grounded commonsense inference. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 93–
104, Brussels, Belgium. Association for Computa-
tional Linguistics.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019.
HellaSwag: Can
a machine really ﬁnish your sentence?
In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 4791–
4800, Florence, Italy. Association for Computational
Linguistics.
Hao Zhang, Jae Ro, and Richard Sproat. 2020. Semi-
supervised URL segmentation with recurrent neu-
ral networks pre-trained on knowledge graph enti-
ties. In Proceedings of the 28th International Con-
ference on Computational Linguistics, pages 4667–
4675, Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.
Rui Zhang and Joel Tetreault. 2019. This email could
save your life: Introducing the task of email subject
line generation. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics, pages 446–456, Florence, Italy. Association
for Computational Linguistics.
Sheng Zhang, X. Liu, J. Liu, Jianfeng Gao, Kevin Duh,
and Benjamin Van Durme. 2018. Record: Bridging
7181
the gap between human and machine commonsense
reading comprehension. ArXiv, abs/1810.12885.
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q
Weinberger, and Yoav Artzi. 2021. Revisiting few-
sample {bert} ﬁne-tuning. In International Confer-
ence on Learning Representations.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. In Proceedings of the 28th International
Conference on Neural Information Processing Sys-
tems - Volume 1, NIPS’15, page 649–657, Cam-
bridge, MA, USA. MIT Press.
Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
PAWS: Paraphrase adversaries from word scram-
bling.
In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
1298–1308, Minneapolis, Minnesota. Association
for Computational Linguistics.
Ruiqi Zhong, Kristy Lee, Zheng Zhang, and D. Klein.
2021.
Adapting language models for zero-shot
learning by meta-tuning on dataset and prompt col-
lections. ArXiv, abs/2104.04670.
Victor Zhong, Caiming Xiong, and Richard Socher.
2017.
Seq2sql:
Generating structured queries
from natural language using reinforcement learning.
CoRR, abs/1709.00103.
Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan
Roth. 2019.
“going on a vacation” takes longer
than “going for a walk”: A study of temporal com-
monsense understanding.
In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 3363–3369, Hong Kong,
China. Association for Computational Linguistics.
7182
A
Selected Tasks in NLP Few-shot Gym
Table 3: Tasks in NLP Few-shot Gym.
Task Name
Ontology
Reference
acronym_identiﬁcation
other
Pouran Ben Veyseh et al. 2020
ade_corpus_v2-classiﬁcation
cls/other
Gurulingappa et al. 2012
ade_corpus_v2-dosage
other/slot ﬁlling
Gurulingappa et al. 2012
ade_corpus_v2-effect
other/slot ﬁlling
Gurulingappa et al. 2012
adversarialqa
qa/machine reading comprehension
Bartolo et al. 2020
aeslc
cg/summarization
Zhang and Tetreault 2019
ag_news
cls/topic
Gulli (link)
ai2_arc
qa/multiple-choice qa
Clark et al. 2018
amazon_polarity
cls/sentiment analysis
McAuley and Leskovec 2013
anli
cls/nli
Nie et al. 2020
app_reviews
other/regression
Missing
aqua_rat
qa/multiple-choice qa
Ling et al. 2017
art (abductive nli)
other
Bhagavatula et al. 2020
aslg_pc12
other
Othman and Jemni 2012
biomrc
qa/machine reading comprehension
Pappas et al. 2020
blimp-anaphor_gender_agreement
other/linguistic phenomenon
Warstadt et al. 2020
blimp-anaphor_number_agreement
other/linguistic phenomenon
Warstadt et al. 2020
blimp-determiner_noun_agreement_with_adj_irregular_1
other/linguistic phenomenon
Warstadt et al. 2020
blimp-ellipsis_n_bar_1
other/linguistic phenomenon
Warstadt et al. 2020
blimp-ellipsis_n_bar_2
other/linguistic phenomenon
Warstadt et al. 2020
blimp-existential_there_quantiﬁers_1
other/linguistic phenomenon
Warstadt et al. 2020
blimp-irregular_past_participle_adjectives
other/linguistic phenomenon
Warstadt et al. 2020
blimp-sentential_negation_npi_licensor_present
other/linguistic phenomenon
Warstadt et al. 2020
blimp-sentential_negation_npi_scope
other/linguistic phenomenon
Warstadt et al. 2020
blimp-wh_questions_object_gap
other/linguistic phenomenon
Warstadt et al. 2020
boolq
qa/binary
Clark et al. 2019
break-QDMR
other
Wolfson et al. 2020
break-QDMR-high-level
other
Wolfson et al. 2020
circa
cls/other
Louis et al. 2020
climate_fever
cls/fact checking
Diggelmann et al. 2020
codah
qa/multiple-choice qa
Chen et al. 2019
common_gen
other
Lin et al. 2020b
commonsense_qa
qa/multiple-choice qa
Talmor et al. 2019
cos_e
other/generate explanation
Rajani et al. 2019
cosmos_qa
qa/multiple-choice qa
Huang et al. 2019
crawl_domain
other
Zhang et al. 2020
crows_pairs
other
Nangia et al. 2020
dbpedia_14
cls/topic
Lehmann et al. 2015
deﬁnite_pronoun_resolution
other
Rahman and Ng 2012
discovery
cls/other
Sileo et al. 2019
dream
qa/multiple-choice qa
Sun et al. 2019
duorc
qa/machine reading comprehension
Saha et al. 2018
e2e_nlg_cleaned
other
Dušek et al. 2020, 2019
eli5-askh
qa/long-form qa
Fan et al. 2019
eli5-asks
qa/long-form qa
Fan et al. 2019
eli5-eli5
qa/long-form qa
Fan et al. 2019
emo
cls/emotion
Chatterjee et al. 2019
emotion
cls/emotion
Saravia et al. 2018
empathetic_dialogues
cg/dialogue
Rashkin et al. 2019
ethos-directed_vs_generalized
cls/hate speech detection
Mollas et al. 2020
ethos-disability
cls/hate speech detection
Mollas et al. 2020
ethos-gender
cls/hate speech detection
Mollas et al. 2020
ethos-national_origin
cls/hate speech detection
Mollas et al. 2020
ethos-race
cls/hate speech detection
Mollas et al. 2020
ethos-religion
cls/hate speech detection
Mollas et al. 2020
ethos-sexual_orientation
cls/hate speech detection
Mollas et al. 2020
ﬁnancial_phrasebank
cls/sentiment analysis
Malo et al. 2014
freebase_qa
qa/closed-book qa
Jiang et al. 2019
gigaword
cg/summarization
Napoles et al. 2012
glue-cola
cls/other
Warstadt et al. 2019
glue-mnli
cls/nli
Williams et al. 2018
glue-mrpc
cls/paraphrase
Dolan and Brockett 2005
glue-qnli
cls/nli
Rajpurkar et al. 2016
glue-qqp
cls/paraphrase
(link)
glue-rte
cls/nli
Dagan et al. 2005; Bar-Haim et al. 2006
Giampiccolo et al. 2007; Bentivogli et al. 2009
glue-sst2
cls/sentiment analysis
Socher et al. 2013
glue-wnli
cls/nli
Levesque et al. 2012
google_wellformed_query
cls/other
Faruqui and Das 2018
hate_speech18
cls/hate speech detection
de Gibert et al. 2018
hate_speech_offensive
cls/hate speech detection
Davidson et al. 2017
hatexplain
cls/hate speech detection
Mathew et al. 2020
health_fact
cls/fact checking
Kotonya and Toni 2020
hellaswag
qa/multiple-choice qa
Zellers et al. 2019
hotpot_qa
qa/machine reading comprehension
Yang et al. 2018
imdb
cls/sentiment analysis
Maas et al. 2011
jeopardy
qa/closed-book qa
(link)
kilt_ay2
other/entity linking
Hoffart et al. 2011
Continued on next page
7183
Task Name
Ontology
Reference
kilt_fever
cls/fact checking
Thorne et al. 2018
kilt_hotpotqa
qa/closed-book qa
Yang et al. 2018
kilt_nq
qa/closed-book qa
Kwiatkowski et al. 2019
kilt_trex
qa/closed-book qa
Elsahar et al. 2018
kilt_wow
cg/dialogue
Dinan et al. 2019
kilt_zsre
qa/closed-book qa
Levy et al. 2017
lama-conceptnet
qa/closed-book qa
Petroni et al. 2019, 2020
lama-google_re
qa/closed-book qa
Petroni et al. 2019, 2020
lama-squad
qa/closed-book qa
Petroni et al. 2019, 2020
lama-trex
qa/closed-book qa
Petroni et al. 2019, 2020
liar
cls/fact checking
Wang 2017
limit
other
Manotas et al. 2020
math_qa
qa/multiple-choice qa
Amini et al. 2019
mc_taco
qa/binary
Zhou et al. 2019
medical_questions_pairs
cls/paraphrase
McCreery et al. 2020
mocha
other/regression
Chen et al. 2020a
multi_news
cg/summarization
Fabbri et al. 2019
numer_sense
qa/closed-book qa
Lin et al. 2020a
onestop_english
cls/other
Vajjala and Luˇci´c 2018
openbookqa
qa/multiple-choice qa
Mihaylov et al. 2018
paws
cls/paraphrase
Zhang et al. 2019
piqa
other
Bisk et al. 2020
poem_sentiment
cls/sentiment analysis
Sheng and Uthus 2020
proto_qa
other
Boratko et al. 2020
qa_srl
other
He et al. 2015
qasc
qa/multiple-choice qa
Khot et al. 2020
quail
qa/multiple-choice qa
Rogers et al. 2020
quarel
qa/multiple-choice qa
Tafjord et al. 2019a
quartz-no_knowledge
qa/multiple-choice qa
Tafjord et al. 2019b
quartz-with_knowledge
qa/multiple-choice qa
Tafjord et al. 2019b
quoref
qa/machine reading comprehension
Dasigi et al. 2019
race-high
qa/multiple-choice qa
Lai et al. 2017
race-middle
qa/multiple-choice qa
Lai et al. 2017
reddit_tifu-title
cg/summarization
Kim et al. 2019
reddit_tifu-tldr
cg/summarization
Kim et al. 2019
ropes
qa/machine reading comprehension
Lin et al. 2019
rotten_tomatoes
cls/sentiment analysis
Pang and Lee 2005
samsum
cg/summarization
Gliwa et al. 2019
scicite
cls/other
Cohan et al. 2019
sciq
qa/multiple-choice qa
Welbl et al. 2017
scitail
cls/nli
Khot et al. 2018
search_qa
qa/closed-book qa
Dunn et al. 2017
sick
cls/nli
Marelli et al. 2014
sms_spam
cls/other
Almeida et al. 2011
social_i_qa
qa/multiple-choice qa
Sap et al. 2019
spider
cg/other
Yu et al. 2018
squad-no_context
qa/closed-book qa
Rajpurkar et al. 2016
squad-with_context
qa/machine reading comprehension
Rajpurkar et al. 2016
superglue-cb
cls/nli
de Marneffe et al. 2019
superglue-copa
qa/multiple-choice qa
Gordon et al. 2012
superglue-multirc
qa/multiple-choice qa
Khashabi et al. 2018
superglue-record
qa/machine reading comprehension
Zhang et al. 2018
superglue-rte
cls/nli
Dagan et al. 2005; Bar-Haim et al. 2006
Giampiccolo et al. 2007; Bentivogli et al. 2009
superglue-wic
cls/other
Pilehvar and Camacho-Collados 2019
superglue-wsc
cls/other
Levesque et al. 2012
swag
qa/multiple-choice qa
Zellers et al. 2018
tab_fact
cls/fact checking
Chen et al. 2020b
trec
cls/other
Li and Roth 2002; Hovy et al. 2001
trec-ﬁnegrained
cls/other
Li and Roth 2002; Hovy et al. 2001
tweet_eval-emoji
cls/emotion
Barbieri et al. 2020
tweet_eval-emotion
cls/emotion
Barbieri et al. 2020
tweet_eval-hate
cls/emotion
Barbieri et al. 2020
tweet_eval-irony
cls/emotion
Barbieri et al. 2020
tweet_eval-offensive
cls/emotion
Barbieri et al. 2020
tweet_eval-sentiment
cls/emotion
Barbieri et al. 2020
tweet_eval-stance_abortion
cls/emotion
Barbieri et al. 2020
tweet_eval-stance_atheism
cls/emotion
Barbieri et al. 2020
tweet_eval-stance_climate
cls/emotion
Barbieri et al. 2020
tweet_eval-stance_feminist
cls/emotion
Barbieri et al. 2020
tweet_eval-stance_hillary
cls/emotion
Barbieri et al. 2020
tweet_qa
qa/machine reading comprehension
Xiong et al. 2019
web_questions
qa/closed-book qa
Berant et al. 2013
wiki_auto
cls/other
Jiang et al. 2020
wiki_bio
cg/other
Lebret et al. 2016
wiki_qa
cls/other
Yang et al. 2015
wiki_split
cg/other
Botha et al. 2018
wikisql
cg/other
Zhong et al. 2017
wino_grande
qa/multiple-choice qa
Sakaguchi et al. 2020
wiqa
qa/multiple-choice qa
Tandon et al. 2019
xsum
cg/summarization
Narayan et al. 2018
yahoo_answers_topics
cls/topic
(link)
yelp_polarity
cls/sentiment analysis
Zhang et al. 2015; (link)
yelp_review_full
other/regression
Zhang et al. 2015; (link)
7184
B
Details about Task Partition
B.1
Partition 1. Random
1
{
2
"train": ['glue -mrpc ', 'math_qa ', 'quarel ', 'e2e_nlg_cleaned ', 'tweet_eval -stance_atheism ', 'lama -squad '
, 'tab_fact ', 'aqua_rat ', 'tweet_eval -emoji ', 'glue -wnli ', 'codah ', 'tweet_eval -offensive ', '
wiki_qa ', 'blimp -ellipsis_n_bar_1', 'openbookqa ', 'sms_spam ', 'acronym_identification ', 'blimp -
determiner_noun_agreement_with_adj_irregular_1', 'ethos -national_origin ', 'spider ', '
definite_pronoun_resolution ', 'hellaswag ', 'superglue -wsc ', 'numer_sense ', 'ade_corpus_v2-dosage ',
'blimp -ellipsis_n_bar_2', 'kilt_ay2', 'squad -no_context ', 'google_wellformed_query ', 'xsum ', 'wiqa '
, 'tweet_eval -stance_abortion ', 'reddit_tifu -tldr ', 'ade_corpus_v2-effect ', 'qa_srl ', 'ethos -
religion ', 'commonsense_qa ', 'jeopardy ', 'biomrc ', 'superglue -multirc ', 'ethos -race ', 'eli5-askh ',
'glue -qqp ', 'paws ', 'ethos -directed_vs_generalized ', 'glue -sst2', 'mocha ', 'tweet_eval -hate ', 'glue
-rte ', 'blimp -anaphor_number_agreement ', 'lama -conceptnet ', 'hate_speech_offensive ', 'superglue -wic
', 'boolq ', 'kilt_hotpotqa ', 'quartz -no_knowledge ', 'aslg_pc12', 'sick ', 'tweet_eval -stance_climate
', 'tweet_eval -sentiment ', 'crows_pairs ', 'glue -mnli ', 'medical_questions_pairs ', 'break -QDMR -high -
level ', 'qasc ', 'imdb ', 'ethos -gender ', 'trec -finegrained ', 'adversarialqa ', 'onestop_english ', '
web_questions ', 'duorc ', 'yelp_review_full ', 'swag ', 'proto_qa ', 'scitail ', 'tweet_eval -
stance_feminist ', 'limit ', 'common_gen ', 'scicite ', 'blimp -irregular_past_participle_adjectives ', '
social_i_qa ', 'anli ', 'kilt_zsre ', 'cosmos_qa ', 'superglue -record ', 'squad -with_context ', 'emotion '
, 'blimp -existential_there_quantifiers_1', 'race -middle ', 'kilt_wow ', 'sciq ', 'wino_grande ', '
rotten_tomatoes ', 'superglue -cb ', 'poem_sentiment ', 'ropes ', 'reddit_tifu -title ', 'piqa ', '
climate_fever ', 'lama -google_re ', 'search_qa ', 'wiki_auto ', 'mc_taco ', 'blimp -
wh_questions_object_gap ', 'hotpot_qa ', 'emo ', 'kilt_nq ', 'kilt_trex ', 'quartz -with_knowledge ', '
dbpedia_14', 'yahoo_answers_topics ', 'app_reviews ', 'superglue -copa ', 'blimp -
anaphor_gender_agreement ', 'hate_speech18', 'gigaword ', 'multi_news ', 'aeslc ', 'quail '],
3
"dev": ['cos_e ', 'kilt_fever ', 'eli5-asks ', 'trec ', 'eli5-eli5', 'art ', 'empathetic_dialogues ', '
tweet_qa ', 'wikisql ', 'lama -trex ', 'tweet_eval -stance_hillary ', 'discovery ', 'tweet_eval -emotion ',
'liar ', 'wiki_bio ', 'dream ', 'ade_corpus_v2-classification ', 'health_fact ', 'samsum ', '
financial_phrasebank '],
4
"test": ['quoref ', 'wiki_split ', 'ethos -disability ', 'yelp_polarity ', 'superglue -rte ', 'glue -cola ', '
ethos -sexual_orientation ', 'blimp -sentential_negation_npi_scope ', 'ai2_arc ', 'amazon_polarity ', '
race -high ', 'blimp -sentential_negation_npi_licensor_present ', 'tweet_eval -irony ', 'break -QDMR ', '
crawl_domain ', 'freebase_qa ', 'glue -qnli ', 'hatexplain ', 'ag_news ', 'circa '],
5
}
B.2
Partition 2.1. 45cls
1
{
2
"train": ["superglue -rte", "tweet_eval -sentiment", "discovery", "glue -rte", "superglue -wsc", "scicite",
"glue -mrpc", "tweet_eval -stance_hillary", "tweet_eval -offensive", "emotion", "hatexplain", "glue -
cola", "sick", "paws", "ethos -sexual_orientation", "glue -qqp", "tweet_eval -emotion", "sms_spam", "
health_fact", "glue -mnli", "imdb", "ethos -disability", "glue -wnli", "scitail", "trec -finegrained",
"yahoo_answers_topics", "liar", "glue -sst2", "tweet_eval -stance_abortion", "circa", "tweet_eval -
stance_climate", "glue -qnli", "tweet_eval -emoji", "ethos -directed_vs_generalized", "ade_corpus_v2-
classification", "wiki_auto", "hate_speech_offensive", "superglue -wic", "google_wellformed_query",
"tweet_eval -irony", "ethos -gender", "onestop_english", "trec", "rotten_tomatoes", "kilt_fever"],
3
"dev": ["tweet_eval -stance_feminist", "ethos -national_origin", "tweet_eval -hate", "ag_news", "
amazon_polarity", "hate_speech18", "poem_sentiment", "climate_fever", "medical_questions_pairs", "
tweet_eval -stance_atheism"],
4
"test": ["superglue -cb", "dbpedia_14", "wiki_qa", "emo", "yelp_polarity", "ethos -religion", "
financial_phrasebank", "tab_fact", "anli", "ethos -race"],
5
}
B.3
Partition 2.2. 23cls+22non-cls
1
{
2
"train": ["ade_corpus_v2-dosage", "biomrc", "blimp -ellipsis_n_bar_2", "blimp -
sentential_negation_npi_scope", "commonsense_qa", "crows_pairs", "duorc", "hellaswag", "kilt_zsre",
"lama -google_re", "lama -squad", "math_qa", "numer_sense", "openbookqa", "piqa", "proto_qa", "
quartz -no_knowledge", "race -high", "reddit_tifu -tldr", "ropes", "sciq", "wiki_bio", "discovery", "
emotion", "ethos -disability", "ethos -sexual_orientation", "glue -cola", "glue -mnli", "glue -mrpc", "
glue -qqp", "glue -rte", "glue -wnli", "hatexplain", "health_fact", "imdb", "paws", "scicite", "sick",
"sms_spam", "superglue -rte", "superglue -wsc", "tweet_eval -emotion", "tweet_eval -offensive", "
tweet_eval -sentiment", "tweet_eval -stance_hillary"],
3
"dev": ["tweet_eval -stance_feminist", "ethos -national_origin", "tweet_eval -hate", "ag_news", "
amazon_polarity", "hate_speech18", "poem_sentiment", "climate_fever", "medical_questions_pairs", "
tweet_eval -stance_atheism"],
4
"test": ["superglue -cb", "dbpedia_14", "wiki_qa", "emo", "yelp_polarity", "ethos -religion", "
financial_phrasebank", "tab_fact", "anli", "ethos -race"]
5
}
B.4
Partition 2.3. 45non-cls
1
{
7185
2
"train": ["ade_corpus_v2-dosage", "art", "biomrc", "blimp -anaphor_number_agreement", "blimp -
ellipsis_n_bar_2", "blimp -sentential_negation_npi_licensor_present", "blimp -
sentential_negation_npi_scope", "break -QDMR -high -level", "commonsense_qa", "crows_pairs", "dream",
"duorc", "eli5-asks", "eli5-eli5", "freebase_qa", "gigaword", "hellaswag", "hotpot_qa", "kilt_ay2",
"kilt_hotpotqa", "kilt_trex", "kilt_zsre", "lama -conceptnet", "lama -google_re", "lama -squad", "
math_qa", "numer_sense", "openbookqa", "piqa", "proto_qa", "qa_srl", "quarel", "quartz -no_knowledge
", "race -high", "reddit_tifu -title", "reddit_tifu -tldr", "ropes", "sciq", "social_i_qa", "spider",
"superglue -multirc", "wiki_bio", "wikisql", "xsum", "yelp_review_full"],
3
"dev": ["tweet_eval -stance_feminist", "ethos -national_origin", "tweet_eval -hate", "ag_news", "
amazon_polarity", "hate_speech18", "poem_sentiment", "climate_fever", "medical_questions_pairs", "
tweet_eval -stance_atheism"],
4
"test": ["superglue -cb", "dbpedia_14", "wiki_qa", "emo", "yelp_polarity", "ethos -religion", "
financial_phrasebank", "tab_fact", "anli", "ethos -race"]
5
}
B.5
Partition 3.1. Held-out-NLI
1
{
2
"train": ["ade_corpus_v2-classification", "ag_news", "amazon_polarity", "circa", "climate_fever", "
dbpedia_14", "discovery", "emo", "emotion", "ethos -directed_vs_generalized", "ethos -disability", "
ethos -gender", "ethos -national_origin", "ethos -race", "ethos -religion", "ethos -sexual_orientation",
"financial_phrasebank", "glue -cola", "glue -mrpc", "glue -qqp", "glue -sst2", "
google_wellformed_query", "hate_speech18", "hate_speech_offensive", "hatexplain", "health_fact", "
imdb", "kilt_fever", "liar", "medical_questions_pairs", "onestop_english", "paws", "poem_sentiment"
, "rotten_tomatoes", "scicite", "sick", "sms_spam", "superglue -wic", "superglue -wsc", "tab_fact", "
trec", "trec -finegrained", "tweet_eval -emoji", "tweet_eval -emotion", "tweet_eval -hate", "tweet_eval
-irony", "tweet_eval -offensive", "tweet_eval -sentiment", "tweet_eval -stance_abortion", "tweet_eval -
stance_atheism", "tweet_eval -stance_climate", "tweet_eval -stance_feminist", "tweet_eval -
stance_hillary", "wiki_auto", "wiki_qa", "yahoo_answers_topics", "yelp_polarity"
3
],
4
"dev": [],
5
"test": ["anli", "glue -mnli", "glue -qnli", "glue -rte", "glue -wnli", "scitail", "sick", "superglue -cb"]
6
}
B.6
Partition 3.2. Held-out-Para
1
{
2
"train": ["ade_corpus_v2-classification", "ag_news", "amazon_polarity", "anli", "circa", "climate_fever"
, "dbpedia_14", "discovery", "emo", "emotion", "ethos -directed_vs_generalized", "ethos -disability",
"ethos -gender", "ethos -national_origin", "ethos -race", "ethos -religion", "ethos -sexual_orientation
", "financial_phrasebank", "glue -cola", "glue -mnli", "glue -qnli", "glue -rte", "glue -sst2", "glue -
wnli", "google_wellformed_query", "hate_speech18", "hate_speech_offensive", "hatexplain", "
health_fact", "imdb", "kilt_fever", "liar", "onestop_english", "poem_sentiment", "rotten_tomatoes",
"scicite", "scitail", "sick", "sms_spam", "superglue -cb", "superglue -rte", "superglue -wic", "
superglue -wsc", "tab_fact", "trec", "trec -finegrained", "tweet_eval -emoji", "tweet_eval -emotion", "
tweet_eval -hate", "tweet_eval -irony", "tweet_eval -offensive", "tweet_eval -sentiment", "tweet_eval -
stance_abortion", "tweet_eval -stance_atheism", "tweet_eval -stance_climate", "tweet_eval -
stance_feminist", "tweet_eval -stance_hillary", "wiki_auto", "wiki_qa", "yahoo_answers_topics", "
yelp_polarity"],
3
"dev": [],
4
"test": ["glue -mrpc", "glue -qqp", "medical_questions_pairs", "paws"]
5
}
B.7
Partition 4.1. Held-out-MRC
1
{
2
"train": ["ai2_arc", "aqua_rat", "boolq", "codah", "commonsense_qa", "cosmos_qa", "dream", "eli5-askh",
"eli5-asks", "eli5-eli5", "freebase_qa", "hellaswag", "jeopardy", "kilt_hotpotqa", "kilt_nq", "
kilt_trex", "kilt_zsre", "lama -conceptnet", "lama -google_re", "lama -squad", "lama -trex", "math_qa",
"mc_taco", "numer_sense", "openbookqa", "qasc", "quail", "quarel", "quartz -no_knowledge", "quartz -
with_knowledge", "race -high", "race -middle", "sciq", "search_qa", "social_i_qa", "squad -no_context"
, "superglue -copa", "superglue -multirc", "swag", "web_questions", "wino_grande", "wiqa"
3
],
4
"dev": [],
5
"test": ["adversarialqa", "biomrc", "duorc", "hotpot_qa", "quoref", "ropes", "squad -with_context", "
superglue -record", "tweet_qa"],
6
}
B.8
Partition 4.2. Held-out-MCQA
1
{
2
"train": ["adversarialqa", "biomrc", "boolq", "duorc", "eli5-askh", "eli5-asks", "eli5-eli5", "
freebase_qa", "hotpot_qa", "jeopardy", "kilt_hotpotqa", "kilt_nq", "kilt_trex", "kilt_zsre", "lama -
conceptnet", "lama -google_re", "lama -squad", "lama -trex", "mc_taco", "numer_sense", "quoref", "
ropes", "search_qa", "squad -no_context", "squad -with_context", "superglue -multirc", "superglue -
record", "tweet_qa", "web_questions"
3
],
4
"dev": [],
7186
5
"test": ["ai2_arc", "aqua_rat", "codah", "commonsense_qa", "cosmos_qa", "dream", "hellaswag", "math_qa",
"openbookqa", "qasc", "quail", "quarel", "quartz -no_knowledge", "quartz -with_knowledge", "race -
high", "race -middle", "sciq", "social_i_qa", "superglue -copa", "swag", "wino_grande", "wiqa"]
6
}
B.9
Partition 5. Held-out-GLUE
To examine whether combining our methods with template-based training (Schick and Schütze, 2020a,b;
Gao et al., 2020) results in even better few-shot performance, we add another partition that uses all
non-GLUE classiﬁcation tasks as Ttrain, and all GLUE tasks as Ttest.
1
{
2
"train": ["ade_corpus_v2-classification", "ag_news", "amazon_polarity", "anli", "circa", "climate_fever"
, "dbpedia_14", "discovery", "emo", "emotion", "ethos -directed_vs_generalized", "ethos -disability",
"ethos -gender", "ethos -national_origin", "ethos -race", "ethos -religion", "ethos -sexual_orientation
", "financial_phrasebank", "google_wellformed_query", "hate_speech18", "hate_speech_offensive", "
hatexplain", "health_fact", "imdb", "kilt_fever", "liar", "medical_questions_pairs", "
onestop_english", "paws", "poem_sentiment", "rotten_tomatoes", "scicite", "scitail", "sick", "
sms_spam", "superglue -cb", "superglue -wic", "superglue -wsc", "tab_fact", "trec", "trec -finegrained"
, "tweet_eval -emoji", "tweet_eval -emotion", "tweet_eval -hate", "tweet_eval -irony", "tweet_eval -
offensive", "tweet_eval -sentiment", "tweet_eval -stance_abortion", "tweet_eval -stance_atheism", "
tweet_eval -stance_climate", "tweet_eval -stance_feminist", "tweet_eval -stance_hillary", "wiki_auto",
"wiki_qa", "yahoo_answers_topics", "yelp_polarity"],
3
"dev": [],
4
"test": ["glue -cola", "glue -mnli", "glue -mrpc", "glue -qnli", "glue -qqp", "glue -rte", "glue -sst2", "glue -
wnli"]
5
}
Continued on next page.
7187
C
Additional Results and Analysis
Q4. Does the improved cross-task general-
ization ability go beyond few-shot settings?
In real-world applications, annotated data usu-
ally grow for a few-shot task over time. Is up-
stream learning still helpful when a target task
has more shots? To study this question, we study
CommonsenseQA (in Held-out-Multiple-Choice Par-
tition), ROPES (in Held-out-MRC Partition), and
MNLI (in Held-out-NLI Partition) as target tasks in
medium and high-resource scenarios. We take their
corresponding checkpoints after upstream learn-
ing and conduct experiments in medium and high-
resource scenarios. That is, we randomly sam-
ple {32, 64, . . . , 4096} examples from the three
datasets, and use them as Dtrain. Then, we sample
a Ddev with the same size as Dtrain, or has the size
of 1024 if |Dtrain| > 1024. We also try ﬁne-tuning
with the full dataset.6 The performance of these
settings is shown in Fig. 7.
From Fig. 7, we see that the beneﬁts brought
by upstream learning methods extend into medium
resource cases with up to 2048 training examples.
For CommonsenseQA, checkpoints from upstream
learning outperform direct ﬁne-tuning signiﬁcantly,
even with the full dataset. This ﬁnding encourages
the use of upstream learning before task-speciﬁc
ﬁne-tuning when the target task has limited an-
notation.
On the other hand, for resource-rich
tasks (e.g., MNLI), the improvement brought by
upstream learning diminishes. This aligns with the
ﬁndings of (Wang et al., 2020) who discuss the
beneﬁts of pre-training on resource-rich tasks.
Q5. Can we further improve few-shot perfor-
mance by using different/larger pre-trained
models?
We have been mainly using BART-Base (139M
parameters) as the main network, while it is possi-
ble to further push the limits of few-shot learning
by using scaling up to larger models or using differ-
ent model architectures. Previous work has shown
that scaling up model size leads to better perfor-
mance (Raffel et al., 2020; Brown et al., 2020).
Moreover, since meta-learning algorithms are natu-
rally unstable, it is important to verify whether they
6We do ﬁve random samples of 1024 examples as Ddev
and use the remaining examples in the original train set as
Dtrain. We use the original dev set for testing.
function as expected with larger models. In Q5, we
experiment with T5-v1.1-Base (248M)7 and BART-
Large (406M) model with Held-out-Para Partition
to verify these assumptions. We only consider ﬁrst-
order methods, as second-order optimization with
these larger models is impossible with our available
computation.
Our results are plotted in Fig. 8. In Fig. 8(a) we
compare the few-shot performance of direct ﬁne-
tuning on these three pre-trained models. On aver-
age, few-shot performance grows with models size,
with a few exceptions such as QQP+T5-v1.1-Base
and MRPC+Bart-Large. In Fig. 8(b-c) we plot
the effect brought by upstream learning method
for larger models. Except for FoMAML+T5-v1.1-
Base8, upstream learning methods consistently im-
proves few-shot performance on Ttest, which ver-
iﬁes that upstream learning methods we use are
model-agnostic, and can be applied to larger mod-
els to further improve few-shot performance.
Q6. Can we use pattern-exploiting training
to replace direct ﬁne-tuning to achieve even
better performance?
Pattern-exploiting training (PET) is a novel method
that formulate a target task into cloze-style ques-
tions (Schick and Schütze, 2020a,b; Gao et al.,
2020). This approach narrows the gap between
the masked language modeling objective during
pre-training and downstream task ﬁne-tuning, and
therefore leads to more efﬁcient transfer. PET is
demonstrated to be effective with encoder mod-
els (e.g., RoBERTa), however, whether it is appli-
cable to text-to-text models with auto-regressive
decoders is underexplored to the best of our knowl-
edge. In Q6, we study whether applying PET-
style methods to text-to-text models is feasible, and
whether combining the two methods further pushes
the few-shot performance.
To align with the experiment settings in (Schick
and Schütze, 2020a,b; Gao et al., 2020), we intro-
duce a new task partition “Held-out-GLUE”, which
uses non-GLUE classiﬁcation tasks as Ttrain, and
GLUE tasks as Ttest. We use the top 3 patterns in
(Gao et al., 2020) for each GLUE task, and use the
7T5-Base was trained on a mixture of downstream tasks
during its pre-training; such practice strays from the purpose
of our study. Therefore, we use T5-v1.1-Base model, which is
trained with the C4 Corpus only.
8We observe instability in training loss during FoMAML
training for T5-v1.1-Base.
7188
32
64
128
256
512
1024 2048 4096 8717(all)
# Train Examples
20%
30%
40%
50%
60%
Accuracy
Commonsense QA, Held­out­Multiple­Choice
BART­Base
Multi­Task Learning
Meta­Learning
32
64
128
256
512
1024 2048 4096  9900(all)
# Train Examples
20%
30%
40%
50%
60%
70%
QA­F1
Ropes, Held­out­MRC
48
96
192
384
768 1536 3072
391678(all)
# Train Examples
30%
40%
50%
60%
70%
80%
90%
Accuracy
MNLI, Held­out­NLI
Figure 7: Performance comparisons in medium and high-resource scenarios. Beneﬁts brought by upstream learning
lasts in medium-resource scenarios.
glue­mrpc
glue­qqp
medical_qpairs
paws
average
40%
50%
60%
70%
80%
Accuracy (%)
(a) Direct Fine­tuning w. Different Base Models
Bart­Base
T5­v1.1­Base
Bart­Large
glue­mrpc
glue­qqp
medical_qpairs
paws
average
­10%
0%
10%
20%
30%
40%
50%
Relative Performance Gain (%)
(b) T5­v1.1­Base
multi
first­order maml
reptile
glue­mrpc
glue­qqp
medical_qpairs
paws
average
­10%
0%
10%
20%
30%
40%
50%
Relative Performance Gain (%)
(c) Bart­Large
multi
first­order maml
reptile
Figure 8: Extending upstream learning to larger pre-trained text-to-text models. (a) Absolute performance with
direct ﬁne-tuning with different pre-trained models. (b-c) Relative performance gain using upstream learning.
ensemble of the three models to produce the ﬁnal
prediction.
Since pattern-exploiting training is originally de-
signed for encoder models (e.g., BERT/RoBERTa),
we ﬁrst tried two of its variants that adapts it to
our auto-regressive transformer models. The ﬁrst
variant generates complete sentence, e.g., generate
“The movie is great. A wonderful piece” from “The
movie is great. A <mask> piece” for sentiment
classiﬁcation. The second variant generates only
the word “wonderful”, from “The movie is great.
A <mask> piece”. Though the ﬁrst variant is more
similar to the denoising pre-training objective of
BART, we ﬁnd the second variant to have better
performance.
We then launch pattern-exploiting training us-
ing variant two with the original BART-Base mod-
els. We observe negative performance on aver-
age (leftmost blue bar in Fig. 9). Performance
is improved with CoLA and MRPC, but not with
the remaining GLUE tasks. We further launch
experiments with/without pattern-exploiting train-
ing, with our upstream learning checkpoints. Still
pattern-exploiting training leads to deteriorated per-
formance on average.
We stop further investigation since this is out of
the scope of our study. Still we believe it is im-
portant to identify the reasons and develop pattern-
exploiting methods for auto-regressive models.
D
Reproducibility
Implementation.
All our experiments are imple-
mented with Huggingface Transformers9 (Wolf
et al., 2020).
For higher-order optimization in
the meta-learning approach optimization, we use
higher library10. Our code has been uploaded in
supplementary materials, and is also open-sourced
at https://github.com/INK-USC/CrossFit.
Hyper-parameters.
We mainly follow the prac-
tice in (Gao et al., 2020). During few-shot ﬁne-
tuning, we select the learning rate from {1e −
5, 2e−5, 5e−5}, and the batch size from {2, 4, 8},
based on Ddev performance. We set the total num-
ber of updates to be 1000, number of warmup up-
dates to be 100. We evaluate the model on Ddev
every 100 steps.
Infrastructure and Runtime.
Upstream learn-
ing are done with one single Quadro RTX 8000
(48GB). Upstream learning jobs ﬁnishes within 3
hours on average. Fine-tuning experiments are all
done with one single GPU, with either NVIDIA
Quadro GP100, NVIDIA Quadro RTX 8000,
NVIDIA Quadro RTX 6000, NVIDIA GeForce
RTX 1080 Ti, or NVIDIA GeForce RTX 2080 Ti,
based on availability. Fine-tuning on one few-shot
9https://github.com/huggingface/transformers
10https://github.com/facebookresearch/higher
7189
glue­rte
glue­sst2
glue­qnli
glue­mnli
glue­qqp
glue­cola
glue­mrpc
average
­10%
0%
10%
20%
30%
40%
50%
60%
Relative Performance Gain (%)
direct fine­tuning
direct fine­tuning + template
multi­task learning
multi­task learning + template
maml
maml + template
fomaml
fomaml + template
reptile
reptile + template
Figure 9: Combining upstream learning with pattern-exploiting training.
task (with hyperparmeter tuning for all 5 random
samples) takes approximately 4 hours on average.
Number of Parameters.
BART-Base model
contains 139 million parameters. T5-v1.1-Base
model contains 246 million parameters. BART-
Large model contains 406 million parameters.
