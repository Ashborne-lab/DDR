A Bidirectional Tree Tagging Scheme for Joint
Medical Relation Extraction
1st Xukun Luo
Peking University
Beijing, China
luoxukun@pku.edu.cn
2nd Weijie Liu
Peking University
Beijing, China
dataliu@pku.edu.cn
3rd Meng Ma
Peking University
Beijing, China
mameng@pku.edu.cn
4th Ping Wang
Peking University
Beijing, China
pwang@pku.edu.cn
Abstract—Joint medical relation extraction refers to extracting
triples, composed of entities and relations, from the medical
text with a single model. One of the solutions is to convert
this task into a sequential tagging task. However, in the existing
works, the methods of representing and tagging the triples in a
linear way failed to the overlapping triples, and the methods of
organizing the triples as a graph faced the challenge of large
computational effort. In this paper, inspired by the tree-like
relation structures in the medical text, we propose a novel scheme
called Bidirectional Tree Tagging (BiTT) to form the medical
relation triples into two two binary trees and convert the trees
into a word-level tags sequence. Based on BiTT scheme, we
develop a joint relation extraction model to predict the BiTT
tags and further extract medical triples efﬁciently. Our model
outperforms the best baselines by 2.0% and 2.5% in F1 score on
two medical datasets. What’s more, the models with our BiTT
scheme also obtain promising results in three public datasets of
other domains.
Index Terms—medical relation extraction, sequential tagging
I. INTRODUCTION
Medical relation extraction is an important task for building
knowledge graphs in the medical domain. Relation extraction
(RE) refers to extracting the relations between entity pairs
contained in unstructured text such as electronic health records
(EHRs) and medication package inserts (MPIs).
The early studies on RE are mainly fallen on pipeline
methods, i.e., ﬁrst identify the entities in a sequence by the
named entity recognition (NER) module, and then classify
the relation for each entity pair by the relation classiﬁcation
(RC) module [25], [36]. However, in the above methods, the
error in NER module would be introduced into the subsequent
RC module. Thus, the joint methods were put forward to
solve this error propagation problem. These studies can be
roughly divided into several paradigms [19], [37], [40], among
them, one stream of work is to convert the RE task into the
sequential tagging task. The ﬁrst work on tagging scheme
[40] is developed to organize the relation triples in a linear
way and turn them into a tagging sequence with the same
length s as the input sentence. As [40] needs to compute the
probability of different tags for each token, the computational
complexity is O(s|R|), where |R| is the size of the predeﬁned
relation set R. However, [40] failed to the overlapping triples,
i.e., EntityPairOverlap (EPO) and SingleEntityOverlap (SEO)
[37]. Take the ﬁrst sentence in Figure 1 as an example, the
Trump was born in America in 1946 and became President of the state in 2016.
 Birth-Place
 President
Easily reversible hypoxemia and hypotension induced by nimodipine.
Drug-Events
Drug-Events
Hewitt was born in washington, the capital of America.
 Birth-Place
Birth-Place
 Capital
EPO
SEO
ELS
ILS
Fig. 1. Examples of the sentences with overlapping triples, including EPO,
ELS and ILS.
tagging scheme of [40] is unable to represent both Birth-
Place and President in a single tag for each word. After that,
some tagging schemes [3], [32] are proposed to handle the
overlapping problem. They formed the relation triples into
a graph represented by an adjacency matrix, and obtained a
tagging sequence of length s2. Though the improved tagging
sequences were able to accommodate more information, the
computation complexity of training is increased from O(s|R|)
to O(s2|R|).
Besides, most of existing medical RE frameworks are based
on the RE models specialized to the generic domains. How-
ever, the relation structure in the medical domain has different
characteristics from other domains. Speciﬁcally, from EHRs
and MPIs, we found that there are many tree-like relation
structures in the medical text. For example, in the second
sentence of Figure 1, a drug (nimodipine) corresponds to
two effects (hypoxemia and hypotension). Furthermore , drug
contains multiple ingredients and can treat multiple symptoms
in a MPI.
Inspired by this discovery, in our paper, we employ a forest
rather than a graph to represent relation triples in the medical
sentence, and convert the forest into a tagging sequence of
length 8s. Speciﬁcally, our scheme called Bidirectional Tree
Tagging (BiTT) is divided into three steps. First, the triples
with the same relation in a sentence are grouped together.
Second, the entities and relations in a group are modeled into
two binary trees according to the order in which they appear
in the sentence. Finally, we establish a mapping between the
binary tree and token-level sequence tags so that they can be
converted to each other. Our BiTT tags can represent more
arXiv:2008.13339v3  [cs.CL]  17 Aug 2022
triples with low computation complexity of O(s|R|).
The key contributions are summarized as:
• We propose a novel tagging scheme called BiTT, which
apply the forest structure to represent the relation triples
in the medical text;
• Based on our BiTT scheme, a joint medical RE model
is developed for automatically predicting BiTT tags and
extract relation triples;
• The models based on BiTT achieve solid results on two
medical datasets and three public generic datasets, which
is a unique beneﬁt obtained by effectively handling the
overlap issue through the BiTT scheme.
II. RELATED WORK
RE is a core task for text mining and information extraction.
Depending on the structural diversity of neural networks, a
number of pipeline RE models [28], [33], [36] were proposed
with improved effect. The joint RE methods were put forward
to solve the error propagation problem of the pipeline methods.
Some works [11], [19] extended the RC module to share the
encoder representation with the NER module, mitigated the
problem of error propagation but still extracted entities and
relations in a pipeline way. Some works [3], [30], [40] studied
on the sequential tagging based methods, which turned the
extracting tasks into a sequence labeling problem, bridging
the information gap between the RC and NER steps. They
focused on solving the overlapping triple problem and made
great progress. However, these works could not deal with
some special cases yet or cost much time. Some works
[21], [37], [38] directly generated the relational triples by
the seq2seq framework. The seq2seq based works did not
require complex tagging schemes, yet the maximum length of
generated sequence was hard to determine and the decoding
step cost much time.
Medical RE is a sub-ﬁeld of RE, and researchers usually
adapted the generic RE models through the challenges in
the medical domain. For example, [17] integrated the entity-
related information, such as part-of-speech (POS) and medical
ontology, into a joint RE model to improve the performance of
medical entity recognition. [16] proposed the trainable char-
acter embedding in the model to solve the out-of-vocabulary
(OOV) problem in the medical text. And [23] exploited the
medical knowledge of medicines, and incorporated the medical
knowledge database into a RE model for Chinese medicine
instructions.
III. METHODOLOGY
In this section, we ﬁrstly present our ﬁne-grained division
for the sentences with overlapping triples. Then we illustrate
how to convert these sentences to BiTT tag sequences and
extract triples from the BiTT tags. Finally, we introduce our
joint RE model for predicting BiTT tags.
A. Fine-grained Division
As shown in Figure 1, [37] categorized the sentences with
overlapping triples into EPO and SEO. A sentence belongs to
Algorithm 1 Relation-to-Tree
Require: A s-words sentence, S; An array of m relational
triples with the same relation category in S, RT; An array
of n entities in S, EN;
Ensure: A binary relation tree, B;
1: /* Construct a relation forest. */
2: Initialize L = [], F = [];
3: for each i ∈[1, n] do
4:
Initialize l = an array of all ENi’s location pair in S;
5:
Add the elements in l to L;
6: end for
7: Sort L from small to large according to beginning index;
8: while L is not empty do
9:
Initialize a tree T with the root L1, remove L1 from L;
10:
for i = 2; i ≤length(L); i + + do
11:
if Li not in T and there is a valid relation between
Li and the node in T then
12:
Remove Li from L and add it to T;
13:
end if
14:
end for
15:
Add T to F;
16: end while
17: /* Transform the forest to a binary tree. */
18: Initialize an empty stack St, push the root nodes in F to
St in order;
19: Initialize a binary tree B whose root node is F1’s root;
20: for i = 2; i ≤length(F); i + + do
21:
Add Fi’s root to B as the right child of Fi−1’s root;
22: end for
23: while St is not empty do
24:
Initialize nodecur = Pop(St), C = the children array
of nodecur in F;
25:
Add C1 to B as the left child of nodecur;
26:
for i = 2; i ≤length(C); i + + do
27:
Add Ci to B as the right child of Ci−1;
28:
end for
29:
Push the children of nodecur to St in order;
30: end while
EPO if there are some triples whose entity sets ({e1, e2}) are
the same, and belongs to SEO if there are some triples whose
entity sets are different and contain at least one overlapping
entity. Note that a sentence can pertain to the intersection
of EPO and SEO. To separately handle the tree-like relation
structures in the medical text, we further divide SEO into
ExcludeLoopSentences (ELS) and IncludeLoopSentences (ILS)
based on the existence of relation loops. A sentence belongs to
ILS if there are some triples whose entity sets satisfy following
conditions: (1) they are different; (2) each of them has at
least one overlapping entity; (3) some of them contain two
overlapping entities. A sentence belongs to ELS if it pertains
to SEO but is not an ILS sentence. Note that there is at least
one loop in the relation graph of an ILS sentence and no loop in
the relation graph of an ELS sentence without considering the
Capital (Ca)
Contains (Co)
B
Sentence:
Contains (Co)
Capital (Ca)
Contains (Co)
Co
 Co    
Co
Washington
The White House
America
Contains (Co)
Process:
(1) Group
Washington
Forward forest
America
The White House
Washington
Backward forest
America
The White 
House
Washington
Forward binary
 tree
America
Backward binary
tree
(2) Relation-to-Tree
The White 
House
Washington
America
The White House
Forward tags
Part 1:
(3) Tree-to-Tag
Part 2:
Part 3:
Part 4:
Backward tags
Part 1:
Part 2:
Part 3:
Part 4:
I
E
Root
Root
Root
2
2
2
NULL
NULL
NULL
O
O
O
S
(l, 1)
NULL
2
O
O
O
S
(r, 1)
NULL
NULL
B
I
E
NULL
NULL
NULL
O
O
O
S
NULL
O
O
O
S
NULL
(r, 2)
(r, 2)
(r, 2)
NULL
NULL
NULL
(l, 2)
1
Root
1
The
White
House
is       located
in
Washington
capital
of
America
the
Fig. 2. Our Bidirectional Tree Tagging (BiTT) Scheme. Take the triples with the relation category Contains as examples.
edges’ direction. Based on the statistics of the datasets from
different domains in Table I, most of the medical sentences
with overlapping triples belong to ELS.
B. Bidirectional Tree Tagging Scheme
We propose a novel tagging scheme called BiTT, which
uses two binary tree structures to incorporate the three kinds
of sentence in Section III-A. We show an example in Figure
2 to detail our handling approach and algorithm for different
classes.
1) EPO Handling: Though there are some triples whose
entity sets are the same in the EPO sentences, their relation
categories are different from each other. Hence we group the
corresponding triples together with the same relation category
in a sentence and label them respectively. For example, as
shown in Figure 2, (America, Capital, Washington) with
the relation category Capital is divided into an independent
group, and triples with the relation category Contains are
aggregated into another group. Note that although the triple
groups are labeled respectively, all triples are predicted simul-
taneously by the joint RE model to preserve their correlations.
2) ELS Handling: The most remarkable feature for the
triples in an ELS sentence is that, there is no loop in its
relation graph, thus the triples can be completely represented
as a forest. We handle the ELS sentences with a Tree Tagging
scheme, which consists of two steps.
Relation-to-Tree The pseudo code is presented in Algo-
rithm 1. First, obtain the positions of all entities in a sentence
and construct a relation forest according to the entities’ for-
ward appearing order. Take the ﬁrst entity as the root of the
ﬁrst tree T1, and then recurrently go through other entities from
left to right, add the entities having relations with T1’s nodes
to T1 gradually. When the remaining entities can no longer be
added to the current tree, select the ﬁrst entity of remains as
a new tree Ti’s root and apply previous operations on Ti until
there is no entity left. After that, all trees are aggregated into
a forest F. Second, we convert F into a binary tree B. For
an entity node e in F, e’s ﬁrst child becomes e’s left child
in B, and right adjacent brother becomes e’s right child in
B. For example, the node Washington is the ﬁrst child of The
White House and the left brother of America in the forward
forest in Figure 2. Therefore, Washington becomes the left
child of The White House and America becomes Washington’s
right child in the forward binary tree. Note that we add the
annotation Brother on an edge in B if the nodes connected
by the edge used to be two root nodes in F. Besides, every
edge is directional, from e1 to e2 in a speciﬁc triple.
Tree-to-Tag We assign a tag for every single word in the
sentence according to the binary relation tree B from Relation-
to-Tree step. If a word does not belong to any entity, its tag
will be “O”. If a word is a part of an entity node e, its tag
will be a combination of following four parts:
• Part 1 (P1) indicates the position of a word in the
entity node e with the “BIES” signs, i.e., P1
∈
{B(begin), I(in), E(end), S(single)}.
• Part 2 (P2) indicates the information on the edge between
e and its parent in B. P2 = Root when e is the root of
B. And P2 = Brother when the annotation on the edge
is also Brother. Except, P2 = (Child2, Role2), where
Child2 ∈{l(left), r(right)} indicates if e is the left
or right child of its parent, and Role2 ∈{1(e1), 2(e2)}
shows the entity role of e pointed out by the direction of
the edge.
• Part 3 (P3) indicates the information on the edge be-
tween e and its left child in B. P3 = NULL when
e has no left child. Except that, P3 = Role3, where
Role3 ∈{1(e1), 2(e2)}.
• Part 4 (P4) indicates the information on the edge between
e and its right child in B. P4 = NULL when e has
no right child. P4 = Brother when the annotation on
the edge is also Brother. Except, P4 = Role4, where
Role4 ∈{1(e1), 2(e2)}.
3) ILS Handling: Since there is at least one loop in the
relation graph of an ILS sentence, the triples can not be
absolutely represented by a single forest. To solve the problem,
we simply build another forest according to the entities’
backward appearing order, then convert it to a backward binary
tree, and obtain the backward tree tags for a sentence. We
apply the forward tags and the backward tags to accommodate
more triples.
C. Tags to Triples
For the forward tags, ﬁrst ﬁnd all root nodes in the forest
by the condition of P2 = Root or P2 = Brother. Then we
start from these root nodes, recursively match P3 or P4 of
the nodes already in forest with other nodes’ P2 to reconnect
the edges between the node pairs. If a parent node can match
more than one node as its left (or right) child, we select the
nearest node behind of it, since we take the entities’ appearing
order into account when building the forest in the ﬁrst step
of the Tree Tagging scheme in Section III-B2. What’s more,
when rebuilding the relation tree, if a child or brother node is
missed, our algorithm simply ignore it and discard its sub-tree.
After reconstructing the forest, we can recursively extract the
relational triples in the sentence from it.
The operation on the backward tags is the same as that on
the forward tags. Except that when a parent node can match
more than one node as its left (or right) child, we select the
nearest node before it.
D. Joint Relation Extraction Model
In this section, we introduce our joint RE model in four
parts, i.e., the text embedding module, the encoder module,
the decoder module and our loss function respectively.
1) Text Embedding: For a given word w in the input
sentence, the representation e ∈Rd of w from the text
embedding module is concatenate by four parts:
e = Linear([ew; el; ec; ep])
(1)
where ew ∈Rdw is the word embedding, el ∈Rdl is the pre-
trained contextualized word embedding from language models
such as BERT [4], ec ∈Rdc is the character embedding, and
ep ∈Rdp is the part-of-speech (POS) embedding.
2) Encoder: The encoder module aims to extract a context
vector representation for each word in the sentence. In this
paper, we adopt the Bi-directional Long Short-Term Memory
(Bi-LSTM) layers and the multi-head self-attention layers as
the encoder.
A Bi-LSTM layer consists of a forward LSTM [9] and
a backward one. Denote the word embedding vectors of a
sentence with s words as V = [e1, . . . , es]. The output ot
and the hidden state ht of the t-th word wt from LSTM are:
ot, ht = lstm block(et, ht−1)
(2)
where lstm block(·) is the function of a memory block in
LSTM. We concatenate the two hidden states corresponding
to the same word together as the Bi-LSTM hidden state. Thus,
the Bi-LSTM hidden state ˙ht of the t-th word wt is:
˙ht = [−→
ht, ←−−−−
hs−t+1]
(3)
where −→
ht and ←−−−−
hs−t+1 are respectively the hidden states of wt
in the forward LSTM and the backward one.
The multi-head self-attention layer, the main component in
the Transformer [27] encoder, is represented mathematically
as follow:
MultiHead(H) = [head1(H); . . . ; headh(H)]
(4)
headi(H) = Atten(HWQ
i , HWK
i , HVQ
i )
(5)
Atten(Q, K, V) = softmax(QKT
√dk
)V
(6)
Here H ∈Rs×dh denotes the hidden state matrix [ ˙h1, . . . , ˙hs]
of the input sentence from the last Bi-LSTM layer. WQ
i , WK
i
and VQ
i
are learnable matrices. h indicates the number of
heads and dk = dh/h means the size of the key vector.
3) Decoder: The decoder module is designed to parse the
word representations and then predict the BiTT tags for each
word. In this paper, it consists of a series of Linear layers.
And there are two alternative construction methods for the
Linear layers to predict BiTT tags, i.e., one-head and multi-
head. Take a forward tree tag as example, the former means
that concatenate the four parts of the tag as one label and
predict it by a single Linear layer, like the work in [40].
The latter predicts these four parts separately with four Linear
layers, requiring fewer parameters. In this study, we employ
the multi-head mechanism to reduce the computational costs.
4) Loss Function: The loss function for training the for-
ward or backward tags is a weighted bias objective function
deﬁned by:
L = λ1L1 + λ2L2 + λ3L3 + λ4L4
(7)
where Lj(j ∈{1, 2, 3, 4}) are the same bias cross entropy
functions as [40] for the four parts of BiTT tags respectively,
λj are the weights for Lj. We obtain Lf for the forward tags
and Lb for the backward tags by Eq.(7), then deﬁne the total
loss LT of our framework as follow:
LT = Lf + γLb
(8)
where γ is a weight hyperparameter.
IV. EXPERIMENTS
A. Experimental Setting
Datasets We evaluate our BiTT scheme on ﬁve datasets,
including two medical datasets (ADE [6] and CMeIE [8]) and
three datasets (NYT [24], WebNLG [7] and DuIE [14]) from
other domains.
ADE, collected from the English medical reports, contains
4.2k samples with the relation Adverse-Effect. Following pre-
vious work [34], the samples with overlapping entities are
ﬁltered out and the 10-fold cross validation is performed
for ADE in our experiment. CMeIE is a Chinese medical
dataset constructed by several rounds of manual annotation.
It consists of 28k sentences and 44 relations derived from
medical textbooks and clinical practices.
NYT contains 66.1k sentences with 24 relation categories
from New York Times news. We adopt the preprocessed
version released by [37]. WebNLG is an English dataset
created for Natural Language Generation (NLG) task. It was
adapted for RE task and released by [37], which contains 6.2k
sentences and 246 relation types. DuIE, consisting of 214.7k
sentences with 49 relation categories, is a big Chinese dataset
released by Baidu Inc. for RE. Since the testing set of DuIE
is not available, we randomly separate the training set into
a new validation set (20k samples) and a new training set
(153.1k samples), and take the old validation set as the testing
set (21.6k samples).
Table I illustrates some statistics of the samples with
overlapping triples in the ﬁve datasets. It indicates that the
overlapping triple problem is very common in all datasets.
Besides, in the medical datasets, the samples with overlapping
triples have a high proportion of ELS samples , reaching more
than 87%. However, this pattern may not apply to the datasets
in other domains, e.g., the sentences with overlapping triples
have a low proportion of ELS sentences at only 42.2%.
Metrics We employ micro Precision (Prec), Recall (Rec)
and F1 score (F1) to evaluate the performance of our models.
We consider a predicted triple (e1, r, e2) as a correct one
only if e1, e2 and r are all correct. Note that some original
achievements of baselines evaluate the performance by Partial
Matching, which means that (e1, r, e2) is simply regarded as
a correct triple if the ﬁrst (or last) tokens of two entities (e1
and e2) and r are correct.
Implementation Details For the medical datasets, we apply
the model called BiTT-Med mentioned in Section III-D for
prediction of BiTT tagging sequence and extraction of relation
triples. In the text embedding module, the hidden sizes of
different embedding vectors are dw = dc = dp = 100 and
dl = d = 768. The pre-trained GloVe [22] vectors are used
to initialize the word embedding ew. The contextualized word
embedding el is ﬁxed and initialized by the vectors from the
pre-trained BERT-base encoder. The character embedding is
randomly initialized and computed by an LSTM [12] to cope
with the OOV problem. And the POS tagging sequence of
the input sentence is generated by SpaCy [10]. In the encoder
module, the number of Bi-LSTM layers in the encoder module
TABLE I
STATISTICS OF THE TRAINING SET AND THE TESTING SET OF THE FIVE
DATASETS. ELS Ratio INDICATES ELS / OVERLAP SAMPLES.
Dataset
EPO
ELS
ILS
Overlap
Samples
ELS
Ratio
ADE [6]
118
1,216
159
1,391
0.874
CMeIE [8]
381
8,805
457
9,213
0.956
NYT [24]
17,004
10,740
2,006
25,422
0.422
WebNLG [7]
622
2,894
1,294
3,957
0.731
DuIE [14]
15,672
94,891
11,780
109,675
0.865
is 2 and the hidden size is 768. The number of multi-head self-
attention layers is 2 and the number of head is h = 8. As for
the loss function, in order to balance the loss of each part of the
BiTT tags, we adopt λ1 = 8/6, λ2 = 8/8, λ3 = 8/5 and λ4 =
8/6 in Eq.(7), since the number of optional labels in the four
parts are 6, 8, 5 and 6 after adding the tag “O” and the padding
tag respectively. And we set γ = 1 in Eq.(8). Besides, the
maximum length of an input sentence, the batch size and the
learning rate are set to 128, 8 and 1e-4 respectively. And the
training operation is terminated when the F1 on validation set
no longer rises. Besides, in Table II, some baselines on CMeIE
dataset is upgraded by ourselves for a further comparison. We
simply substitute our text embedding module for the word
embedding layers of the original achievements their papers.
For the other three datasets, due to the large amount of
training data and the absence of medical proper nouns in
the generic datasets, we simplify BiTT-Med and obtain two
new models called BiTT-LSTM and BiTT-BERT. Through
these two models we can evaluate the performance of the
BiTT scheme on generic domains. Speciﬁcally, we replace
the text embedding layer of BiTT-Med with a learnable GloVe
embedding in BiTT-LSTM and an unﬁxed BERT-base encoder
in BiTT-BERT. The warm up rate in BiTT-BERT is set to
0.1. And we remove the multi-head self-attention layers of
BiTT-Med in these two models. What’s more, similar to the
experiments on the medical datasets, in Table III, we apply
the BERT-base encoder as the word embedding layer of some
baseline models and report the results. Note that the evaluation
metrics in original NovelTagging and GraphRel are much
looser than that for their upgraded version with BERT in our
experiment, which makes the results of upgraded models with
BERT look worse.
B. Baseline Models
For comparison, we employ 13 models as baselines, which
can be roughly divided into one-stage models and two-stage
models. The one-stage models output both entities and rela-
tions at the same time, including Neural Joint [17], NovelT-
agging [40], Rel-Metric [26], PA [3], GraphRel [5], Table-
Sequence [29], TPLinker [32], and PFN [34]. The two-stage
models identify the entities ﬁrstly and then classify the rela-
tions of all entity pairs, or identify the head entities ﬁrst and
then ﬁnd out the tail entities based on the relation categories
TABLE II
THE PERFORMANCE COMPARISON OF DIFFERENT METHODS ON ADE AND
CMEIE. ♥INDICATES THAT THE RESULTS ON CMEIE IS UPGRADED
FROM ORIGINAL ACHIEVEMENTS BY OUR TEXT EMBEDDING MODULE.
THE MAIN ENCODERS APPLIED IN DIFFERENT MODELS: L = LSTM, L+C
= LSTM + CNN, ALB = ALBERT-XXLARGE-V1, BB = BERT-BASE,
BB+G = BERT-BASE + GCN.
Model
Encoder
Prec
Rec
F1
ADE
Neural Joint [17]
L
64.0
62.9
63.4
Multi-head [1]
L
72.1
77.2
74.5
Multi-head + AT [2]
L
-
-
75.5
Rel-Metric [26]
L+C
77.4
77.3
77.3
Table-Sequence [29]
ALB
-
-
80.1
PFN [34]
Bb
-
-
80.0
BiTT-Med (Ours)
Bb
83.1
81.3
82.1
CMeIE
NovelTagging [40] ♥
Bb
51.4
17.1
25.6
GraphRel-1p [5] ♥
Bb+G
31.2
26.0
28.4
GraphRel-2p [5] ♥
Bb+G
28.5
23.1
25.5
CasRel [30] ♥
Bb
53.5
28.2
37.0
ER+RE [35]
ALB
-
-
47.6
BiTT-Med (Ours)
Bb
55.6
45.5
50.1
for each head entity. The corresponding representative methods
include Multi-head [1], Multi-head + AT [2], CopyRE [37],
CopyMTL [38] and CasRel [30].
C. Results
Compared Results on Medical Datasets Table II presents
the comparison of BiTT-Med with previous methods on two
medical datasets. Our BiTT-Med achieves solid F1 scores on
ADE and CMeIE, which are 82.1% and 50.1%. On ADE,
our model outperforms Table-Sequence by 2.0% in F1, and
outperforms Rel-Metric by 5.7% in Prec and 4.0% in Rec. On
CMeIE, our model outperforms ER+RE by 2.5% in F1, and
outperforms CasRel by 2.1% in Prec and 17.3% in Rec. This
indicates the effectiveness of our model for jointly extracting
medical entities and relations.
Compare Results on Common Datasets To further evalu-
ate the performance of our BiTT scheme on the joint RE task,
we apply our BiTT-LSTM and BiTT-BERT models on three
generic datasets. Table III shows the comparison of our models
with baselines on NYT, WebNLG and DuIE. For BiTT-LSTM,
it achieves ideal F1 scores on NYT and WebNLG, which
are 71.1% and 73.8%. BiTT-LSTM outperforms CopyMTL-
One by 7.6% and 11.1% in Rec on NYT and WebNLG.
There is an impressive Rec gap between BiTT-LSTM and
the baselines without BERT encoder, which veriﬁes the utility
of BiTT scheme when dealing with overlapping triples. For
BiTT-BERT, it also achieves solid F1 scores close to CasRel
and TPLinker on NYT, WebNLG and DuIE datasets, which
are 88.9% , 86.2% and 78.0% respectively. And BiTT-BERT
achieve the best Prec on three datasets, which are 89.7%,
89.1% and 75.7%. In addition, we note that the sequential
tagging based models (NovelTagging, CasRel, TPLinker and
BiTT-BERT) achieve higher Prec than other models. It proves
TABLE III
THE PERFORMANCE COMPARISON OF DIFFERENT METHODS ON NYT,
WEBNLG AND DUIE. ♠INDICATES THE METRICS FOR THE MODELS
FOLLOW PARTIAL MATCHING. ♥INDICATES THAT THE RESULTS ON NYT
AND DUIE IS UPGRADED FROM ORIGINAL ACHIEVEMENTS BY THE
BERT-BASE ENCODER. THE MAIN ENCODERS APPLIED IN DIFFERENT
MODELS: L = LSTM, L+G = LSTM + GCN, BB = BERT-BASE, BB+G =
BERT-BASE + GCN.
Model
Encoder
Prec
Rec
F1
NYT
NovelTagging [40] ♠
L
62.4
31.7
42.0
CopyRE-Mul [37] ♠
L
61.0
56.6
58.7
GraphRel-2p [5] ♠
L+G
63.9
60.0
61.9
PA [3]
L
49.4
59.1
53.8
CopyMTL-Mul [38]
L
75.7
68.7
72.0
NovelTagging [40] ♥
Bb
89.0
55.6
69.3
CopyRE-Mul [37] ♥
Bb
39.1
36.5
37.8
GraphRel-2p [5] ♥
Bb+G
82.5
57.9
68.1
CasRel [30] ♠
Bb
89.7
89.5
89.6
BiTT-LSTM (Ours)
L
66.5
76.3
71.1
BiTT-BERT (Ours)
Bb
89.7
88.0
88.9
WebNLG
NovelTagging [40] ♠
L
52.5
19.3
28.3
CopyRE-Mul [37] ♠
L
37.7
36.4
37.1
GraphRel-2p [5] ♠
L+G
44.7
41.1
42.9
CopyMTL-Mul [38]
L
58.0
54.9
56.4
TPLinker [32]
Bb
88.9
84.5
86.7
BiTT-LSTM (Ours)
L
83.8
66.0
73.8
BiTT-BERT (Ours)
Bb
89.1
83.0
86.2
DuIE
NovelTagging [40] ♥
Bb
75.0
38.0
50.4
GraphRel-1p [5] ♥
Bb+G
52.2
23.9
32.8
GraphRel-2p [5] ♥
Bb+G
41.1
25.8
31.8
CaseRel [30] ♥
Bb
75.7
80.0
77.8
BiTT-BERT (Ours)
Bb
75.7
80.6
78.0
the superiority of the sequential tagging based methods for
conservative prediction. However, BiTT-BERT does not per-
form as well as the best method on NYT and WebNLG.
It probably results from the fact that the proportion of ELS
sentences in the samples with overlapping triples of NYT and
WebNLG are 42.2% and 73.1%, which are not as large as in
DuIE. This demonstrates the advantages of our BiTT scheme
for handling ELS sentences.
D. Efﬁciency of BiTT based models
Our BiTT based models can predict the BiTT tags with
low computation complexity of O(s|R|), thus have higher
efﬁciency compared to graph-based and two-stage models.
First, we compare our BiTT-BERT model with the one-stage
baselines. As shown in Figure 3, in our experiments on NYT,
most of baseline models with BERT-base encoder converge at
a similar epoch, i.e., about the 15th epoch. Besides, the number
of decoder parameters in our BiTT-BERT (48.9M) is almost
the same as that in NovelTagging (47.3M) and much less
than that in other one-stage baseline models, i.e., GraphRel-2p
(106.4M) and TPLinker (1293.3M). This indicates that with
the same encoder, the converge speed of our framework is


 
 
 

 
 
 
 
  
    
   	 	     
  
                
        
       
 	               
        
Fig. 3. The training curves of different models with BERT-base encoder on
the NYT dataset.
much faster than Graph-Rel and TPLinker. Second, we com-
pare our BiTT-BERT with the competitive two-stage baseline
CasRel. We measure the time of a training epoch (traversing
all triples in the WebNLG training set) for BiTT-BERT and
CasRel on a same NIVIDIA GeForce RTX 2080 Ti. The result
is that BiTT-BERT takes 255.0s and CasRel takes 1701.2s.
CasRel takes about 6.7 times longer to traverse the dataset
than BiTT-BERT, since the two-stage model CasRel copies
a sentence into multiple samples for training based on the
number of head entities.
E. Ablation Study
As is shown in Table IV, to explore the effects of the
handling methods for overlapping triples in BiTT scheme
and our decoder architecture, we perform the ablation tests
based on BiTT-BERT and the NYT dataset. BiTT-BERT
“w/o Group” means that the “EPO Handling” described in
Section III-B1 is dropped while replaced by adding the relation
categories information to P2, P3 and P4 of the BiTT tags.
BiTT-BERT “w/o Bidirectional” implies that we only build
the forward forest in a sentence and then generate the forward
tags, BiTT-BERT “w/o Multi-head” indicates that the multi-
head structure in our framework is substituted by the one-head
structure, which is illustrated in the decoder module of our
joint RE model in Section III-D.
From Table IV, we have observed some impressive facts
when comparing these three ablation tests with BiTT-BERT.
First, the grouping operation greatly improves the performance
of our framework on not only EPO (17.2%) but also ELS
(11.1%) and ILS (17.2%), since it reduces a complex relation
graph into several simpler ones in a sentence. And it also
reduces the categories of BiTT tags, since the information
of relation categories and entity types does not need to be
added to the tags. Second, the backward forest can effectively
supplement the triples that cannot be totally represented by
a single forward forest, which provides an F1 score boost of
10.8% for ILS sentences. The backward forest also boosts the
F1 score on EPO and ELS sentences by 1.0% and 2.8%. This
indicates that when the sub-tree of a node is discarded due
to the wrong prediction of the forward forest, the backward
TABLE IV
THE RESULTS OF THE ABLATION STUDY ON BITT-BERT MODEL AND THE
NYT DATASET. F1-EPO, F1-ELS, F1-ILS ARE THE F1 SCORES ON THE
EPO, ELS AND ILS SENTENCES. F1-ALL IS THE F1 SCORE ON ALL
SENTENCES. THE SCALE OF DECODER PARAMETERS IS MILLION (M),
AND THE SCALE OF TRAINING TIME IS SECOND PER EPOCH (S/EPOCH).
Metrics
w/o
Group
w/o
Bidirectional
w/o
Multi-head
BiTT-BERT
F1-EPO
74.0
90.2
90.2
91.2
F1-ELS
76.2
84.5
85.5
87.3
F1-ILS
68.3
74.7
81.8
85.5
F1-All
81.9
88.2
87.7
88.9
Decoder Params
71.3
48.0
68.5
48.9
Training Time
-
-
2125.0
1739.0
forest can supplement the information of the missing node.
Last, compared with the one-head structure, the multi-head
structure can decrease the parameters of the decoder module,
thus lessen the computation burden and the training time in
an epoch. Besides, the F1 score of multi-head is 1.2% higher
than one-head. This indicates that multi-head does not make
the interaction of the four parts of BiTT labels less, but makes
the labels with fewer occurrences to be trained more.
V. CONCLUSION
In this paper, motivated by the tree-like relation structures
in the medical text, we propose a Bidirectional Tree Tagging
(BiTT) scheme to label the overlapping entities and relations
in medical sentences with solid accuracy and high efﬁciency.
We build up a jointly RE model called BiTT-Med and two
simpliﬁed versions (BiTT-LSTM and BiTT-BERT) for exper-
iments. The results on two public medical datasets and three
generic datasets demonstrate that our proposal outperforms
several baselines, especially when processing the ELS cases.
Our future work aims to improve the BiTT scheme and the
RE models in following aspects. (1) When extracting results
from BiTT tags, we will try reconstructing a binary forest
instead of a binary tree to reduce the error propagation if a
node is dropped. (2) More rule restrictions for BiTT will be
proposed to make it more robust. (3) We will apply more
powerful pre-trained encoders to the extraction framework for
better performance.
REFERENCES
[1] G. Bekoulis, J. Deleu, T. Demeester and C. Develder, ”Joint entity
recognition and relation extraction as a multi-head selection problem,”
Expert Systems with Applications, 2018, pp. 34–45.
[2] G. Bekoulis, J. Deleu, T. Demeester and C. Develder, ”Adversarial train-
ing for multi-context joint entity and relation extraction,” in Proceedings
of EMNLP, 2018, pp. 2830–2836.
[3] D. Dai, X. Xiao, Y. Lyu, S. Dou, Q. She and H. Wang, ”Joint extraction
of entities and overlapping relations using position-attentive sequence
labeling,” in Proceedings of AAAI, 2019, pp. 6300-6308.
[4] J. Devlin, M. W. Chang, K. Lee and K. Toutanova, ”BERT: pre-
training of deep bidirectional transformers for language understanding,”
in Proceedings of ACL, 2019, pp. 4171-4186.
[5] T. J. Fu, P. H. Li and W. Y. Ma, ”GraphRel: modeling text as relational
graphs for joint entity and relation extraction,” in Proceedings of ACL,
2019, pp. 1409-1418.
[6] H. Gurulingappa, A. Mateen, Angus, J. Fluck, M. Hofmann-Apitius and
L. Toldo, ”Development of a benchmark corpus to support the automatic
extraction of drug-related adverse effects from medical case reports,” J.
Biomed. Informatics, vol. 45, no. 5, pp. 885-892, 2012.
[7] C. Gardent, A. Shimorina, S. Narayan and L. Perez-Beltrachini, ”Creat-
ing training corpora for NLG micro-planners,” in Proceedings of ACL,
2017, pp. 179-188.
[8] T. Guan, H. Zan, X. Zhou, H. Xu, and K Zhang. ”CMeIE: construction
and evaluation of Chinese medical information extraction dataset,” In
proceedings of NLPCC, 2020.
[9] S. Hochreiter and J. Schmidhuber, ”Long short-term memory, ” Neural
Comput, pp. 1735-1780, 1997.
[10] M. Honnibal and M. Johnson, ”An improved non-monotonic transition
system for dependency parsing,” in Proceedings of EMNLP, 2015, pp.
1373-1378.
[11] A. Katiyar and C. Cardie, ”Investigating LSTMs for joint extraction of
opinion entities and relations,” in Proceedings of ACL, 2016, pp. 919-
929.
[12] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer.
”Neural architectures for named entity recognition,” In Proceedings of
HLT-NAACL, 2016.
[13] Q. Li and H. Ji, ”Incremental joint extraction of entity mentions and
relations,” in Proceedings of ACL, 2014, pp. 402-412.
[14] S. Li, W. He, Y. Shi, W. Jiang, H. Liang and Y. Jiang, et al., ”DuIE:
a large-scale chinese dataset for information extraction,” in Proceedings
of NLPCC, 2019, pp. 791-800.
[15] F. Li, M. Zhang, G. Fu, T. Qian and D. Ji, ”A bi-lstm-rnn model for
relation classiﬁcation using low-cost sequence features,” CoRR, vol.
abs/1608.07720, 2016.
[16] F. Li, M. Zhang, G. Fu and D. Ji, ”A neural joint model for entity and
relation extraction from biomedical text, ” BMC Bioinform., vol. 18,
no. 1, pp. 198:1-198:11, 2017.
[17] F. Li, Y. Zhang and D. Ji, ”Joint models for extracting adverse drug
events from biomedical text,” in Proceedings of IJCAI, 2016, pp. 2838-
2844.
[18] M. Mintz, S. Bills, R. Snow and D. Jurafsky, ”Distant supervision for
relation extraction without labeled data,” in Proceedings of ACL and
AFNLP, 2009, pp. 1003-1011.
[19] M. Miwa and M. Bansal, ”End-to-end relation extraction using LSTMs
on sequences and tree structures,” in Proceedings of ACL, 2016, pp.
1105-1116.
[20] M. Miwa and Y. Sasaki, ”Modeling joint entity and relation extraction
with table representation,” in Proceedings of EMNLP, 2014, pp. 1858-
1869.
[21] T. Nayak and H. Tou, ”Effective modeling of encoder-decoder architec-
ture for joint entity and relation extraction,” in Proceedings of AAAI,
2020, pp. 8528-8535.
[22] J. Pennington, R. Socher and C. Manning, ”Glove: global vectors for
word representation,” in Proceedings of EMNLP, 2014, pp. 1532-1543.
[23] T. Qi, S. Qiu, X. Shen, H. Chen, S. Yang and H. Wen, et al., ”KeMRE:
knowledge-enhanced medical relation extraction for chinese medicine
instructions, ” J. Biomed. Informatics, vol. 120, pp. 103834, 2021.
[24] S. Riedel, L. Yao and A. McCallum, ”Modeling relations and their
mentions without labeled text,” in Proceedings of ECML-PKDD, 2010,
pp. 148–163.
[25] R. Socher, B. Huval, C. D. Manning and A. Y. Ng, ”Semantic com-
positionality through recursive matrix-vector spaces,” in Proceedings of
EMNLP, 2012, pp. 1201-1211.
[26] T. Tran, R. Kavuluru, ”Neural metric learning for fast end-to-end relation
extraction,” CoRR, vol. abs/1905.07458, 2019.
[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones and A. N.
Gomez, et al., ”Attention is all you need,” in Advances in NeurIPS,
2017, pp. 5998-6008.
[28] N. T. Vu, H. Adel, P. Gupta and H. Schutze, ”Combining recurrent and
convolutional neural networks for relation classiﬁcation,” in Proceedings
of ACL, 2016, pp. 534-539.
[29] J. Wang and W. Lu, ”Two are better than one: Joint entity and relation
extraction with table-sequence encoders,” in Proceedings of EMNLP,
2020, pp. 1706-1721.
[30] Z. Wei, J. Su, Y. Wang, Y. Tian and Y. Chang, ”A novel cascade binary
tagging framework for relational triple extraction,” in Proceedings of
ACL, 2020, pp. 1476-1488.
[31] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi and W. Macherey
et al., ”Google’s neural machine translation system: bridging the gap
between, ” CoRR, vol. abs/1609.08144, 2016.
[32] Y. Wang, B. Yu, Y. Zhang, T. Liu, H. Zhu and L. Sun, ”TPLinker:
single-stage joint extraction of entities and relations through token pair
linking,” in Proceedings of COLING, 2020, pp. 1572–1582.
[33] Y. Xu, L. Mou, G. Li, Y. Chen, H. Peng and Z. Jin, ”Classifying relations
via long short term memory networks along shortest dependency paths,”
in Proceedings of EMNLP, 2015, pp. 1785-1794.
[34] Z. Yan, C. Zhang, J. Fu, Q. Zhang and Z. Wei, ”A Partition Filter
Network for Joint Entity and Relation Extraction”, in Proceedings of
EMNLP, 2021, pp. 185–197.
[35] N. Zhang, M. Chen, Z. Bi, X. Liang, L. Li and X. Shang, et al., ”CBLUE:
a chinese biomedical language understanding evaluation,” in Proceedings
of ACL, 2022, pp. 7888-7915.
[36] D. Zeng, K. Liu, S. Lai, G. Zhou and J. Zhao, ”Relation classiﬁcation
via convolutional deep neural network,” in Proceedings of COLING,
2014, pp. 2335-2344.
[37] X. Zeng, D. Zeng, S. He, K. Liu and J. Zhao, ”Extracting relational facts
by an end-to-end neural model with copy mechanism,” in Proceedings
of ACL, 2018, pp. 506-514.
[38] D. Zeng, H. Zhang and Q. Liu, ”CopyMTL: copy mechanism for
joint extraction of entities and relations with multi-task learning,” in
Proceedings of AAAI, 2020, pp. 9507-9514.
[39] Z. Zhao, H. Chen, J. Zhang, X. Zhao, T. Liu and W. Lu, et al., ”UER: an
open-source toolkit for pre-training models,” in Proceedings of EMNLP-
IJCNLP, 2019, pp. 241-246.
[40] S. Zheng, F. Wang, H. Bao, Y. Hao, P. Zhou and B. Xu, ”Joint
extraction of entities and relations based on a novel tagging scheme,”
in Proceedings of ACL, 2017, pp. 1227-1236.
